{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOiV4++12JDZNIbVcU/lugK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BardRimon/Study/blob/main/Reinforcement_Learning/HW2/HW2_DDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flappy-bird-gymnasium torch"
      ],
      "metadata": {
        "id": "ZwNaNFkwrBLd",
        "outputId": "c7713c98-21f8-46ab-f70a-1e0c75c5d397",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flappy-bird-gymnasium\n",
            "  Downloading flappy_bird_gymnasium-0.4.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (from flappy-bird-gymnasium) (1.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from flappy-bird-gymnasium) (2.0.2)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (from flappy-bird-gymnasium) (2.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from flappy-bird-gymnasium) (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium->flappy-bird-gymnasium) (3.1.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium->flappy-bird-gymnasium) (0.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->flappy-bird-gymnasium) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->flappy-bird-gymnasium) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->flappy-bird-gymnasium) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->flappy-bird-gymnasium) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->flappy-bird-gymnasium) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->flappy-bird-gymnasium) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->flappy-bird-gymnasium) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->flappy-bird-gymnasium) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->flappy-bird-gymnasium) (1.17.0)\n",
            "Downloading flappy_bird_gymnasium-0.4.0-py3-none-any.whl (37.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, flappy-bird-gymnasium\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed flappy-bird-gymnasium-0.4.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import argparse\n",
        "RUNS_DIR = \"runs\"\n",
        "os.makedirs(RUNS_DIR, exist_ok=True)\n",
        "\n",
        "DATE_FORMAT = \"%m-%d %H:%M:%S\""
      ],
      "metadata": {
        "id": "54vt7FtzsSCH"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config_content = '''\n",
        "{\n",
        "\n",
        "    \"version\": \"0.2.0\",\n",
        "    \"configurations\": [\n",
        "        {\n",
        "            \"name\": \"Train cartpole1\",\n",
        "            \"type\": \"debugpy\",\n",
        "            \"request\": \"launch\",\n",
        "            \"program\": \"agent.py\",\n",
        "            \"console\": \"integratedTerminal\",\n",
        "            \"args\": [\"cartpole1\", \"--train\"]\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"Train flappybird1\",\n",
        "            \"type\": \"debugpy\",\n",
        "            \"request\": \"launch\",\n",
        "            \"program\": \"agent.py\",\n",
        "            \"console\": \"integratedTerminal\",\n",
        "            \"args\": [\"flappybird1\", \"--train\"]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "'''\n",
        "with open('launch.json', 'w') as file:\n",
        "    file.write(config_content)"
      ],
      "metadata": {
        "id": "GCW3eQvyzvQt"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config_content = '''cartpole1:\n",
        "  env_id: CartPole-v1\n",
        "  replay_memory_size: 100000\n",
        "  mini_batch_size: 32\n",
        "  epsilon_init: 1\n",
        "  epsilon_decay: 0.9995\n",
        "  epsilon_min: 0.05\n",
        "  network_sync_rate: 10\n",
        "  learning_rate_a: 0.0001\n",
        "  discount_factor_g: 0.99\n",
        "  stop_on_reward: 100000\n",
        "  fc1_nodes: 512\n",
        "  env_make_params:\n",
        "    use_lidar: False\n",
        "  enable_double_dqn: True\n",
        "  enable_dueling_dqn: True\n",
        "\n",
        "flappybird1:\n",
        "  env_id: FlappyBird-v0\n",
        "  replay_memory_size: 100000\n",
        "  mini_batch_size: 32\n",
        "  epsilon_init: 1\n",
        "  epsilon_decay: 0.99_99_5\n",
        "  epsilon_min: 0.05\n",
        "  network_sync_rate: 10\n",
        "  learning_rate_a: 0.0001\n",
        "  discount_factor_g: 0.99\n",
        "  stop_on_reward: 100000\n",
        "  fc1_nodes: 512\n",
        "  env_make_params:\n",
        "    use_lidar: False\n",
        "  enable_double_dqn: True\n",
        "  enable_dueling_dqn: True\n",
        "'''\n",
        "\n",
        "with open('hyperparameters.yml', 'w') as file:\n",
        "    file.write(config_content)"
      ],
      "metadata": {
        "id": "61o7GVhug86U"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=256, enable_dueling_dqn=True):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.enable_dueling_dqn=enable_dueling_dqn\n",
        "\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
        "\n",
        "        if self.enable_dueling_dqn:\n",
        "            # Value stream\n",
        "            self.fc_value = nn.Linear(hidden_dim, 256)\n",
        "            self.value = nn.Linear(256, 1)\n",
        "\n",
        "            # Advantages stream\n",
        "            self.fc_advantages = nn.Linear(hidden_dim, 256)\n",
        "            self.advantages = nn.Linear(256, action_dim)\n",
        "\n",
        "        else:\n",
        "            self.output = nn.Linear(hidden_dim, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        if self.enable_dueling_dqn:\n",
        "            # Value calc\n",
        "            v = F.relu(self.fc_value(x))\n",
        "            V = self.value(v)\n",
        "\n",
        "            # Advantages calc\n",
        "            a = F.relu(self.fc_advantages(x))\n",
        "            A = self.advantages(a)\n",
        "\n",
        "            # Calc Q\n",
        "            Q = V + A - torch.mean(A, dim=1, keepdim=True)\n",
        "\n",
        "        else:\n",
        "            Q = self.output(x)\n",
        "\n",
        "        return Q\n",
        "\n",
        "\n",
        "\n",
        "state_dim = 12\n",
        "action_dim = 2\n",
        "net = DQN(state_dim, action_dim)\n",
        "state = torch.randn(10, state_dim)\n",
        "output = net(state)\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "JIKqDwFJWoJd",
        "outputId": "16241151-d8d4-45e9-fa54-d1879b57bbc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 2.1688e-02, -4.2941e-01],\n",
            "        [-2.0843e-01, -3.2929e-01],\n",
            "        [-1.3379e-01, -2.4609e-01],\n",
            "        [-9.1848e-02, -2.1786e-01],\n",
            "        [ 6.8156e-02, -4.6601e-01],\n",
            "        [ 1.8730e-02, -3.6038e-01],\n",
            "        [-1.1307e-04, -2.5051e-01],\n",
            "        [-1.8811e-01, -2.2911e-01],\n",
            "        [-1.1324e-01, -2.9127e-01],\n",
            "        [-1.3768e-01, -4.9660e-01]], grad_fn=<SubBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "import random\n",
        "\n",
        "class ReplayMemory():\n",
        "    def __init__(self, maxlen, seed=0):\n",
        "        self.memory = deque([], maxlen=maxlen)\n",
        "\n",
        "        if seed is not None:\n",
        "            random.seed(seed)\n",
        "\n",
        "\n",
        "    def append(self, transition):\n",
        "        self.memory.append(transition)\n",
        "\n",
        "    def sample(self, sample_size):\n",
        "        return random.sample(self.memory, sample_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "vHv8un-GadDj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "import yaml\n",
        "\n",
        "\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "import argparse\n",
        "import itertools\n",
        "\n",
        "import flappy_bird_gymnasium\n",
        "import os\n",
        "\n",
        "# For printing date and time\n",
        "DATE_FORMAT = \"%m-%d %H:%M:%S\"\n",
        "\n",
        "# Directory for saving run info\n",
        "RUNS_DIR = \"runs\"\n",
        "os.makedirs(RUNS_DIR, exist_ok=True)\n",
        "\n",
        "# 'Agg': used to generate plots as images and save them to a file instead of rendering to screen\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = 'cpu' # force cpu, sometimes GPU not always faster than CPU due to overhead of moving data to GPU\n",
        "\n",
        "# Deep Q-Learning Agent\n",
        "class Agent():\n",
        "\n",
        "    def __init__(self, hyperparameter_set):\n",
        "        with open('hyperparameters.yml', 'r') as file:\n",
        "            all_hyperparameter_sets = yaml.safe_load(file)\n",
        "            hyperparameters = all_hyperparameter_sets[hyperparameter_set]\n",
        "            # print(hyperparameters)\n",
        "\n",
        "        self.hyperparameter_set = hyperparameter_set\n",
        "\n",
        "        # Hyperparameters (adjustable)\n",
        "        self.env_id             = hyperparameters['env_id']\n",
        "        self.learning_rate_a    = hyperparameters['learning_rate_a']        # learning rate (alpha)\n",
        "        self.discount_factor_g  = hyperparameters['discount_factor_g']      # discount rate (gamma)\n",
        "        self.network_sync_rate  = hyperparameters['network_sync_rate']      # number of steps the agent takes before syncing the policy and target network\n",
        "        self.replay_memory_size = hyperparameters['replay_memory_size']     # size of replay memory\n",
        "        self.mini_batch_size    = hyperparameters['mini_batch_size']        # size of the training data set sampled from the replay memory\n",
        "        self.epsilon_init       = hyperparameters['epsilon_init']           # 1 = 100% random actions\n",
        "        self.epsilon_decay      = hyperparameters['epsilon_decay']          # epsilon decay rate\n",
        "        self.epsilon_min        = hyperparameters['epsilon_min']            # minimum epsilon value\n",
        "        self.stop_on_reward     = hyperparameters['stop_on_reward']         # stop training after reaching this number of rewards\n",
        "        self.fc1_nodes          = hyperparameters['fc1_nodes']\n",
        "        self.env_make_params    = hyperparameters.get('env_make_params',{}) # Get optional environment-specific parameters, default to empty dict\n",
        "        self.enable_double_dqn  = hyperparameters['enable_double_dqn']      # double dqn on/off flag\n",
        "        self.enable_dueling_dqn = hyperparameters['enable_dueling_dqn']     # dueling dqn on/off flag\n",
        "\n",
        "        # Neural Network\n",
        "        self.loss_fn = nn.MSELoss()          # NN Loss function. MSE=Mean Squared Error can be swapped to something else.\n",
        "        self.optimizer = None                # NN Optimizer. Initialize later.\n",
        "\n",
        "        # Path to Run info\n",
        "        self.LOG_FILE   = os.path.join(RUNS_DIR, f'{self.hyperparameter_set}.log')\n",
        "        self.MODEL_FILE = os.path.join(RUNS_DIR, f'{self.hyperparameter_set}.pt')\n",
        "        self.GRAPH_FILE = os.path.join(RUNS_DIR, f'{self.hyperparameter_set}.png')\n",
        "\n",
        "    def run(self, is_training=True, render=False):\n",
        "        if is_training:\n",
        "            start_time = datetime.now()\n",
        "            last_graph_update_time = start_time\n",
        "\n",
        "            log_message = f\"{start_time.strftime(DATE_FORMAT)}: Training starting...\"\n",
        "            print(log_message)\n",
        "            with open(self.LOG_FILE, 'w') as file:\n",
        "                file.write(log_message + '\\n')\n",
        "\n",
        "        # Create instance of the environment.\n",
        "        # Use \"**self.env_make_params\" to pass in environment-specific parameters from hyperparameters.yml.\n",
        "        env = gym.make(self.env_id, render_mode='human' if render else None, **self.env_make_params)\n",
        "\n",
        "        # Number of possible actions\n",
        "        num_actions = env.action_space.n\n",
        "\n",
        "        # Get observation space size\n",
        "        num_states = env.observation_space.shape[0] # Expecting type: Box(low, high, (shape0,), float64)\n",
        "\n",
        "        # List to keep track of rewards collected per episode.\n",
        "        rewards_per_episode = []\n",
        "\n",
        "        # Create policy and target network. Number of nodes in the hidden layer can be adjusted.\n",
        "        policy_dqn = DQN(num_states, num_actions, self.fc1_nodes, self.enable_dueling_dqn).to(device)\n",
        "\n",
        "        if is_training:\n",
        "            # Initialize epsilon\n",
        "            epsilon = self.epsilon_init\n",
        "\n",
        "            # Initialize replay memory\n",
        "            memory = ReplayMemory(self.replay_memory_size)\n",
        "\n",
        "            # Create the target network and make it identical to the policy network\n",
        "            target_dqn = DQN(num_states, num_actions, self.fc1_nodes, self.enable_dueling_dqn).to(device)\n",
        "            target_dqn.load_state_dict(policy_dqn.state_dict())\n",
        "\n",
        "            # Policy network optimizer. \"Adam\" optimizer can be swapped to something else.\n",
        "            self.optimizer = torch.optim.Adam(policy_dqn.parameters(), lr=self.learning_rate_a)\n",
        "\n",
        "            # List to keep track of epsilon decay\n",
        "            epsilon_history = []\n",
        "\n",
        "            # Track number of steps taken. Used for syncing policy => target network.\n",
        "            step_count=0\n",
        "\n",
        "            # Track best reward\n",
        "            best_reward = -9999999\n",
        "        else:\n",
        "            # Load learned policy\n",
        "            policy_dqn.load_state_dict(torch.load(self.MODEL_FILE))\n",
        "\n",
        "            # switch model to evaluation mode\n",
        "            policy_dqn.eval()\n",
        "\n",
        "        # Train INDEFINITELY, manually stop the run when you are satisfied (or unsatisfied) with the results\n",
        "        for episode in itertools.count():\n",
        "\n",
        "            state, _ = env.reset()  # Initialize environment. Reset returns (state,info).\n",
        "            state = torch.tensor(state, dtype=torch.float, device=device) # Convert state to tensor directly on device\n",
        "\n",
        "            terminated = False      # True when agent reaches goal or fails\n",
        "            episode_reward = 0.0    # Used to accumulate rewards per episode\n",
        "\n",
        "            # Perform actions until episode terminates or reaches max rewards\n",
        "            # (on some envs, it is possible for the agent to train to a point where it NEVER terminates, so stop on reward is necessary)\n",
        "            while(not terminated and episode_reward < self.stop_on_reward):\n",
        "\n",
        "                # Select action based on epsilon-greedy\n",
        "                if is_training and random.random() < epsilon:\n",
        "                    # select random action\n",
        "                    action = env.action_space.sample()\n",
        "                    action = torch.tensor(action, dtype=torch.int64, device=device)\n",
        "                else:\n",
        "                    # select best action\n",
        "                    with torch.no_grad():\n",
        "                        # state.unsqueeze(dim=0): Pytorch expects a batch layer, so add batch dimension i.e. tensor([1, 2, 3]) unsqueezes to tensor([[1, 2, 3]])\n",
        "                        # policy_dqn returns tensor([[1], [2], [3]]), so squeeze it to tensor([1, 2, 3]).\n",
        "                        # argmax finds the index of the largest element.\n",
        "                        action = policy_dqn(state.unsqueeze(dim=0)).squeeze().argmax()\n",
        "\n",
        "                # Execute action. Truncated and info is not used.\n",
        "                new_state,reward,terminated,truncated,info = env.step(action.item())\n",
        "\n",
        "                # Accumulate rewards\n",
        "                episode_reward += reward\n",
        "\n",
        "                # Convert new state and reward to tensors on device\n",
        "                new_state = torch.tensor(new_state, dtype=torch.float, device=device)\n",
        "                reward = torch.tensor(reward, dtype=torch.float, device=device)\n",
        "\n",
        "                if is_training:\n",
        "                    # Save experience into memory\n",
        "                    memory.append((state, action, new_state, reward, terminated))\n",
        "\n",
        "                    # Increment step counter\n",
        "                    step_count+=1\n",
        "\n",
        "                # Move to the next state\n",
        "                state = new_state\n",
        "\n",
        "            # Keep track of the rewards collected per episode.\n",
        "            rewards_per_episode.append(episode_reward)\n",
        "\n",
        "            # Save model when new best reward is obtained.\n",
        "            if is_training:\n",
        "                if episode_reward > best_reward:\n",
        "                    log_message = f\"{datetime.now().strftime(DATE_FORMAT)}: New best reward {episode_reward:0.1f} ({(episode_reward-best_reward)/best_reward*100:+.1f}%) at episode {episode}, saving model...\"\n",
        "                    print(log_message)\n",
        "                    with open(self.LOG_FILE, 'a') as file:\n",
        "                        file.write(log_message + '\\n')\n",
        "\n",
        "                    torch.save(policy_dqn.state_dict(), self.MODEL_FILE)\n",
        "                    best_reward = episode_reward\n",
        "\n",
        "\n",
        "                # Update graph every x seconds\n",
        "                current_time = datetime.now()\n",
        "                if current_time - last_graph_update_time > timedelta(seconds=10):\n",
        "                    self.save_graph(rewards_per_episode, epsilon_history)\n",
        "                    last_graph_update_time = current_time\n",
        "\n",
        "                # If enough experience has been collected\n",
        "                if len(memory)>self.mini_batch_size:\n",
        "                    mini_batch = memory.sample(self.mini_batch_size)\n",
        "                    self.optimize(mini_batch, policy_dqn, target_dqn)\n",
        "\n",
        "                    # Decay epsilon\n",
        "                    epsilon = max(epsilon * self.epsilon_decay, self.epsilon_min)\n",
        "                    epsilon_history.append(epsilon)\n",
        "\n",
        "                    # Copy policy network to target network after a certain number of steps\n",
        "                    if step_count > self.network_sync_rate:\n",
        "                        target_dqn.load_state_dict(policy_dqn.state_dict())\n",
        "                        step_count=0\n",
        "\n",
        "\n",
        "    def save_graph(self, rewards_per_episode, epsilon_history):\n",
        "        # Save plots\n",
        "        fig = plt.figure(1)\n",
        "\n",
        "        # Plot average rewards (Y-axis) vs episodes (X-axis)\n",
        "        mean_rewards = np.zeros(len(rewards_per_episode))\n",
        "        for x in range(len(mean_rewards)):\n",
        "            mean_rewards[x] = np.mean(rewards_per_episode[max(0, x-99):(x+1)])\n",
        "        plt.subplot(121) # plot on a 1 row x 2 col grid, at cell 1\n",
        "        # plt.xlabel('Episodes')\n",
        "        plt.ylabel('Mean Rewards')\n",
        "        plt.plot(mean_rewards)\n",
        "\n",
        "        # Plot epsilon decay (Y-axis) vs episodes (X-axis)\n",
        "        plt.subplot(122) # plot on a 1 row x 2 col grid, at cell 2\n",
        "        # plt.xlabel('Time Steps')\n",
        "        plt.ylabel('Epsilon Decay')\n",
        "        plt.plot(epsilon_history)\n",
        "\n",
        "        plt.subplots_adjust(wspace=1.0, hspace=1.0)\n",
        "\n",
        "        # Save plots\n",
        "        fig.savefig(self.GRAPH_FILE)\n",
        "        plt.close(fig)\n",
        "\n",
        "\n",
        "    # Optimize policy network\n",
        "    def optimize(self, mini_batch, policy_dqn, target_dqn):\n",
        "\n",
        "        # Transpose the list of experiences and separate each element\n",
        "        states, actions, new_states, rewards, terminations = zip(*mini_batch)\n",
        "\n",
        "        # Stack tensors to create batch tensors\n",
        "        # tensor([[1,2,3]])\n",
        "        states = torch.stack(states)\n",
        "\n",
        "        actions = torch.stack(actions)\n",
        "\n",
        "        new_states = torch.stack(new_states)\n",
        "\n",
        "        rewards = torch.stack(rewards)\n",
        "        terminations = torch.tensor(terminations).float().to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if self.enable_double_dqn:\n",
        "                best_actions_from_policy = policy_dqn(new_states).argmax(dim=1)\n",
        "\n",
        "                target_q = rewards + (1-terminations) * self.discount_factor_g * \\\n",
        "                                target_dqn(new_states).gather(dim=1, index=best_actions_from_policy.unsqueeze(dim=1)).squeeze()\n",
        "            else:\n",
        "                # Calculate target Q values (expected returns)\n",
        "                target_q = rewards + (1-terminations) * self.discount_factor_g * target_dqn(new_states).max(dim=1)[0]\n",
        "                '''\n",
        "                    target_dqn(new_states)  ==> tensor([[1,2,3],[4,5,6]])\n",
        "                        .max(dim=1)         ==> torch.return_types.max(values=tensor([3,6]), indices=tensor([3, 0, 0, 1]))\n",
        "                            [0]             ==> tensor([3,6])\n",
        "                '''\n",
        "\n",
        "        # Calcuate Q values from current policy\n",
        "        current_q = policy_dqn(states).gather(dim=1, index=actions.unsqueeze(dim=1)).squeeze()\n",
        "        '''\n",
        "            policy_dqn(states)  ==> tensor([[1,2,3],[4,5,6]])\n",
        "                actions.unsqueeze(dim=1)\n",
        "                .gather(1, actions.unsqueeze(dim=1))  ==>\n",
        "                    .squeeze()                    ==>\n",
        "        '''\n",
        "\n",
        "        # Compute loss\n",
        "        loss = self.loss_fn(current_q, target_q)\n",
        "\n",
        "        # Optimize the model (backpropagation)\n",
        "        self.optimizer.zero_grad()  # Clear gradients\n",
        "        loss.backward()             # Compute gradients\n",
        "        self.optimizer.step()       # Update network parameters i.e. weights and biases\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     # Parse command line inputs\n",
        "#     parser = argparse.ArgumentParser(description='Train or test model.')\n",
        "#     parser.add_argument('hyperparameters', help='')\n",
        "#     parser.add_argument('--train', help='Training mode', action='store_true')\n",
        "#     args = parser.parse_args()\n",
        "\n",
        "#     dql = Agent(hyperparameter_set=args.hyperparameters)\n",
        "\n",
        "#     if args.train:\n",
        "#         dql.run(is_training=True)\n",
        "#     else:\n",
        "#         dql.run(is_training=False, render=True)\n",
        "\n",
        "dql = Agent(hyperparameter_set='flappybird1')\n",
        "\n",
        "dql.run(is_training=True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Tcw41QA44Lyq",
        "outputId": "6461708d-cd35-453a-ff6b-7ef82586cafc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05-23 11:23:28: Training starting...\n",
            "05-23 11:23:36: New best reward -5.7 (-100.0%) at episode 0, saving model...\n",
            "05-23 11:23:36: New best reward -5.1 (-10.5%) at episode 31, saving model...\n",
            "05-23 11:23:37: New best reward -4.5 (-11.8%) at episode 74, saving model...\n",
            "05-23 11:23:38: New best reward -3.9 (-13.3%) at episode 124, saving model...\n",
            "05-23 11:23:38: New best reward -3.3 (-15.4%) at episode 161, saving model...\n",
            "05-23 11:23:46: New best reward -2.1 (-36.4%) at episode 634, saving model...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-88179ce97ca2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0mdql\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'flappybird1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m \u001b[0mdql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-33-88179ce97ca2>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, is_training, render)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mmini_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_dqn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_dqn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0;31m# Decay epsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-88179ce97ca2>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, mini_batch, policy_dqn, target_dqn)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Clear gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m# Compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# Update network parameters i.e. weights and biases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;31m# if __name__ == '__main__':\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m                             )\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    242\u001b[0m             )\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    245\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    877\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    474\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/BardRimon/Study/main/Reinforcement_Learning/HW2/runs/"
      ],
      "metadata": {
        "id": "o_0io2GrNe4K",
        "outputId": "d341846c-5124-4839-f26d-d5763c9bdaba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-23 12:56:33--  https://raw.githubusercontent.com/BardRimon/Study/main/Reinforcement_Learning/HW2/runs/\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 400 Bad Request\n",
            "2025-05-23 12:56:33 ERROR 400: Bad Request.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OwX940XrL-dc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-GqyHIrLPRxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Image\n",
        "image_path = '/content/runs/flappybird1.png'\n",
        "display(Image(filename=image_path))"
      ],
      "metadata": {
        "id": "Gta9xoJTPLxa",
        "outputId": "72132685-1a36-456d-8059-4126c4e3c677",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MhlkZN8qPYft"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}