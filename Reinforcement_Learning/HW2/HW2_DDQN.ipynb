{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPjdG4xcVhlA8OexsxG8+L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BardRimon/Study/blob/main/Reinforcement_Learning/HW2/HW2_DDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flappy-bird-gymnasium"
      ],
      "metadata": {
        "id": "ZwNaNFkwrBLd",
        "outputId": "088b133a-9030-49e4-ea6c-cb46911a2e25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flappy-bird-gymnasium\n",
            "  Downloading flappy_bird_gymnasium-0.4.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (from flappy-bird-gymnasium) (1.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from flappy-bird-gymnasium) (2.0.2)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (from flappy-bird-gymnasium) (2.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from flappy-bird-gymnasium) (3.10.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium->flappy-bird-gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium->flappy-bird-gymnasium) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium->flappy-bird-gymnasium) (0.0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->flappy-bird-gymnasium) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->flappy-bird-gymnasium) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->flappy-bird-gymnasium) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->flappy-bird-gymnasium) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->flappy-bird-gymnasium) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->flappy-bird-gymnasium) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->flappy-bird-gymnasium) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->flappy-bird-gymnasium) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->flappy-bird-gymnasium) (1.17.0)\n",
            "Downloading flappy_bird_gymnasium-0.4.0-py3-none-any.whl (37.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: flappy-bird-gymnasium\n",
            "Successfully installed flappy-bird-gymnasium-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config_content = '''\n",
        "flappybird1:\n",
        "  env_id: FlappyBird-v0\n",
        "  replay_memory_size: 100000\n",
        "  mini_batch_size: 32\n",
        "  epsilon_init: 1\n",
        "  epsilon_decay: 0.99_99_5\n",
        "  epsilon_min: 0.05\n",
        "  network_sync_rate: 10\n",
        "  learning_rate_a: 0.0001\n",
        "  discount_factor_g: 0.99\n",
        "  stop_on_reward: 100000\n",
        "  fc1_nodes: 512\n",
        "  env_make_params:\n",
        "    use_lidar: False\n",
        "  enable_double_dqn: True\n",
        "  enable_dueling_dqn: True\n",
        "'''\n",
        "\n",
        "with open('hyperparameters.yml', 'w') as file:\n",
        "    file.write(config_content)"
      ],
      "metadata": {
        "id": "61o7GVhug86U"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config_content = '''\n",
        "{\n",
        "\n",
        "    \"version\": \"0.2.0\",\n",
        "    \"configurations\": [\n",
        "        {\n",
        "            \"name\": \"Train cartpole1\",\n",
        "            \"type\": \"debugpy\",\n",
        "            \"request\": \"launch\",\n",
        "            \"program\": \"agent.py\",\n",
        "            \"console\": \"integratedTerminal\",\n",
        "            \"args\": [\"cartpole1\", \"--train\"]\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"Train flappybird1\",\n",
        "            \"type\": \"debugpy\",\n",
        "            \"request\": \"launch\",\n",
        "            \"program\": \"agent.py\",\n",
        "            \"console\": \"integratedTerminal\",\n",
        "            \"args\": [\"flappybird1\", \"--train\"]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "'''\n",
        "with open('launch.json', 'w') as file:\n",
        "    file.write(config_content)\n"
      ],
      "metadata": {
        "id": "93ZDvhaYwar5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5IE3g_9nqf2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# experience_rep\n",
        "from collections import deque\n",
        "import random\n",
        "class ReplayMemory():\n",
        "    def __init__(self, maxlen, seed=None):\n",
        "        self.memory = deque([], maxlen=maxlen)\n",
        "\n",
        "        # Optional seed for reproducibility\n",
        "        if seed is not None:\n",
        "            random.seed(seed)\n",
        "\n",
        "    def append(self, transition):\n",
        "        self.memory.append(transition)\n",
        "\n",
        "    def sample(self, sample_size):\n",
        "        return random.sample(self.memory, sample_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "DQRQTHDkhgbF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=256, enable_dueling_dqn=True):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.enable_dueling_dqn=enable_dueling_dqn\n",
        "\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
        "\n",
        "        if self.enable_dueling_dqn:\n",
        "            # Value stream\n",
        "            self.fc_value = nn.Linear(hidden_dim, 256)\n",
        "            self.value = nn.Linear(256, 1)\n",
        "\n",
        "            # Advantages stream\n",
        "            self.fc_advantages = nn.Linear(hidden_dim, 256)\n",
        "            self.advantages = nn.Linear(256, action_dim)\n",
        "\n",
        "        else:\n",
        "            self.output = nn.Linear(hidden_dim, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        if self.enable_dueling_dqn:\n",
        "            # Value calc\n",
        "            v = F.relu(self.fc_value(x))\n",
        "            V = self.value(v)\n",
        "\n",
        "            # Advantages calc\n",
        "            a = F.relu(self.fc_advantages(x))\n",
        "            A = self.advantages(a)\n",
        "\n",
        "            # Calc Q\n",
        "            Q = V + A - torch.mean(A, dim=1, keepdim=True)\n",
        "\n",
        "        else:\n",
        "            Q = self.output(x)\n",
        "\n",
        "        return Q\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "    # state_dim = 12\n",
        "    # action_dim = 2\n",
        "    # net = DQN(state_dim, action_dim)\n",
        "    # state = torch.randn(10, state_dim)\n",
        "    # output = net(state)\n",
        "    # print(output)\n",
        "# state_dim = 12\n",
        "# action_dim = 2\n",
        "# net = DQN(state_dim, action_dim)\n",
        "# state = torch.randn(10, state_dim)\n",
        "# output = net(state)\n",
        "# print(output)"
      ],
      "metadata": {
        "id": "5npYVrDCqlGH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "import yaml\n",
        "\n",
        "# from experience_replay import ReplayMemory\n",
        "# from dqn import DQN\n",
        "\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "import argparse\n",
        "import itertools\n",
        "\n",
        "import flappy_bird_gymnasium\n",
        "import os\n",
        "\n",
        "# For printing date and time\n",
        "DATE_FORMAT = \"%m-%d %H:%M:%S\"\n",
        "\n",
        "# Directory for saving run info\n",
        "RUNS_DIR = \"runs\"\n",
        "os.makedirs(RUNS_DIR, exist_ok=True)\n",
        "\n",
        "# 'Agg': used to generate plots as images and save them to a file instead of rendering to screen\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = 'cpu' # force cpu, sometimes GPU not always faster than CPU due to overhead of moving data to GPU\n",
        "\n",
        "# Deep Q-Learning Agent\n",
        "class Agent():\n",
        "\n",
        "    def __init__(self, hyperparameter_set):\n",
        "        with open('hyperparameters.yml', 'r') as file:\n",
        "            all_hyperparameter_sets = yaml.safe_load(file)\n",
        "            # hyperparameters = all_hyperparameter_sets[hyperparameter_set]\n",
        "            hyperparameters = hyperparameter_set\n",
        "            # print(hyperparameters)\n",
        "\n",
        "        self.hyperparameter_set = hyperparameter_set\n",
        "\n",
        "        # Hyperparameters (adjustable)\n",
        "        self.env_id             = hyperparameters['env_id']\n",
        "        self.learning_rate_a    = hyperparameters['learning_rate_a']        # learning rate (alpha)\n",
        "        self.discount_factor_g  = hyperparameters['discount_factor_g']      # discount rate (gamma)\n",
        "        self.network_sync_rate  = hyperparameters['network_sync_rate']      # number of steps the agent takes before syncing the policy and target network\n",
        "        self.replay_memory_size = hyperparameters['replay_memory_size']     # size of replay memory\n",
        "        self.mini_batch_size    = hyperparameters['mini_batch_size']        # size of the training data set sampled from the replay memory\n",
        "        self.epsilon_init       = hyperparameters['epsilon_init']           # 1 = 100% random actions\n",
        "        self.epsilon_decay      = hyperparameters['epsilon_decay']          # epsilon decay rate\n",
        "        self.epsilon_min        = hyperparameters['epsilon_min']            # minimum epsilon value\n",
        "        self.stop_on_reward     = hyperparameters['stop_on_reward']         # stop training after reaching this number of rewards\n",
        "        self.fc1_nodes          = hyperparameters['fc1_nodes']\n",
        "        self.env_make_params    = hyperparameters.get('env_make_params',{}) # Get optional environment-specific parameters, default to empty dict\n",
        "        self.enable_double_dqn  = hyperparameters['enable_double_dqn']      # double dqn on/off flag\n",
        "        self.enable_dueling_dqn = hyperparameters['enable_dueling_dqn']     # dueling dqn on/off flag\n",
        "\n",
        "        # Neural Network\n",
        "        self.loss_fn = nn.MSELoss()          # NN Loss function. MSE=Mean Squared Error can be swapped to something else.\n",
        "        self.optimizer = None                # NN Optimizer. Initialize later.\n",
        "\n",
        "        # Path to Run info\n",
        "        self.LOG_FILE   = os.path.join(RUNS_DIR, f'{self.hyperparameter_set}.log')\n",
        "        self.MODEL_FILE = os.path.join(RUNS_DIR, f'{self.hyperparameter_set}.pt')\n",
        "        self.GRAPH_FILE = os.path.join(RUNS_DIR, f'{self.hyperparameter_set}.png')\n",
        "\n",
        "    def run(self, is_training=True, render=False):\n",
        "        if is_training:\n",
        "            start_time = datetime.now()\n",
        "            last_graph_update_time = start_time\n",
        "\n",
        "            log_message = f\"{start_time.strftime(DATE_FORMAT)}: Training starting...\"\n",
        "            print(log_message)\n",
        "            with open(self.LOG_FILE, 'w') as file:\n",
        "                file.write(log_message + '\\n')\n",
        "\n",
        "        # Create instance of the environment.\n",
        "        # Use \"**self.env_make_params\" to pass in environment-specific parameters from hyperparameters.yml.\n",
        "        env = gym.make(self.env_id, render_mode='human' if render else None, **self.env_make_params)\n",
        "\n",
        "        # Number of possible actions\n",
        "        num_actions = env.action_space.n\n",
        "\n",
        "        # Get observation space size\n",
        "        num_states = env.observation_space.shape[0] # Expecting type: Box(low, high, (shape0,), float64)\n",
        "\n",
        "        # List to keep track of rewards collected per episode.\n",
        "        rewards_per_episode = []\n",
        "\n",
        "        # Create policy and target network. Number of nodes in the hidden layer can be adjusted.\n",
        "        policy_dqn = DQN(num_states, num_actions, self.fc1_nodes, self.enable_dueling_dqn).to(device)\n",
        "\n",
        "        if is_training:\n",
        "            # Initialize epsilon\n",
        "            epsilon = self.epsilon_init\n",
        "\n",
        "            # Initialize replay memory\n",
        "            memory = ReplayMemory(self.replay_memory_size)\n",
        "\n",
        "            # Create the target network and make it identical to the policy network\n",
        "            target_dqn = DQN(num_states, num_actions, self.fc1_nodes, self.enable_dueling_dqn).to(device)\n",
        "            target_dqn.load_state_dict(policy_dqn.state_dict())\n",
        "\n",
        "            # Policy network optimizer. \"Adam\" optimizer can be swapped to something else.\n",
        "            self.optimizer = torch.optim.Adam(policy_dqn.parameters(), lr=self.learning_rate_a)\n",
        "\n",
        "            # List to keep track of epsilon decay\n",
        "            epsilon_history = []\n",
        "\n",
        "            # Track number of steps taken. Used for syncing policy => target network.\n",
        "            step_count=0\n",
        "\n",
        "            # Track best reward\n",
        "            best_reward = -9999999\n",
        "        else:\n",
        "            # Load learned policy\n",
        "            policy_dqn.load_state_dict(torch.load(self.MODEL_FILE))\n",
        "\n",
        "            # switch model to evaluation mode\n",
        "            policy_dqn.eval()\n",
        "\n",
        "        # Train INDEFINITELY, manually stop the run when you are satisfied (or unsatisfied) with the results\n",
        "        for episode in itertools.count():\n",
        "\n",
        "            state, _ = env.reset()  # Initialize environment. Reset returns (state,info).\n",
        "            state = torch.tensor(state, dtype=torch.float, device=device) # Convert state to tensor directly on device\n",
        "\n",
        "            terminated = False      # True when agent reaches goal or fails\n",
        "            episode_reward = 0.0    # Used to accumulate rewards per episode\n",
        "\n",
        "            # Perform actions until episode terminates or reaches max rewards\n",
        "            # (on some envs, it is possible for the agent to train to a point where it NEVER terminates, so stop on reward is necessary)\n",
        "            while(not terminated and episode_reward < self.stop_on_reward):\n",
        "\n",
        "                # Select action based on epsilon-greedy\n",
        "                if is_training and random.random() < epsilon:\n",
        "                    # select random action\n",
        "                    action = env.action_space.sample()\n",
        "                    action = torch.tensor(action, dtype=torch.int64, device=device)\n",
        "                else:\n",
        "                    # select best action\n",
        "                    with torch.no_grad():\n",
        "                        # state.unsqueeze(dim=0): Pytorch expects a batch layer, so add batch dimension i.e. tensor([1, 2, 3]) unsqueezes to tensor([[1, 2, 3]])\n",
        "                        # policy_dqn returns tensor([[1], [2], [3]]), so squeeze it to tensor([1, 2, 3]).\n",
        "                        # argmax finds the index of the largest element.\n",
        "                        action = policy_dqn(state.unsqueeze(dim=0)).squeeze().argmax()\n",
        "\n",
        "                # Execute action. Truncated and info is not used.\n",
        "                new_state,reward,terminated,truncated,info = env.step(action.item())\n",
        "\n",
        "                # Accumulate rewards\n",
        "                episode_reward += reward\n",
        "\n",
        "                # Convert new state and reward to tensors on device\n",
        "                new_state = torch.tensor(new_state, dtype=torch.float, device=device)\n",
        "                reward = torch.tensor(reward, dtype=torch.float, device=device)\n",
        "\n",
        "                if is_training:\n",
        "                    # Save experience into memory\n",
        "                    memory.append((state, action, new_state, reward, terminated))\n",
        "\n",
        "                    # Increment step counter\n",
        "                    step_count+=1\n",
        "\n",
        "                # Move to the next state\n",
        "                state = new_state\n",
        "\n",
        "            # Keep track of the rewards collected per episode.\n",
        "            rewards_per_episode.append(episode_reward)\n",
        "\n",
        "            # Save model when new best reward is obtained.\n",
        "            if is_training:\n",
        "                if episode_reward > best_reward:\n",
        "                    log_message = f\"{datetime.now().strftime(DATE_FORMAT)}: New best reward {episode_reward:0.1f} ({(episode_reward-best_reward)/best_reward*100:+.1f}%) at episode {episode}, saving model...\"\n",
        "                    print(log_message)\n",
        "                    with open(self.LOG_FILE, 'a') as file:\n",
        "                        file.write(log_message + '\\n')\n",
        "\n",
        "                    torch.save(policy_dqn.state_dict(), self.MODEL_FILE)\n",
        "                    best_reward = episode_reward\n",
        "\n",
        "\n",
        "                # Update graph every x seconds\n",
        "                current_time = datetime.now()\n",
        "                if current_time - last_graph_update_time > timedelta(seconds=10):\n",
        "                    self.save_graph(rewards_per_episode, epsilon_history)\n",
        "                    last_graph_update_time = current_time\n",
        "\n",
        "                # If enough experience has been collected\n",
        "                if len(memory)>self.mini_batch_size:\n",
        "                    mini_batch = memory.sample(self.mini_batch_size)\n",
        "                    self.optimize(mini_batch, policy_dqn, target_dqn)\n",
        "\n",
        "                    # Decay epsilon\n",
        "                    epsilon = max(epsilon * self.epsilon_decay, self.epsilon_min)\n",
        "                    epsilon_history.append(epsilon)\n",
        "\n",
        "                    # Copy policy network to target network after a certain number of steps\n",
        "                    if step_count > self.network_sync_rate:\n",
        "                        target_dqn.load_state_dict(policy_dqn.state_dict())\n",
        "                        step_count=0\n",
        "\n",
        "\n",
        "    def save_graph(self, rewards_per_episode, epsilon_history):\n",
        "        # Save plots\n",
        "        fig = plt.figure(1)\n",
        "\n",
        "        # Plot average rewards (Y-axis) vs episodes (X-axis)\n",
        "        mean_rewards = np.zeros(len(rewards_per_episode))\n",
        "        for x in range(len(mean_rewards)):\n",
        "            mean_rewards[x] = np.mean(rewards_per_episode[max(0, x-99):(x+1)])\n",
        "        plt.subplot(121) # plot on a 1 row x 2 col grid, at cell 1\n",
        "        # plt.xlabel('Episodes')\n",
        "        plt.ylabel('Mean Rewards')\n",
        "        plt.plot(mean_rewards)\n",
        "\n",
        "        # Plot epsilon decay (Y-axis) vs episodes (X-axis)\n",
        "        plt.subplot(122) # plot on a 1 row x 2 col grid, at cell 2\n",
        "        # plt.xlabel('Time Steps')\n",
        "        plt.ylabel('Epsilon Decay')\n",
        "        plt.plot(epsilon_history)\n",
        "\n",
        "        plt.subplots_adjust(wspace=1.0, hspace=1.0)\n",
        "\n",
        "        # Save plots\n",
        "        fig.savefig(self.GRAPH_FILE)\n",
        "        plt.close(fig)\n",
        "\n",
        "\n",
        "    # Optimize policy network\n",
        "    def optimize(self, mini_batch, policy_dqn, target_dqn):\n",
        "\n",
        "        # Transpose the list of experiences and separate each element\n",
        "        states, actions, new_states, rewards, terminations = zip(*mini_batch)\n",
        "\n",
        "        # Stack tensors to create batch tensors\n",
        "        # tensor([[1,2,3]])\n",
        "        states = torch.stack(states)\n",
        "\n",
        "        actions = torch.stack(actions)\n",
        "\n",
        "        new_states = torch.stack(new_states)\n",
        "\n",
        "        rewards = torch.stack(rewards)\n",
        "        terminations = torch.tensor(terminations).float().to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if self.enable_double_dqn:\n",
        "                best_actions_from_policy = policy_dqn(new_states).argmax(dim=1)\n",
        "\n",
        "                target_q = rewards + (1-terminations) * self.discount_factor_g * \\\n",
        "                                target_dqn(new_states).gather(dim=1, index=best_actions_from_policy.unsqueeze(dim=1)).squeeze()\n",
        "            else:\n",
        "                # Calculate target Q values (expected returns)\n",
        "                target_q = rewards + (1-terminations) * self.discount_factor_g * target_dqn(new_states).max(dim=1)[0]\n",
        "                '''\n",
        "                    target_dqn(new_states)  ==> tensor([[1,2,3],[4,5,6]])\n",
        "                        .max(dim=1)         ==> torch.return_types.max(values=tensor([3,6]), indices=tensor([3, 0, 0, 1]))\n",
        "                            [0]             ==> tensor([3,6])\n",
        "                '''\n",
        "\n",
        "        # Calcuate Q values from current policy\n",
        "        current_q = policy_dqn(states).gather(dim=1, index=actions.unsqueeze(dim=1)).squeeze()\n",
        "        '''\n",
        "            policy_dqn(states)  ==> tensor([[1,2,3],[4,5,6]])\n",
        "                actions.unsqueeze(dim=1)\n",
        "                .gather(1, actions.unsqueeze(dim=1))  ==>\n",
        "                    .squeeze()                    ==>\n",
        "        '''\n",
        "\n",
        "        # Compute loss\n",
        "        loss = self.loss_fn(current_q, target_q)\n",
        "\n",
        "        # Optimize the model (backpropagation)\n",
        "        self.optimizer.zero_grad()  # Clear gradients\n",
        "        loss.backward()             # Compute gradients\n",
        "        self.optimizer.step()       # Update network parameters i.e. weights and biases\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     # Parse command line inputs\n",
        "#     parser = argparse.ArgumentParser(description='train') # Train or test model.\n",
        "#     parser.add_argument('hyperparameters.yml', help='')\n",
        "#     parser.add_argument('--train', help='Training mode', action='store_true')\n",
        "#     args = parser.parse_args()\n",
        "\n",
        "#     dql = Agent(hyperparameter_set=args.hyperparameters)\n",
        "\n",
        "#     if args.train:\n",
        "#         dql.run(is_training=True)\n",
        "#     else:\n",
        "#         dql.run(is_training=False, render=True)\n",
        "\n",
        "\n",
        "# parser = argparse.ArgumentParser(description='train') # Train or test model.\n",
        "# parser.add_argument('hyperparameters', help='')\n",
        "# parser.add_argument('--train', help='Training mode', action='store_true')\n",
        "# args = parser.parse_args()\n",
        "\n",
        "hyp_p = {\n",
        "    'env_id': 'FlappyBird-v0',\n",
        "    'replay_memory_size': 100000,\n",
        "    'mini_batch_size': 32,\n",
        "    'epsilon_init': 1,\n",
        "    'epsilon_decay': '0.99_99_5',\n",
        "    'epsilon_min': 0.05,\n",
        "    'network_sync_rate': 10,\n",
        "    'learning_rate_a': 0.0001,\n",
        "    'discount_factor_g': 0.99,\n",
        "    'stop_on_reward': 100000,\n",
        "    'fc1_nodes': 512,\n",
        "    'env_make_params': {'use_lidar': False},\n",
        "    'enable_double_dqn': True,\n",
        "    'enable_dueling_dqn': True\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "dql = Agent(hyperparameter_set=hyp_p)\n",
        "\n",
        "\n",
        "train_flag = True\n",
        "\n",
        "if train_flag:\n",
        "    dql.run(is_training=True)\n",
        "else:\n",
        "    dql.run(is_training=False, render=True)"
      ],
      "metadata": {
        "id": "Fm8ajmR_qxDw",
        "outputId": "b3375abc-49a6-4520-c856-2a971570da08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "04-25 13:26:26: Training starting...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "[Errno 36] File name too long: \"runs/{'env_id': 'FlappyBird-v0', 'replay_memory_size': 100000, 'mini_batch_size': 32, 'epsilon_init': 1, 'epsilon_decay': '0.99_99_5', 'epsilon_min': 0.05, 'network_sync_rate': 10, 'learning_rate_a': 0.0001, 'discount_factor_g': 0.99, 'stop_on_reward': 100000, 'fc1_nodes': 512, 'env_make_params': {'use_lidar': False}, 'enable_double_dqn': True, 'enable_dueling_dqn': True}.log\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-182cc8be0068>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrain_flag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m     \u001b[0mdql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0mdql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-182cc8be0068>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, is_training, render)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mlog_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{start_time.strftime(DATE_FORMAT)}: Training starting...\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOG_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_message\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 36] File name too long: \"runs/{'env_id': 'FlappyBird-v0', 'replay_memory_size': 100000, 'mini_batch_size': 32, 'epsilon_init': 1, 'epsilon_decay': '0.99_99_5', 'epsilon_min': 0.05, 'network_sync_rate': 10, 'learning_rate_a': 0.0001, 'discount_factor_g': 0.99, 'stop_on_reward': 100000, 'fc1_nodes': 512, 'env_make_params': {'use_lidar': False}, 'enable_double_dqn': True, 'enable_dueling_dqn': True}.log\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6J-z2nIFGUWp",
        "outputId": "31bb95f1-5ada-4443-f248-d14a9417c86f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'hyperparameters' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-09c42f84a134>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconfig_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'hyperparameters' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "! pip install pyyaml"
      ],
      "metadata": {
        "id": "yTNkEhc7-V0v"
      }
    }
  ]
}
