{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac6451de-d992-474a-936a-1c90e54afa27",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle  # ⬅️ Этот импорт обязателен!\n",
    "import imageio\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbafe9f1-d891-4cfd-a54b-4a886b641f5c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ------------------ Пользовательская среда GridWorld ------------------\n",
    "class GridWorld:\n",
    "    def __init__(self, size=4):\n",
    "        self.size = size\n",
    "        self.state = (0, 0)\n",
    "        self.goal = (size - 1, size - 1)  # (x, y)\n",
    "        self.action_space = 4  # 0: up, 1: right, 2: down, 3: left\n",
    "        self.observation_space = size * size  # flat state space\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = (0, 0)\n",
    "        return self._to_flat(self.state)\n",
    "\n",
    "    def _to_flat(self, pos):\n",
    "        x, y = pos\n",
    "        return x * self.size + y\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y = self.state\n",
    "        if action == 0 and x > 0:\n",
    "            x -= 1\n",
    "        elif action == 1 and y < self.size - 1:\n",
    "            y += 1\n",
    "        elif action == 2 and x < self.size - 1:\n",
    "            x += 1\n",
    "        elif action == 3 and y > 0:\n",
    "            y -= 1\n",
    "\n",
    "        new_state = (x, y)\n",
    "        done = new_state == self.goal\n",
    "        reward = 1.0 if done else -0.01\n",
    "        self.state = new_state\n",
    "        return self._to_flat(new_state), reward, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a2ebf19-f01e-4934-951a-74431b8e9bbe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# ------------------ Функция преобразования индекса в координаты ------------------\n",
    "def idx_to_pos(idx, size):\n",
    "    return (idx // size, idx % size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c7aa12f-ff94-4a53-9bac-db64c8273153",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------ Визуализация текущего состояния и сохранение кадра ------------------\n",
    "def visualize_and_save_frame(env, state_idx, episode, step, frames_folder=\"frames\"):\n",
    "    grid_size = env.size\n",
    "    grid = np.zeros((grid_size, grid_size, 3))  # RGB\n",
    "\n",
    "    x, y = idx_to_pos(state_idx, grid_size)\n",
    "    gx, gy = env.goal  # цель как (x, y)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    ax.imshow(grid)\n",
    "\n",
    "    # Рисуем решётку\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            rect = Rectangle((j - 0.5, i - 0.5), 1, 1, fill=False, color=\"black\")\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "    # Агент\n",
    "    agent_rect = Rectangle((y - 0.4, x - 0.4), 0.8, 0.8, color=\"blue\", label=\"Agent\")\n",
    "    ax.add_patch(agent_rect)\n",
    "\n",
    "    # Цель\n",
    "    goal_rect = Rectangle((gy - 0.4, gx - 0.4), 0.8, 0.8, color=\"green\")\n",
    "    ax.add_patch(goal_rect)\n",
    "\n",
    "    ax.set_title(f\"Episode {episode} | Step {step}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # Сохраняем кадр\n",
    "    frame_path = os.path.join(frames_folder, f\"frame_{step:03d}.png\")\n",
    "    plt.savefig(frame_path)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23f62ca8-3cc6-423b-89c0-f179fadbc5ee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# ------------------ Модель A2C ------------------\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.shared = nn.Linear(num_inputs, 64)\n",
    "        self.actor = nn.Linear(64, num_actions)\n",
    "        self.critic = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.shared(x))\n",
    "        return self.actor(x), self.critic(x)\n",
    "\n",
    "\n",
    "# ------------------ Гиперпараметры ------------------\n",
    "env = GridWorld()\n",
    "state_size = env.observation_space\n",
    "action_size = env.action_space\n",
    "n_episodes = 100\n",
    "gamma = 0.99\n",
    "lr = 3e-3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ActorCritic(state_size, action_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d8682ac-053d-4b58-8e48-5c83a51870bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ------------------ Функция обучения ------------------\n",
    "def train():\n",
    "    win_history = []\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        transitions = []\n",
    "\n",
    "        while True:\n",
    "            # One-hot encoding\n",
    "            state_tensor = torch.zeros(1, state_size)\n",
    "            state_tensor[0, state] = 1.0\n",
    "            state_tensor = state_tensor.to(device)\n",
    "\n",
    "            logits, value = model(state_tensor)\n",
    "            policy = Categorical(logits=logits)\n",
    "            action = policy.sample()\n",
    "\n",
    "            next_state, reward, done = env.step(action.item())\n",
    "            total_reward += reward\n",
    "\n",
    "            transitions.append((state_tensor, action, reward, value, policy.log_prob(action)))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        # Обучение\n",
    "        loss = 0\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for t in reversed(range(len(transitions))):\n",
    "            R = transitions[t][2] + gamma * R\n",
    "            returns.insert(0, R)\n",
    "\n",
    "        returns = torch.tensor(returns).to(device)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        for i, (state_tensor, action, _, value, log_prob) in enumerate(transitions):\n",
    "            advantage = returns[i] - value.item()\n",
    "            actor_loss = -log_prob * advantage\n",
    "            critic_loss = F.mse_loss(value, returns[i].unsqueeze(0))\n",
    "            loss += actor_loss + critic_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Логирование побед\n",
    "        won = total_reward > 0.9\n",
    "        win_history.append(1 if won else 0)\n",
    "\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward:.2f}, {'✅ Win' if won else '❌ Lose'}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"a2c_gridworld.pth\")\n",
    "    print(\"Модель сохранена.\")\n",
    "    plot_wins(win_history)\n",
    "    save_video_from_agent(model, env)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34a162ef-6e9b-43c2-be96-09b56872278c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# ------------------ График с победами ------------------\n",
    "def plot_wins(wins):\n",
    "    wins_smooth = np.convolve(wins, np.ones(10)/10, mode='valid')\n",
    "    plt.plot(wins_smooth)\n",
    "    plt.title(\"Win Rate Over Episodes (Smoothed)\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Win Rate\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a69ee1b-2592-4de0-aa6e-3c620cb40880",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chirkova_aa\\AppData\\Local\\Temp\\ipykernel_24632\\589209778.py:43: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  critic_loss = F.mse_loss(value, returns[i].unsqueeze(0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: 0.90, ❌ Lose\n",
      "Episode 1, Total Reward: -0.87, ❌ Lose\n",
      "Episode 2, Total Reward: 0.13, ❌ Lose\n",
      "Episode 3, Total Reward: 0.34, ❌ Lose\n",
      "Episode 4, Total Reward: 0.45, ❌ Lose\n",
      "Episode 5, Total Reward: 0.67, ❌ Lose\n",
      "Episode 6, Total Reward: 0.77, ❌ Lose\n",
      "Episode 7, Total Reward: 0.16, ❌ Lose\n",
      "Episode 8, Total Reward: 0.39, ❌ Lose\n",
      "Episode 9, Total Reward: -0.53, ❌ Lose\n",
      "Episode 10, Total Reward: 0.54, ❌ Lose\n",
      "Episode 11, Total Reward: 0.75, ❌ Lose\n",
      "Episode 12, Total Reward: 0.60, ❌ Lose\n",
      "Episode 13, Total Reward: 0.95, ✅ Win\n",
      "Episode 14, Total Reward: 0.94, ✅ Win\n",
      "Episode 15, Total Reward: 0.74, ❌ Lose\n",
      "Episode 16, Total Reward: 0.86, ❌ Lose\n",
      "Episode 17, Total Reward: 0.92, ✅ Win\n",
      "Episode 18, Total Reward: 0.86, ❌ Lose\n",
      "Episode 19, Total Reward: 0.89, ❌ Lose\n",
      "Episode 20, Total Reward: 0.90, ❌ Lose\n",
      "Episode 21, Total Reward: 0.87, ❌ Lose\n",
      "Episode 22, Total Reward: 0.65, ❌ Lose\n",
      "Episode 23, Total Reward: 0.85, ❌ Lose\n",
      "Episode 24, Total Reward: 0.85, ❌ Lose\n",
      "Episode 25, Total Reward: 0.89, ❌ Lose\n",
      "Episode 26, Total Reward: 0.58, ❌ Lose\n",
      "Episode 27, Total Reward: 0.92, ✅ Win\n",
      "Episode 28, Total Reward: 0.46, ❌ Lose\n",
      "Episode 29, Total Reward: 0.81, ❌ Lose\n",
      "Episode 30, Total Reward: 0.76, ❌ Lose\n",
      "Episode 31, Total Reward: 0.82, ❌ Lose\n",
      "Episode 32, Total Reward: 0.89, ❌ Lose\n",
      "Episode 33, Total Reward: 0.89, ❌ Lose\n",
      "Episode 34, Total Reward: 0.89, ❌ Lose\n",
      "Episode 35, Total Reward: 0.76, ❌ Lose\n",
      "Episode 36, Total Reward: 0.90, ❌ Lose\n",
      "Episode 37, Total Reward: 0.82, ❌ Lose\n",
      "Episode 38, Total Reward: 0.74, ❌ Lose\n",
      "Episode 39, Total Reward: 0.93, ✅ Win\n",
      "Episode 40, Total Reward: 0.95, ✅ Win\n",
      "Episode 41, Total Reward: 0.74, ❌ Lose\n",
      "Episode 42, Total Reward: 0.95, ✅ Win\n",
      "Episode 43, Total Reward: 0.50, ❌ Lose\n",
      "Episode 44, Total Reward: 0.87, ❌ Lose\n",
      "Episode 45, Total Reward: 0.71, ❌ Lose\n",
      "Episode 46, Total Reward: 0.93, ✅ Win\n",
      "Episode 47, Total Reward: 0.94, ✅ Win\n",
      "Episode 48, Total Reward: 0.35, ❌ Lose\n",
      "Episode 49, Total Reward: 0.85, ❌ Lose\n",
      "Episode 50, Total Reward: 0.73, ❌ Lose\n",
      "Episode 51, Total Reward: 0.86, ❌ Lose\n",
      "Episode 52, Total Reward: 0.94, ✅ Win\n",
      "Episode 53, Total Reward: 0.82, ❌ Lose\n",
      "Episode 54, Total Reward: 0.95, ✅ Win\n",
      "Episode 55, Total Reward: 0.91, ✅ Win\n",
      "Episode 56, Total Reward: 0.92, ✅ Win\n",
      "Episode 57, Total Reward: 0.86, ❌ Lose\n",
      "Episode 58, Total Reward: 0.92, ✅ Win\n",
      "Episode 59, Total Reward: 0.92, ✅ Win\n",
      "Episode 60, Total Reward: 0.79, ❌ Lose\n",
      "Episode 61, Total Reward: 0.83, ❌ Lose\n",
      "Episode 62, Total Reward: 0.87, ❌ Lose\n",
      "Episode 63, Total Reward: 0.79, ❌ Lose\n",
      "Episode 64, Total Reward: 0.89, ❌ Lose\n",
      "Episode 65, Total Reward: 0.93, ✅ Win\n",
      "Episode 66, Total Reward: 0.81, ❌ Lose\n",
      "Episode 67, Total Reward: 0.79, ❌ Lose\n",
      "Episode 68, Total Reward: 0.73, ❌ Lose\n",
      "Episode 69, Total Reward: 0.91, ✅ Win\n",
      "Episode 70, Total Reward: 0.83, ❌ Lose\n",
      "Episode 71, Total Reward: 0.85, ❌ Lose\n",
      "Episode 72, Total Reward: 0.88, ❌ Lose\n",
      "Episode 73, Total Reward: 0.84, ❌ Lose\n",
      "Episode 74, Total Reward: 0.87, ❌ Lose\n",
      "Episode 75, Total Reward: 0.93, ✅ Win\n",
      "Episode 76, Total Reward: 0.80, ❌ Lose\n",
      "Episode 77, Total Reward: 0.91, ✅ Win\n",
      "Episode 78, Total Reward: 0.81, ❌ Lose\n",
      "Episode 79, Total Reward: 0.91, ✅ Win\n",
      "Episode 80, Total Reward: 0.95, ✅ Win\n",
      "Episode 81, Total Reward: 0.90, ❌ Lose\n",
      "Episode 82, Total Reward: 0.91, ✅ Win\n",
      "Episode 83, Total Reward: 0.80, ❌ Lose\n",
      "Episode 84, Total Reward: 0.94, ✅ Win\n",
      "Episode 85, Total Reward: 0.89, ❌ Lose\n",
      "Episode 86, Total Reward: 0.93, ✅ Win\n",
      "Episode 87, Total Reward: 0.87, ❌ Lose\n",
      "Episode 88, Total Reward: 0.88, ❌ Lose\n",
      "Episode 89, Total Reward: 0.86, ❌ Lose\n",
      "Episode 90, Total Reward: 0.93, ✅ Win\n",
      "Episode 91, Total Reward: 0.82, ❌ Lose\n",
      "Episode 92, Total Reward: 0.87, ❌ Lose\n",
      "Episode 93, Total Reward: 0.94, ✅ Win\n",
      "Episode 94, Total Reward: 0.90, ❌ Lose\n",
      "Episode 95, Total Reward: 0.95, ✅ Win\n",
      "Episode 96, Total Reward: 0.89, ❌ Lose\n",
      "Episode 97, Total Reward: 0.94, ✅ Win\n",
      "Episode 98, Total Reward: 0.93, ✅ Win\n",
      "Episode 99, Total Reward: 0.94, ✅ Win\n",
      "Модель сохранена.\n"
     ]
    }
   ],
   "source": [
    "# ------------------ Запись видео с агентом ------------------\n",
    "from matplotlib.patches import Rectangle\n",
    "def save_video_from_agent(model, env, video_name=\"agent_win.gif\", max_steps=100):\n",
    "    frames_folder = \"frames\"\n",
    "    os.makedirs(frames_folder, exist_ok=True)\n",
    "\n",
    "    model.eval()\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    step = 0\n",
    "\n",
    "    print(\"Запись видео с победой...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while not done and step < max_steps:\n",
    "            visualize_and_save_frame(env, state, 0, step, frames_folder)\n",
    "\n",
    "            state_tensor = torch.zeros(1, env.observation_space)\n",
    "            state_tensor[0, state] = 1.0\n",
    "            logits, _ = model(state_tensor)\n",
    "            action = logits.argmax().item()\n",
    "            state, _, done = env.step(action)\n",
    "            step += 1\n",
    "\n",
    "    # Собираем кадры в GIF\n",
    "    images = []\n",
    "    for filename in sorted(os.listdir(frames_folder)):\n",
    "        if filename.endswith(\".png\"):\n",
    "            images.append(imageio.imread(os.path.join(frames_folder, filename)))\n",
    "\n",
    "    imageio.mimsave(video_name, images, duration=0.3)\n",
    "    print(f\"Видео сохранено как '{video_name}'\")\n",
    "\n",
    "    # Очистка папки с кадрами\n",
    "    for f in os.listdir(frames_folder):\n",
    "        os.remove(os.path.join(frames_folder, f))\n",
    "\n",
    "\n",
    "# ------------------ Запуск обучения ------------------\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57e34bb-bc11-424a-97c6-a9c1be90a241",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126f3c4a-5e36-4535-9a05-192a294144bd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617f743c-1d5b-4afe-a8fa-fc2c9ba178dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
