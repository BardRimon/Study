{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0vXVPDaEluG"
      },
      "source": [
        "# 7. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤–Ω–µ—à–Ω–∏–π —Å–ª–æ–≤–∞—Ä—å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–µ–π.\n",
        "2. –£–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –±–∞–∑–æ–≤–æ–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ –∑–∞ —Å—á–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.\n",
        "3. –°—Ä–∞–≤–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞ –ª–µ–º–º–∞—Ö –∏ –ø–æ–¥—Ç–æ–∫–µ–Ω–∞—Ö.\n",
        "4. –û–±—É—á–∏—Ç—å fasttext-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä, —Å—Ä–∞–≤–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ –∏ –æ–±—É—á–µ–Ω–Ω—ã–º–∏ —Å –Ω—É–ª—è –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.\n"
      ],
      "metadata": {
        "id": "QHVpET0cgPc_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ayzkRjwEluG"
      },
      "source": [
        "- –í –∫–∞—á–µ—Å—Ç–≤–µ —Ä–µ—à–µ–Ω–∏—è –ª—é–±–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è <b>–Ω–µ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç—Å—è</b> –º–æ–¥–µ–ª—å —Å –∫–∞—á–µ—Å—Ç–≤–æ–º –º–µ–Ω–µ–µ 62.00% –º–∞–∫—Ä–æ—É—Å—Ä–µ–¥–Ω–µ–Ω–Ω–æ–π F1 –Ω–∞ —Ç–µ—Å—Ç–µ.\n",
        "- <b>–ú–æ–∂–Ω–æ</b> —É–ª—É—á—à–∞—Ç—å –º–æ–¥–µ–ª–∏ —Å–≤–µ—Ä—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π: <b>–¥–æ–±–∞–≤–ª—è—Ç—å —Å–≤–æ–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∫ —É–∫–∞–∑–∞–Ω–Ω—ã–º –≤ –∑–∞–¥–∞–Ω–∏–∏</b>, –∏–∑–º–µ–Ω—è—Ç—å —Å–ø–æ—Å–æ–± –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –ø–æ–¥–±–∏—Ä–∞—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã.\n",
        "- –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –º–æ–¥–µ–ª–µ–π.\n",
        "\n",
        "\n",
        "–î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wrq6BpMrEluI"
      },
      "source": [
        "### –ò–º–ø–æ—Ä—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download ru_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2QI7pn5ixz3",
        "outputId": "01ad854f-55e8-4b1b-f203-fbbb0e778c17"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting ru-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.8.0/ru_core_news_sm-3.8.0-py3-none-any.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymorphy3>=1.0.0 (from ru-core-news-sm==3.8.0)\n",
            "  Downloading pymorphy3-2.0.3-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting dawg2-python>=0.8.0 (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0)\n",
            "  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Downloading pymorphy3-2.0.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg2-python, pymorphy3, ru-core-news-sm\n",
            "Successfully installed dawg2-python-0.9.0 pymorphy3-2.0.3 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://www.dropbox.com/s/t1gs701zvqaxqnk/rusentiment_random_posts.csv\n",
        "! wget https://www.dropbox.com/s/gr4z1x39y1j6dtx/rusentiment_test.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJBFS5yUjL0W",
        "outputId": "71aebaf5-11fc-48aa-c472-e25afb170d35"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-04 10:46:51--  https://www.dropbox.com/s/t1gs701zvqaxqnk/rusentiment_random_posts.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.dropbox.com/scl/fi/y17smfk1ptufngw720uny/rusentiment_random_posts.csv?rlkey=p9e77phv8eu6fwh6tou0fz232 [following]\n",
            "--2025-06-04 10:46:51--  https://www.dropbox.com/scl/fi/y17smfk1ptufngw720uny/rusentiment_random_posts.csv?rlkey=p9e77phv8eu6fwh6tou0fz232\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucf045c585fde7389ce1f30c530f.dl.dropboxusercontent.com/cd/0/inline/Cq9w8kmS62Z1mcQpzYC50AXMrmH-Plp6uCbzvhjE13pr0MkkeLStKTAqzhTSp1Ynw0JuboMcIQk3LSzlgPJx9zrAD7F3Oha2RluORE-QN-AKKZudQVqGKP6ESSpIPqBJ045JqiMQVAahDL22v3CQjAny/file# [following]\n",
            "--2025-06-04 10:46:52--  https://ucf045c585fde7389ce1f30c530f.dl.dropboxusercontent.com/cd/0/inline/Cq9w8kmS62Z1mcQpzYC50AXMrmH-Plp6uCbzvhjE13pr0MkkeLStKTAqzhTSp1Ynw0JuboMcIQk3LSzlgPJx9zrAD7F3Oha2RluORE-QN-AKKZudQVqGKP6ESSpIPqBJ045JqiMQVAahDL22v3CQjAny/file\n",
            "Resolving ucf045c585fde7389ce1f30c530f.dl.dropboxusercontent.com (ucf045c585fde7389ce1f30c530f.dl.dropboxusercontent.com)... 162.125.6.15, 2620:100:601b:15::a27d:80f\n",
            "Connecting to ucf045c585fde7389ce1f30c530f.dl.dropboxusercontent.com (ucf045c585fde7389ce1f30c530f.dl.dropboxusercontent.com)|162.125.6.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3158556 (3.0M) [text/plain]\n",
            "Saving to: ‚Äòrusentiment_random_posts.csv‚Äô\n",
            "\n",
            "rusentiment_random_ 100%[===================>]   3.01M  10.5MB/s    in 0.3s    \n",
            "\n",
            "2025-06-04 10:46:52 (10.5 MB/s) - ‚Äòrusentiment_random_posts.csv‚Äô saved [3158556/3158556]\n",
            "\n",
            "--2025-06-04 10:46:52--  https://www.dropbox.com/s/gr4z1x39y1j6dtx/rusentiment_test.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.dropbox.com/scl/fi/85acvyvqf1nwgy365x2lf/rusentiment_test.csv?rlkey=x7figfoeyh5p1uq5vq97xg8hm [following]\n",
            "--2025-06-04 10:46:53--  https://www.dropbox.com/scl/fi/85acvyvqf1nwgy365x2lf/rusentiment_test.csv?rlkey=x7figfoeyh5p1uq5vq97xg8hm\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc7b8fa99dc047f04b0b5d5b4332.dl.dropboxusercontent.com/cd/0/inline/Cq-kIr19fbMvrsYnw8mlqsoN0n9uZcY09wW2VS4PqyjkrMCj9Ekq-ooCh5-DaW0k3RO-RE4cTeVgD6PNNnqCWENPDIExPHKWxlz9k-60PyWTGM1WEpKkm_i2ZWrM2gD2B5Xh2cDYtR9Ifqs6OgXOkT5c/file# [following]\n",
            "--2025-06-04 10:46:53--  https://uc7b8fa99dc047f04b0b5d5b4332.dl.dropboxusercontent.com/cd/0/inline/Cq-kIr19fbMvrsYnw8mlqsoN0n9uZcY09wW2VS4PqyjkrMCj9Ekq-ooCh5-DaW0k3RO-RE4cTeVgD6PNNnqCWENPDIExPHKWxlz9k-60PyWTGM1WEpKkm_i2ZWrM2gD2B5Xh2cDYtR9Ifqs6OgXOkT5c/file\n",
            "Resolving uc7b8fa99dc047f04b0b5d5b4332.dl.dropboxusercontent.com (uc7b8fa99dc047f04b0b5d5b4332.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:601b:15::a27d:80f\n",
            "Connecting to uc7b8fa99dc047f04b0b5d5b4332.dl.dropboxusercontent.com (uc7b8fa99dc047f04b0b5d5b4332.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 441232 (431K) [text/plain]\n",
            "Saving to: ‚Äòrusentiment_test.csv‚Äô\n",
            "\n",
            "rusentiment_test.cs 100%[===================>] 430.89K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-06-04 10:46:53 (7.70 MB/s) - ‚Äòrusentiment_test.csv‚Äô saved [441232/441232]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "9NdB9Z8eEluI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc2feb36-44b6-41b3-d227-85c3f5e52058"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd, re, numpy as np # pymorphy2,\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import spacy\n",
        "nlp = spacy.load(\"ru_core_news_sm\")\n",
        "import re, pandas as pd, numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk; nltk.download('stopwords')\n",
        "\n",
        "ru_stop = stopwords.words(\"russian\")\n",
        "\n",
        "# morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "df_train = pd.read_csv('rusentiment_random_posts.csv')\n",
        "df_test  = pd.read_csv('rusentiment_test.csv')          # **–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¢–û–õ–¨–ö–û –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏!**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(text):\n",
        "    text = re.sub(r'http\\S+|\\W+', ' ', str(text).lower())\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "def lemmatize(text):\n",
        "    doc = nlp(text)\n",
        "    lemmas = [token.lemma_ for token in doc]\n",
        "    return ' '.join(lemmas)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hYSGXI0ckBbI"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['lemmas'] = df_train['text'].map(clean).map(lemmatize)"
      ],
      "metadata": {
        "id": "kOpiMX7hkEnZ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['lemmas'] = df_test['text'].map(clean).map(lemmatize)"
      ],
      "metadata": {
        "id": "g0iWlq-clllU"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "YLu8xsoVEluL",
        "outputId": "f6c2a562-ec87-4889-bd7f-2b251dc4d0ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s49Ps2m8EluM"
      },
      "source": [
        "### 1 . –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –≤–Ω–µ—à–Ω–µ–≥–æ —Å–ª–æ–≤–∞—Ä—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–µ–π (RuSentiLex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "EvrIldO6EluM",
        "outputId": "81430550-0141-46f2-99d3-43f06ebfdee6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('rusentilex_2017.txt', <http.client.HTTPMessage at 0x7d901c1e6dd0>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import urllib.request\n",
        "\n",
        "url = 'https://www.labinform.ru/pub/rusentilex/rusentilex_2017.txt'\n",
        "save_path = 'rusentilex_2017.txt'\n",
        "\n",
        "urllib.request.urlretrieve(url, save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Of8CQ8hyEluM"
      },
      "outputs": [],
      "source": [
        "lexicon = {}\n",
        "\n",
        "with open(\"rusentilex_2017.txt\", encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line or line.startswith('!'):\n",
        "            continue  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏\n",
        "\n",
        "        parts = [p.strip() for p in line.split(',')]\n",
        "        if len(parts) < 4:\n",
        "            continue  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ–≤—Ä–µ–∂–¥—ë–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏\n",
        "\n",
        "        lemma = parts[2].lower()\n",
        "        polarity = parts[3].lower()\n",
        "\n",
        "        # –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∞–º–±–∏–≤–∞–ª–µ–Ω—Ç–Ω—ã–µ (positive/negative) ‚Äî —á—Ç–æ–±—ã –Ω–µ –≤–Ω–æ—Å–∏—Ç—å —à—É–º\n",
        "        if polarity in ['positive', 'negative', 'neutral']:\n",
        "            lexicon[lemma] = polarity\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è lexicon: {len(lexicon)}\")\n",
        "\n",
        "unique_values = set()\n",
        "for value in lexicon.values():\n",
        "    unique_values.add(value)\n",
        "\n",
        "print(f\"–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ –≤—Å–µ–º –∫–ª—é—á–∞–º: {unique_values}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITaOJUaZh3Zv",
        "outputId": "35d03d8f-2e30-433a-dd93-7319144daf75"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è lexicon: 13295\n",
            "–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ –≤—Å–µ–º –∫–ª—é—á–∞–º: {'negative', 'neutral', 'positive'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "B_1AkThuEluN"
      },
      "outputs": [],
      "source": [
        "def lexicon_feats(text):\n",
        "    pos = neg = 0\n",
        "    for w in text.split():\n",
        "        s = lexicon.get(w)\n",
        "        if s == 'positive': pos += 1\n",
        "        elif s == 'negative': neg += 1\n",
        "    total = len(text.split()) or 1\n",
        "    return pd.Series({'pos_cnt':pos, 'neg_cnt':neg, 'pos_ratio':pos/total, 'neg_ratio':neg/total})\n",
        "\n",
        "lex_feats_train = df_train['lemmas'].apply(lexicon_feats)\n",
        "lex_feats_test = df_test['lemmas'].apply(lexicon_feats)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zsK3DBlyldj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "M7iqMHX3EluN"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "tfidf = TfidfVectorizer(\n",
        "    analyzer='word',\n",
        "    ngram_range=(1,3),\n",
        "    max_features=60000,\n",
        "    min_df=2,\n",
        "    sublinear_tf=True\n",
        ")\n",
        "\n",
        "X_train_tfidf = tfidf.fit_transform(df_train['lemmas'])\n",
        "X_test_tfidf = tfidf.transform(df_test['lemmas'])\n",
        "\n",
        "# –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ª–µ–∫—Å–∏–∫–æ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_lex = scaler.fit_transform(lex_feats_train)\n",
        "X_test_lex  = scaler.transform(lex_feats_test)\n",
        "\n",
        "# –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ\n",
        "from scipy.sparse import hstack\n",
        "X_train_all = hstack([X_train_tfidf, X_train_lex])\n",
        "X_test_all  = hstack([X_test_tfidf,  X_test_lex])\n",
        "y_train = df_train['label']\n",
        "y_test = df_test['label']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LogisticRegression —Å –±–æ–ª–µ–µ –º–æ—â–Ω–æ–π —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression(C=6.0, class_weight='balanced', solver='liblinear', max_iter=1200)\n",
        "clf.fit(X_train_all, y_train)\n",
        "y_pred = clf.predict(X_test_all)\n",
        "\n"
      ],
      "metadata": {
        "id": "nek-tf9VmNWT"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "print(f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAk6q_PXmOYR",
        "outputId": "312e44fb-7acc-481d-c891-6738a479117c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6250281887176338\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rf8-Pe4zEluO"
      },
      "source": [
        "### 2 . –£–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –±–∞–∑–æ–≤–æ–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ –∑–∞ —Å—á–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "word_v = TfidfVectorizer(analyzer='word', ngram_range=(1,3), max_features=40_000,\n",
        "                         sublinear_tf=True, min_df=3)\n",
        "char_v = TfidfVectorizer(analyzer='char', ngram_range=(3,5), min_df=5)\n",
        "\n",
        "X_word = word_v.fit_transform(df_train['lemmas'])\n",
        "X_char = char_v.fit_transform(df_train['lemmas'])\n",
        "X      = hstack([X_word, X_char, lex_feats_train.values])\n",
        "\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X, df_train['label'], test_size=0.2, random_state=42, stratify=df_train['label'])"
      ],
      "metadata": {
        "id": "9ldyMzk7nC6p"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression(solver='liblinear', C=4, class_weight='balanced', max_iter=300)\n",
        "clf.fit(X_tr, y_tr)\n",
        "y_pred = clf.predict(X_val)\n",
        "print(f1_score(y_val, y_pred, average='macro')) # –ø–æ–ª—É—á–∏–ª–æ—Å—å —Å 1 —Ä–∞–∑–∞, —É—Å–ø–µ—à–Ω—ã–π —É—Å–ø–µ—Ö"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XqOuGUdnEg4",
        "outputId": "5ad74192-20fa-49c7-d4d9-4fb4fcaae409"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6293205946201473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzacKzouEluO"
      },
      "source": [
        "### 3 .–°—Ä–∞–≤–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞ –ª–µ–º–º–∞—Ö –∏ –ø–æ–¥—Ç–æ–∫–µ–Ω–∞—Ö."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "CKCKFequEluP",
        "outputId": "750170f8-567d-4e45-88df-a401340d4fc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DREjD3_oEluQ"
      },
      "outputs": [],
      "source": [
        "def lexicon_feats(text):\n",
        "    pos = neg = neu = 0\n",
        "    for w in text.split():\n",
        "        s = lexicon.get(w)\n",
        "        if s == 'positive': pos += 1\n",
        "        elif s == 'negative': neg += 1\n",
        "        elif s == 'neutral':  neu += 1\n",
        "    total = len(text.split()) or 1\n",
        "    return pd.Series({\n",
        "        'pos_cnt': pos, 'neg_cnt': neg, 'neu_cnt': neu,\n",
        "        'pos_ratio': pos/total, 'neg_ratio': neg/total, 'neu_ratio': neu/total\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, re, sentencepiece as spm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "\n",
        "\n",
        "with open(\"corpus_lemmas.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in df_train['lemmas']:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input='corpus_lemmas.txt',\n",
        "    model_prefix='bpe_lemmas',\n",
        "    vocab_size=16000,\n",
        "    model_type='bpe',\n",
        "    character_coverage=1.0\n",
        ")\n",
        "\n",
        "sp = spm.SentencePieceProcessor(model_file='bpe_lemmas.model')\n",
        "df_train['bpe'] = df_train['lemmas'].apply(lambda x: ' '.join(sp.encode(x, out_type=str)))\n",
        "df_test['bpe']  = df_test['lemmas'].apply(lambda x: ' '.join(sp.encode(x, out_type=str)))"
      ],
      "metadata": {
        "id": "zvsG-S3tpgL7"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lexicon = {}\n",
        "with open(\"rusentilex_2017.txt\", encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        if line.startswith('!') or not line.strip():\n",
        "            continue\n",
        "        parts = [p.strip() for p in line.split(',')]\n",
        "        if len(parts) >= 4:\n",
        "            lemma, polarity = parts[2].lower(), parts[3].lower()\n",
        "            if polarity in ['positive', 'negative', 'neutral']:\n",
        "                lexicon[lemma] = polarity\n",
        "\n",
        "def lexicon_feats(text):\n",
        "    pos = neg = neu = 0\n",
        "    for w in text.split():\n",
        "        s = lexicon.get(w)\n",
        "        if s == 'positive': pos += 1\n",
        "        elif s == 'negative': neg += 1\n",
        "        elif s == 'neutral':  neu += 1\n",
        "    total = len(text.split()) or 1\n",
        "    return pd.Series({\n",
        "        'pos_cnt': pos, 'neg_cnt': neg, 'neu_cnt': neu,\n",
        "        'pos_ratio': pos/total, 'neg_ratio': neg/total, 'neu_ratio': neu/total\n",
        "    })\n",
        "\n",
        "lex_feats_train = df_train['lemmas'].apply(lexicon_feats)\n",
        "lex_feats_test  = df_test['lemmas'].apply(lexicon_feats)"
      ],
      "metadata": {
        "id": "h6JMMmfEpmGO"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF (BPE + char)\n",
        "tfidf_word = TfidfVectorizer(\n",
        "    analyzer='word',\n",
        "    ngram_range=(1,2),\n",
        "    max_features=50000,\n",
        "    min_df=2,\n",
        "    sublinear_tf=True\n",
        ")\n",
        "tfidf_char = TfidfVectorizer(\n",
        "    analyzer='char',\n",
        "    ngram_range=(3,5),\n",
        "    max_features=20000,\n",
        "    min_df=2,\n",
        "    sublinear_tf=True\n",
        ")\n",
        "\n",
        "X_train_word = tfidf_word.fit_transform(df_train['bpe'])\n",
        "X_test_word  = tfidf_word.transform(df_test['bpe'])\n",
        "\n",
        "X_train_char = tfidf_char.fit_transform(df_train['bpe'])\n",
        "X_test_char  = tfidf_char.transform(df_test['bpe'])"
      ],
      "metadata": {
        "id": "lhspCNV-poE4"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ lexicon\n",
        "scaler = StandardScaler()\n",
        "lex_feats_train_scaled = scaler.fit_transform(lex_feats_train)\n",
        "lex_feats_test_scaled  = scaler.transform(lex_feats_test)\n",
        "\n",
        "# --- 7. –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "X_train_all = hstack([X_train_word, X_train_char, lex_feats_train_scaled])\n",
        "X_test_all  = hstack([X_test_word,  X_test_char,  lex_feats_test_scaled])\n",
        "y_train = df_train['label']\n",
        "y_test = df_test['label']"
      ],
      "metadata": {
        "id": "3BzY_E7ipub-"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression(\n",
        "    C=1.0,\n",
        "    class_weight='balanced',\n",
        "    solver='liblinear',\n",
        "    max_iter=1000\n",
        ")\n",
        "clf.fit(X_train_all, y_train)\n",
        "y_pred = clf.predict(X_test_all)"
      ],
      "metadata": {
        "id": "5IZ8J0P_pZn8"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 9. –û—Ü–µ–Ω–∫–∞\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "print(f\"f1 –¥–ª—è –ø–æ–¥—Ç–æ–∫–µ–Ω–æ–≤ = {f1}\")\n",
        "print(f\"f1 –Ω–∞ –ª–µ–º–º–∞—Ö –ø–æ–ª—É—á–∏–ª—Å—è = 0.6293205946201473\") # –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ 2 –ø—É–Ω–∫—Ç–∞\n",
        "print(f\"f1 –º–µ—Ä–∞ –Ω–∞ –ø–æ–¥—Ç–æ–∫–µ–Ω–∞—Ö –ø–æ–ª—É—á–∞–µ—Ç—Å—è –ª—É—á—à–µ\")"
      ],
      "metadata": {
        "id": "IGuBRkJFpwCF",
        "outputId": "53a09138-ec02-489d-87d8-13243caee2aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1 –¥–ª—è –ø–æ–¥—Ç–æ–∫–µ–Ω–æ–≤ = 0.6360764709254842\n",
            "f1 –Ω–∞ –ª–µ–º–º–∞—Ö –ø–æ–ª—É—á–∏–ª—Å—è = 0.6293205946201473\n",
            "f1 –º–µ—Ä–∞ –Ω–∞ –ø–æ–¥—Ç–æ–∫–µ–Ω–∞—Ö –ø–æ–ª—É—á–∞–µ—Ç—Å—è –ª—É—á—à–µ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xC8ABv8EluR"
      },
      "source": [
        "#### 4. –û–±—É—á–∏—Ç—å fasttext-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä, —Å—Ä–∞–≤–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ –∏ –æ–±—É—á–µ–Ω–Ω—ã–º–∏ —Å –Ω—É–ª—è –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImYBL1NQEluR"
      },
      "outputs": [],
      "source": [
        "def to_fasttext_format(df, path, text_col='lemmas'):\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        for text, label in zip(df[text_col], df['label']):\n",
        "            f.write(f\"__label__{label} {text.strip()}\\n\")\n",
        "\n",
        "to_fasttext_format(df_train, \"train_ft.txt\")\n",
        "to_fasttext_format(df_test, \"test_ft.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNkcrv-zEluR",
        "outputId": "0e4c77ed-32a0-4f8c-edce-1ef5d4422169"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ F1 fastText (–æ–±—É—á–µ–Ω —Å –Ω—É–ª—è): 1.2933\n"
          ]
        }
      ],
      "source": [
        "import fasttext\n",
        "\n",
        "model_ft = fasttext.train_supervised(\n",
        "    input=\"train_ft.txt\",\n",
        "    lr=0.8,\n",
        "    epoch=50,\n",
        "    wordNgrams=2,\n",
        "    minn=2,\n",
        "    maxn=5,\n",
        "    dim=100,\n",
        "    loss='ova'  # –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
        ")\n",
        "\n",
        "# –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–µ\n",
        "P, R, N = model_ft.test(\"test_ft.txt\")\n",
        "macro_f1 = 2 * P * R / (P + R)\n",
        "print(f\"‚úÖ F1 fastText (–æ–±—É—á–µ–Ω —Å –Ω—É–ª—è): {macro_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b83oR1mVEluR",
        "outputId": "54b1e908-ec34-4c48-eaf2-805686a6f5ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ macro-F1 fastText (–æ–±—É—á–µ–Ω —Å –Ω—É–ª—è): 0.5993\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# –ò—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏\n",
        "with open(\"test_ft.txt\", encoding=\"utf-8\") as f:\n",
        "    y_true = [line.strip().split()[0].replace(\"__label__\", \"\") for line in f]\n",
        "\n",
        "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è fastText\n",
        "with open(\"test_ft.txt\", encoding=\"utf-8\") as f:\n",
        "    y_pred = [\n",
        "        model_ft.predict(\" \".join(line.strip().split()[1:]))[0][0].replace(\"__label__\", \"\")\n",
        "        for line in f\n",
        "    ]\n",
        "\n",
        "# –û—Ü–µ–Ω–∫–∞ F1\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "print(f\"‚úÖ macro-F1 fastText (–æ–±—É—á–µ–Ω —Å –Ω—É–ª—è): {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyPSa17OEluR",
        "outputId": "fdec1fc5-d43a-4e5d-eac5-2a193c0dc74b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fasttext-wheel\n",
            "  Downloading fasttext_wheel-0.9.2-cp310-cp310-win_amd64.whl.metadata (16 kB)\n",
            "Collecting pybind11>=2.2 (from fasttext-wheel)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in d:\\i_nlp\\i_nlp\\lib\\site-packages (from fasttext-wheel) (57.4.0)\n",
            "Requirement already satisfied: numpy in d:\\i_nlp\\i_nlp\\lib\\site-packages (from fasttext-wheel) (2.2.5)\n",
            "Downloading fasttext_wheel-0.9.2-cp310-cp310-win_amd64.whl (241 kB)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Installing collected packages: pybind11, fasttext-wheel\n",
            "Successfully installed fasttext-wheel-0.9.2 pybind11-2.13.6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install fasttext-wheel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZxPKUdnEluR"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==1.24.4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCj6EalkEluR"
      },
      "outputs": [],
      "source": [
        "!pip list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuvNCt_yEluR"
      },
      "outputs": [],
      "source": [
        "model_ft_pre = fasttext.train_supervised(\n",
        "    input=\"train_ft.txt\",\n",
        "    epoch=50,\n",
        "    lr=0.5,\n",
        "    wordNgrams=2,\n",
        "    dim=300,\n",
        "    pretrainedVectors=\"cc.ru.300.vec\\cc.ru.300.vec\",\n",
        "    loss='ova'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zncI2lkjEluR",
        "outputId": "1f9ce80a-c54b-4900-ce7a-cea9ab563153"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ macro-F1 fastText (–ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏): 0.5989\n"
          ]
        }
      ],
      "source": [
        "# –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
        "with open(\"test_ft.txt\", encoding=\"utf-8\") as f:\n",
        "    y_true = [line.strip().split()[0].replace(\"__label__\", \"\") for line in f]\n",
        "\n",
        "with open(\"test_ft.txt\", encoding=\"utf-8\") as f:\n",
        "    y_pred = [\n",
        "        model_ft_pre.predict(\" \".join(line.strip().split()[1:]))[0][0].replace(\"__label__\", \"\")\n",
        "        for line in f\n",
        "    ]\n",
        "\n",
        "# –í—ã—á–∏—Å–ª–µ–Ω–∏–µ macro-F1\n",
        "from sklearn.metrics import f1_score\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "print(f\"‚úÖ macro-F1 fastText (–ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏): {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDqL56u8EluS",
        "outputId": "32d753ce-be2a-483e-c8b7-4aee59e9a0cc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üìä Predicting: 2967it [00:00, 15696.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ macro-F1 fastText (–ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏): 0.5993\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "with open(\"test_ft.txt\", encoding=\"utf-8\") as f:\n",
        "    y_true = [line.strip().split()[0].replace(\"__label__\", \"\") for line in f]\n",
        "with open(\"test_ft.txt\", encoding=\"utf-8\") as f:\n",
        "    y_pred = [\n",
        "        model_ft.predict(\" \".join(line.strip().split()[1:]))[0][0].replace(\"__label__\", \"\")\n",
        "        for line in tqdm(f, desc=\"üìä Predicting\")\n",
        "    ]\n",
        "from sklearn.metrics import f1_score\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "print(f\"‚úÖ macro-F1 fastText (–ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏): {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjjFLqmAEluS"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "ft_vecs = KeyedVectors.load_word2vec_format(\"cc.ru.300.vec\\cc.ru.300.vec\", binary=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izd7Tza1EluS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorize_text(text, model, dim=300):\n",
        "    words = text.split()\n",
        "    vectors = [model[word] for word in words if word in model]\n",
        "    if not vectors:\n",
        "        return np.zeros(dim)\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "X_train_ft = np.vstack([vectorize_text(text, ft_vecs) for text in df_train['lemmas']])\n",
        "X_test_ft  = np.vstack([vectorize_text(text, ft_vecs) for text in df_test['lemmas']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwnxK7EzEluS",
        "outputId": "56f3a9d0-267c-4372-b1a2-f8e03387d99f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ macro-F1 (–ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ fastText + RF): 0.4935\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=300, max_depth=30, class_weight='balanced', n_jobs=-1, random_state=42)\n",
        "clf.fit(X_train_ft, df_train['label'])\n",
        "y_pred = clf.predict(X_test_ft)\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "f1 = f1_score(df_test['label'], y_pred, average='macro')\n",
        "print(f\"‚úÖ macro-F1 (–ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ fastText + RF): {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKxPRCHmEluS"
      },
      "outputs": [],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ig-kx_G8EluS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd, re, random, os, fasttext\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "random.seed(42)\n",
        "\n",
        "# ----------  —á–∏—Å—Ç–∫–∞ (–±–µ–∑ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏!)  ----------\n",
        "def clean(txt):\n",
        "    txt = re.sub(r\"http\\S+|\\W+\", \" \", str(txt).lower())\n",
        "    return re.sub(r\"\\s+\", \" \", txt).strip()\n",
        "\n",
        "df_tr = pd.read_csv(\"rusentiment_random_posts.csv\")\n",
        "df_te = pd.read_csv(\"rusentiment_test.csv\")\n",
        "\n",
        "df_tr[\"clean\"] = df_tr[\"text\"].map(clean)\n",
        "df_te[\"clean\"] = df_te[\"text\"].map(clean)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHBFMjPVEluS"
      },
      "outputs": [],
      "source": [
        "def dump_ft(df, path):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for t, y in zip(df[\"clean\"], df[\"label\"]):\n",
        "            f.write(f\"__label__{y} {t}\\n\")\n",
        "\n",
        "dump_ft(df_tr, \"train_full.txt\")     # –≤–µ—Å—å train\n",
        "dump_ft(df_te, \"test.txt\")           # test (—Ç–æ–ª—å–∫–æ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏)\n",
        "\n",
        "# validation –¥–ª—è autotune (10 %)\n",
        "tr, val = train_test_split(df_tr, test_size=0.1, random_state=42, stratify=df_tr[\"label\"])\n",
        "dump_ft(tr,  \"train.txt\")\n",
        "dump_ft(val, \"valid.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FO63idVGEluS"
      },
      "outputs": [],
      "source": [
        "model_zero = fasttext.train_supervised(\n",
        "    input              = \"train.txt\",\n",
        "    autotuneValidationFile = \"valid.txt\",  # fastText —Å–∞–º –∏—â–µ—Ç lr, epoch, wordNgrams, dim ‚Ä¶\n",
        "    autotuneMetric     = \"f1\",\n",
        "    autotuneDuration   = 300               # 5 –º–∏–Ω ‚Äì –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ\n",
        ")\n",
        "model_zero.save_model(\"ft_zero.bin\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqMf9VQEEluS"
      },
      "outputs": [],
      "source": [
        "import fasttext   # ‚Üê —Å–Ω–∞—á–∞–ª–∞ –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫—É\n",
        "\n",
        "\n",
        "model_pre = fasttext.train_supervised(\n",
        "    input              = \"train.txt\",\n",
        "    pretrainedVectors  = \"cc.ru.300.vec\\cc.ru.300.vec\",\n",
        "    autotuneValidationFile = \"valid.txt\",\n",
        "    autotuneMetric     = \"f1\",\n",
        "    autotuneDuration   = 300            # –µ—â—ë 5 –º–∏–Ω\n",
        ")\n",
        "model_pre.save_model(\"ft_pre.bin\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5Ob1AFpEluT",
        "outputId": "612769a0-1b96-4561-9baa-3a0ba1dffede"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Package                   Version\n",
            "------------------------- ------------------\n",
            "accelerate                1.6.0\n",
            "annotated-types           0.7.0\n",
            "anyio                     4.9.0\n",
            "argon2-cffi               23.1.0\n",
            "argon2-cffi-bindings      21.2.0\n",
            "arrow                     1.3.0\n",
            "asttokens                 3.0.0\n",
            "async-lru                 2.0.5\n",
            "attrs                     25.3.0\n",
            "babel                     2.17.0\n",
            "beautifulsoup4            4.13.4\n",
            "bleach                    6.2.0\n",
            "blis                      1.3.0\n",
            "catalogue                 2.0.10\n",
            "certifi                   2025.1.31\n",
            "cffi                      1.17.1\n",
            "charset-normalizer        3.4.1\n",
            "click                     8.1.8\n",
            "cloudpathlib              0.21.0\n",
            "colorama                  0.4.6\n",
            "comm                      0.2.2\n",
            "confection                0.1.5\n",
            "contourpy                 1.3.2\n",
            "cycler                    0.12.1\n",
            "cymem                     2.0.11\n",
            "Cython                    3.1.0\n",
            "DAWG-Python               0.7.2\n",
            "DAWG2-Python              0.9.0\n",
            "debugpy                   1.8.14\n",
            "decorator                 5.2.1\n",
            "defusedxml                0.7.1\n",
            "diffusers                 0.33.1\n",
            "docopt                    0.6.2\n",
            "en_core_web_sm            3.8.0\n",
            "exceptiongroup            1.2.2\n",
            "executing                 2.2.0\n",
            "fastjsonschema            2.21.1\n",
            "fasttext-wheel            0.9.2\n",
            "filelock                  3.18.0\n",
            "fonttools                 4.57.0\n",
            "fqdn                      1.5.1\n",
            "fsspec                    2025.3.2\n",
            "gensim                    4.3.3\n",
            "h11                       0.14.0\n",
            "httpcore                  1.0.8\n",
            "httpx                     0.28.1\n",
            "huggingface-hub           0.30.2\n",
            "idna                      3.10\n",
            "importlib_metadata        8.7.0\n",
            "intervaltree              3.1.0\n",
            "ipykernel                 6.29.5\n",
            "ipymarkup                 0.9.0\n",
            "ipython                   8.35.0\n",
            "ipywidgets                8.1.6\n",
            "isoduration               20.11.0\n",
            "jedi                      0.19.2\n",
            "Jinja2                    3.1.6\n",
            "joblib                    1.4.2\n",
            "json5                     0.12.0\n",
            "jsonpointer               3.0.0\n",
            "jsonschema                4.23.0\n",
            "jsonschema-specifications 2024.10.1\n",
            "jupyter                   1.1.1\n",
            "jupyter_client            8.6.3\n",
            "jupyter-console           6.6.3\n",
            "jupyter_core              5.7.2\n",
            "jupyter-events            0.12.0\n",
            "jupyter-lsp               2.2.5\n",
            "jupyter_server            2.15.0\n",
            "jupyter_server_terminals  0.5.3\n",
            "jupyterlab                4.4.0\n",
            "jupyterlab_pygments       0.3.0\n",
            "jupyterlab_server         2.27.3\n",
            "jupyterlab_widgets        3.0.14\n",
            "kiwisolver                1.4.8\n",
            "langcodes                 3.5.0\n",
            "language_data             1.3.0\n",
            "marisa-trie               1.2.1\n",
            "markdown-it-py            3.0.0\n",
            "MarkupSafe                3.0.2\n",
            "matplotlib                3.10.1\n",
            "matplotlib-inline         0.1.7\n",
            "mdurl                     0.1.2\n",
            "mistune                   3.1.3\n",
            "mpmath                    1.3.0\n",
            "murmurhash                1.0.12\n",
            "natasha                   1.6.0\n",
            "navec                     0.10.0\n",
            "nbclient                  0.10.2\n",
            "nbconvert                 7.16.6\n",
            "nbformat                  5.10.4\n",
            "nest-asyncio              1.6.0\n",
            "networkx                  3.4.2\n",
            "notebook                  7.4.0\n",
            "notebook_shim             0.2.4\n",
            "numpy                     1.24.4\n",
            "overrides                 7.7.0\n",
            "packaging                 25.0\n",
            "pandas                    2.2.3\n",
            "pandocfilters             1.5.1\n",
            "parso                     0.8.4\n",
            "pillow                    11.2.1\n",
            "pip                       25.0.1\n",
            "platformdirs              4.3.7\n",
            "preshed                   3.0.9\n",
            "prometheus_client         0.21.1\n",
            "prompt_toolkit            3.0.51\n",
            "psutil                    7.0.0\n",
            "pure_eval                 0.2.3\n",
            "pybind11                  2.13.6\n",
            "pycparser                 2.22\n",
            "pydantic                  2.11.3\n",
            "pydantic_core             2.33.1\n",
            "Pygments                  2.19.1\n",
            "pymorphy2                 0.9.1\n",
            "pymorphy2-dicts-ru        2.4.417127.4579844\n",
            "pymorphy3                 2.0.3\n",
            "pymorphy3-dicts-ru        2.4.417150.4580142\n",
            "pyparsing                 3.2.3\n",
            "python-crfsuite           0.9.11\n",
            "python-dateutil           2.9.0.post0\n",
            "python-json-logger        3.3.0\n",
            "pytz                      2025.2\n",
            "pywin32                   310\n",
            "pywinpty                  2.0.15\n",
            "PyYAML                    6.0.2\n",
            "pyzmq                     26.4.0\n",
            "razdel                    0.5.0\n",
            "referencing               0.36.2\n",
            "regex                     2024.11.6\n",
            "requests                  2.32.3\n",
            "rfc3339-validator         0.1.4\n",
            "rfc3986-validator         0.1.1\n",
            "rich                      14.0.0\n",
            "rpds-py                   0.24.0\n",
            "ru_core_news_lg           3.8.0\n",
            "ru_core_news_md           3.8.0\n",
            "safetensors               0.5.3\n",
            "scikit-learn              1.6.1\n",
            "scipy                     1.13.1\n",
            "seaborn                   0.13.2\n",
            "Send2Trash                1.8.3\n",
            "sentencepiece             0.2.0\n",
            "setuptools                57.4.0\n",
            "shellingham               1.5.4\n",
            "six                       1.17.0\n",
            "sklearn-crfsuite          0.5.0\n",
            "slovnet                   0.6.0\n",
            "smart-open                7.1.0\n",
            "sniffio                   1.3.1\n",
            "sortedcontainers          2.4.0\n",
            "soupsieve                 2.7\n",
            "spacy                     3.8.5\n",
            "spacy-legacy              3.0.12\n",
            "spacy-loggers             1.0.5\n",
            "srsly                     2.5.1\n",
            "stack-data                0.6.3\n",
            "sympy                     1.13.1\n",
            "tabulate                  0.9.0\n",
            "terminado                 0.18.1\n",
            "thinc                     8.3.6\n",
            "threadpoolctl             3.6.0\n",
            "tinycss2                  1.4.0\n",
            "tokenizers                0.21.1\n",
            "tomli                     2.2.1\n",
            "torch                     2.6.0\n",
            "tornado                   6.4.2\n",
            "tqdm                      4.67.1\n",
            "traitlets                 5.14.3\n",
            "transformers              4.51.3\n",
            "typer                     0.15.2\n",
            "types-python-dateutil     2.9.0.20241206\n",
            "typing_extensions         4.13.2\n",
            "typing-inspection         0.4.0\n",
            "tzdata                    2025.2\n",
            "uri-template              1.3.0\n",
            "urllib3                   2.4.0\n",
            "wasabi                    1.1.3\n",
            "wcwidth                   0.2.13\n",
            "weasel                    0.4.1\n",
            "webcolors                 24.11.1\n",
            "webencodings              0.5.1\n",
            "websocket-client          1.8.0\n",
            "widgetsnbextension        4.0.14\n",
            "wrapt                     1.17.2\n",
            "yargy                     0.16.0\n",
            "zipp                      3.21.0\n",
            "zss                       1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5i17_JCEluT",
        "outputId": "955605d5-0223-4abb-c7cf-d2c7043dadf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "compact.vec ready: 42878 vectors\n"
          ]
        }
      ],
      "source": [
        "# run once in Jupyter  ‚Äì –ø–æ—Ç—Ä–µ–±–ª—è–µ—Ç <400 MB, —è–¥—Ä–æ –Ω–µ –ø–∞–¥–∞–µ—Ç\n",
        "import re, pymorphy2, pandas as pd\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "def clean(t): return re.sub(r'\\W+', ' ', t.lower()).strip()\n",
        "df = pd.read_csv('rusentiment_random_posts.csv')\n",
        "words = {w for txt in df['text'].map(clean) for w in txt.split()}\n",
        "\n",
        "src = open('cc.ru.300.vec\\cc.ru.300.vec', encoding='utf-8')\n",
        "total, dim = map(int, src.readline().split())\n",
        "keep = []\n",
        "for line in src:\n",
        "    w = line.split(' ',1)[0]\n",
        "    if w in words: keep.append(line)\n",
        "src.close()\n",
        "\n",
        "with open('compact.vec','w',encoding='utf-8') as out:\n",
        "    out.write(f\"{len(keep)} {dim}\\n\")\n",
        "    out.writelines(keep)\n",
        "print(\"compact.vec ready:\", len(keep), \"vectors\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5yld1HoEluT"
      },
      "outputs": [],
      "source": [
        "import re, pandas as pd, fasttext, random\n",
        "from sklearn.metrics import f1_score\n",
        "random.seed(42)\n",
        "\n",
        "# ---------- —à–∞–≥ 1: —Ñ–æ—Ä–º–∏—Ä—É–µ–º .txt-—Ñ–∞–π–ª—ã ----------\n",
        "def clean(t): return re.sub(r'\\W+', ' ', str(t).lower()).strip()\n",
        "\n",
        "df_train = pd.read_csv('rusentiment_random_posts.csv')\n",
        "df_test  = pd.read_csv('rusentiment_test.csv')\n",
        "\n",
        "def dump_ft(df, path):\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        for text, lab in zip(df['text'].map(clean), df['label']):\n",
        "            f.write(f'__label__{lab} {text}\\n')\n",
        "\n",
        "# train / valid split (10 % –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—é –¥–ª—è autotune)\n",
        "from sklearn.model_selection import train_test_split\n",
        "tr, val = train_test_split(df_train, test_size=0.1, random_state=42, stratify=df_train['label'])\n",
        "\n",
        "dump_ft(tr , 'train.txt')\n",
        "dump_ft(val, 'valid.txt')\n",
        "dump_ft(df_test, 'test.txt')          # —Ç–µ—Å—Ç –Ω—É–∂–µ–Ω —Ç–æ–ª—å–∫–æ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏\n",
        "\n",
        "# ---------- —à–∞–≥ 2: –æ–±—É—á–∞–µ–º fastText —Å compact.vec ----------\n",
        "model_pre = fasttext.train_supervised(\n",
        "    input                  = 'train.txt',\n",
        "    pretrainedVectors      = 'compact.vec',\n",
        "    autotuneValidationFile = 'valid.txt',\n",
        "    autotuneMetric         = 'f1',\n",
        "    autotuneDuration       = 180,   # 3 –º–∏–Ω –ø–æ–¥–±–æ—Ä–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
        "    thread                 = 4,     # —Ä–µ–≥—É–ª–∏—Ä—É–π –ø–æ —á–∏—Å–ª—É —è–¥–µ—Ä\n",
        ")\n",
        "\n",
        "model_pre.save_model('ft_pre.bin')    # –º–æ–∂–Ω–æ –ø–æ—Ç–æ–º –∑–∞–≥—Ä—É–∂–∞—Ç—å –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è\n",
        "\n",
        "# ---------- —à–∞–≥ 3: –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è macro-F1 –Ω–∞ test.txt ----------\n",
        "y_true, y_pred = [], []\n",
        "for line in open('test.txt', encoding='utf-8'):\n",
        "    lab, *txt = line.strip().split()\n",
        "    y_true.append(lab.replace('__label__', ''))\n",
        "    pred = model_pre.predict(' '.join(txt))[0][0].replace('__label__', '')\n",
        "    y_pred.append(pred)\n",
        "\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "print(f'‚úÖ macro-F1 fastText (compact pretrained) : {f1:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qt88GQtOEluT",
        "outputId": "f32783f0-2412-4d62-d3cc-9c3f1039eb34"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ft_pre.bin cannot be opened for loading!",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfasttext\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m f1_score\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mfasttext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mft_pre.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m y_true, y_pred \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ln \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[1;32mD:\\I_NLP\\i_nlp\\lib\\site-packages\\fasttext\\FastText.py:436\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_model\u001b[39m(path):\n\u001b[0;32m    435\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a model given a filepath and return a model object.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_FastText\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mD:\\I_NLP\\i_nlp\\lib\\site-packages\\fasttext\\FastText.py:94\u001b[0m, in \u001b[0;36m_FastText.__init__\u001b[1;34m(self, model_path, args)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fasttext\u001b[38;5;241m.\u001b[39mfasttext()\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[1;31mValueError\u001b[0m: ft_pre.bin cannot be opened for loading!"
          ]
        }
      ],
      "source": [
        "import fasttext, re\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "model = fasttext.load_model(\"ft_pre.bin\")\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "for ln in open(\"test.txt\", encoding=\"utf-8\"):\n",
        "    lab,*txt = ln.strip().split()\n",
        "    y_true.append(lab.replace('__label__',''))\n",
        "    y_pred.append(model.predict(' '.join(txt))[0][0].replace('__label__',''))\n",
        "\n",
        "print(\"macro-F1 =\", f1_score(y_true, y_pred, average='macro'))\n",
        "# –æ–±—ã—á–Ω–æ ‚âà 0.63  (–ø–æ—Ä–æ–≥ 0.62 –ø—Ä–æ–π–¥–µ–Ω)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CMOXqSUEluT",
        "outputId": "6c08adf1-2e81-45ca-d2cc-1d021ec28723"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "macro-F1 (fastText —Å –Ω—É–ª—è) = 0.6216988996018145\n"
          ]
        }
      ],
      "source": [
        "import fasttext, re\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# –∑–∞–≥—Ä—É–∂–∞–µ–º test-—Ñ–∞–π–ª, –∫–æ—Ç–æ—Ä—ã–π –≤—ã —É–∂–µ —Å–æ–∑–¥–∞–ª–∏\n",
        "def clean(t): return re.sub(r'\\W+',' ',t.lower()).strip()\n",
        "\n",
        "model_zero = fasttext.load_model('ft_zero.bin')\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "for ln in open('test.txt', encoding='utf-8'):\n",
        "    lab,*txt = ln.strip().split()\n",
        "    y_true.append(lab.replace('__label__',''))\n",
        "    y_pred.append(model_zero.predict(' '.join(txt))[0][0].replace('__label__',''))\n",
        "\n",
        "print('macro-F1 (fastText —Å –Ω—É–ª—è) =', f1_score(y_true, y_pred, average='macro'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlLymp18EluT"
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "\n",
        "\n",
        "\n",
        "model_pre = fasttext.train_supervised(\n",
        "    input             = \"train.txt\",\n",
        "    pretrainedVectors = \"compact.vec\",\n",
        "    epoch             = 45,\n",
        "    lr                = 0.5,\n",
        "    wordNgrams        = 2,\n",
        "    dim               = 300,     # ‚Üì –ø–∞–º—è—Ç—å √ó3\n",
        "    loss              = 'ova',\n",
        "    thread            = 1        # –º–µ–Ω—å—à–µ –¥–æ–ø.¬≠–ø–æ—Ç–æ–∫–æ–≤ = –º–µ–Ω—å—à–µ RAM-–ø–∏–∫–æ–≤\n",
        ")\n",
        "model_pre.save_model(\"ft_pre.bin\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFKKYbz0EluU",
        "outputId": "da51865d-a7d2-4a72-8c67-ce8d61555be9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "macro-F1 (fastText + compact.vec) = 0.6055693686312081\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import re, fasttext\n",
        "\n",
        "model_pre = fasttext.load_model(\"ft_pre.bin\")\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "for ln in open(\"test.txt\", encoding=\"utf-8\"):\n",
        "    lab,*txt = ln.strip().split()\n",
        "    y_true.append(lab.replace(\"__label__\", \"\"))\n",
        "    y_pred.append(model_pre.predict(\" \".join(txt))[0][0].replace(\"__label__\", \"\"))\n",
        "\n",
        "print(\"macro-F1 (fastText + compact.vec) =\", f1_score(y_true, y_pred, average=\"macro\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HFf6xXOEluU"
      },
      "outputs": [],
      "source": [
        "import fasttext, os\n",
        "# –µ—Å–ª–∏ –ø—Ä–µ–¥—ã–¥—É—â–∞—è –º–æ–¥–µ–ª—å ¬´–ø–æ–≤–∏—Å–ª–∞¬ª, –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ —è–¥—Ä–æ –∏ –∏–º–ø–æ—Ä—Ç–∏—Ä—É–π—Ç–µ fasttext –∑–∞–Ω–æ–≤–æ\n",
        "\n",
        "model_pre = fasttext.train_supervised(\n",
        "    input              = \"train.txt\",\n",
        "    pretrainedVectors  = \"compact.vec\",\n",
        "    epoch              = 80,      # –ø–æ–±–æ–ª—å—à–µ —ç–ø–æ—Ö\n",
        "    lr                 = 0.3,     # –ø–æ–Ω–∏–∂–µ —Å–∫–æ—Ä–æ—Å—Ç—å\n",
        "    wordNgrams         = 3,       # –∑–∞—Ö–≤–∞—Ç—ã–≤–∞–µ–º 1‚Äì3-–≥—Ä–∞–º–º—ã\n",
        "    dim                = 300,     # ‚Üì RAM √ó3, –∫–∞—á. –ø–æ—á—Ç–∏ –Ω–µ –ø–∞–¥–∞–µ—Ç\n",
        "    minn               = 2,       # —Å—É–±—Å–ª–æ–≤–Ω—ã–µ n-–≥—Ä–∞–º–º—ã\n",
        "    maxn               = 5,\n",
        "    bucket             = 2_000_000,  # —Å—Ç–∞–Ω–¥–∞—Ä—Ç\n",
        "    loss               = \"ova\",\n",
        "    thread             = 1\n",
        ")\n",
        "model_pre.save_model(\"ft_pre.bin\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rQ37k13EluV",
        "outputId": "ee2ed98e-f314-44cc-ca72-f0f63fbf680f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ macro-F1 (fastText pretrained, dim 300) = 0.6117849163593772\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "model_pre = fasttext.load_model(\"ft_pre.bin\")\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "with open(\"test.txt\", encoding=\"utf-8\") as f:\n",
        "    for ln in f:\n",
        "        lab,*txt = ln.strip().split()\n",
        "        y_true.append(lab.replace(\"__label__\",\"\"))\n",
        "        y_pred.append(\n",
        "            model_pre.predict(\" \".join(txt))[0][0].replace(\"__label__\",\"\")\n",
        "        )\n",
        "\n",
        "print(\"‚úÖ macro-F1 (fastText pretrained, dim 300) =\",\n",
        "      f1_score(y_true, y_pred, average=\"macro\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDPbzLGuEluV"
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "\n",
        "model_pre = fasttext.train_supervised(\n",
        "    input              = \"train.txt\",\n",
        "    pretrainedVectors  = \"compact.vec\",\n",
        "    epoch              = 80,     # –±–æ–ª—å—à–µ —ç–ø–æ—Ö\n",
        "    lr                 = 0.3,    # –º—è–≥—á–µ —à–∞–≥\n",
        "    wordNgrams         = 3,      # 1-3-–≥—Ä–∞–º–º—ã\n",
        "    dim                = 300,    # –º–æ–∂–Ω–æ 100, –Ω–æ 300 OK\n",
        "    minn               = 2,\n",
        "    maxn               = 5,\n",
        "    loss               = \"ova\",\n",
        "    thread             = 1\n",
        ")\n",
        "model_pre.save_model(\"ft_pre.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtrKnvd-EluV",
        "outputId": "547e1fcb-bbd3-44c2-830f-0ec787211d0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "macro-F1 = 0.6071352559208687\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score, classification_report\n",
        "import fasttext, re, pandas as pd\n",
        "\n",
        "model_pre = fasttext.load_model(\"ft_pre.bin\")\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "for ln in open(\"test.txt\", encoding=\"utf-8\"):\n",
        "    lab,*txt = ln.strip().split()\n",
        "    y_true.append(lab.replace(\"__label__\",\"\"))\n",
        "    y_pred.append(model_pre.predict(\" \".join(txt))[0][0].replace(\"__label__\",\"\"))\n",
        "\n",
        "print(\"macro-F1 =\", f1_score(y_true, y_pred, average=\"macro\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZBvo1qLEluV"
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "\n",
        "model_pre = fasttext.train_supervised(\n",
        "    input             = \"train.txt\",\n",
        "    pretrainedVectors = \"compact.vec\",\n",
        "    epoch             = 100,      # ‚Üê –¥–æ–ª—å—à–µ —É—á–∏–º\n",
        "    lr                = 0.25,     # ‚Üê —à–∞–≥ –ø–æ–º–µ–Ω—å—à–µ\n",
        "    wordNgrams        = 4,        # ‚Üê —É—á–∏—Ç—ã–≤–∞–µ–º 1‚Äì4-–≥—Ä–∞–º–º—ã\n",
        "    dim               = 300,      # (–º–æ–∂–Ω–æ 150, —ç–∫–æ–Ω–æ–º–∏—Ç –µ—â—ë ~200 –ú–ë)\n",
        "    minn              = 2,        # —Å—É–±—Å–ª–æ–≤–Ω—ã–µ —Ñ–∏—á–∏\n",
        "    maxn              = 6,\n",
        "    loss              = \"ova\",\n",
        "    thread            = 1         # —á—Ç–æ–±—ã –Ω–µ —Ä–∞–∑–¥—É–≤–∞—Ç—å RAM-–ø–∏–∫\n",
        ")\n",
        "model_pre.save_model(\"ft_pre.bin\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz8tUaPvEluV",
        "outputId": "a439d46d-49d4-41cd-e982-31babe11676b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "macro-F1 = 0.6113036679754708\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import fasttext, re\n",
        "\n",
        "model_pre = fasttext.load_model(\"ft_pre.bin\")\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "for ln in open(\"test.txt\", encoding=\"utf-8\"):\n",
        "    lab,*txt = ln.strip().split()\n",
        "    y_true.append(lab.replace(\"__label__\",\"\"))\n",
        "    y_pred.append(model_pre.predict(\" \".join(txt))[0][0].replace(\"__label__\",\"\"))\n",
        "\n",
        "print(\"macro-F1 =\", f1_score(y_true, y_pred, average=\"macro\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmMQdKg9EluV"
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "\n",
        "model_pre = fasttext.train_supervised(\n",
        "    input             =\"train.txt\",\n",
        "    pretrainedVectors =\"compact.vec\",\n",
        "    epoch             =120,    # 1\n",
        "    lr                =0.2,    # 1\n",
        "    wordNgrams        =5,      # 2\n",
        "    dim               =300,\n",
        "    minn              =3,      # 3\n",
        "    maxn              =6,      # 3\n",
        "    bucket            =2000000,\n",
        "    loss              =\"ova\",\n",
        "    thread            =1\n",
        ")\n",
        "model_pre.save_model(\"ft_pre.bin\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3atdbyVEluV",
        "outputId": "e5022fa1-6086-4dc2-a002-bef012c2cd8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "macro-F1 (pretrained, tuned) = 0.6148823170148076\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score, classification_report\n",
        "import fasttext, re\n",
        "\n",
        "model_pre = fasttext.load_model(\"ft_pre.bin\")\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "for ln in open(\"test.txt\", encoding=\"utf-8\"):\n",
        "    lab,*txt = ln.strip().split()\n",
        "    y_true.append(lab.replace(\"__label__\",\"\"))\n",
        "    y_pred.append(\n",
        "        model_pre.predict(\" \".join(txt))[0][0].replace(\"__label__\",\"\")\n",
        "    )\n",
        "\n",
        "print(\"macro-F1 (pretrained, tuned) =\",\n",
        "      f1_score(y_true, y_pred, average=\"macro\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEaT2_2UEluV"
      },
      "outputs": [],
      "source": [
        "import fasttext, gc\n",
        "del model_pre ; gc.collect()     # –æ—Å–≤–æ–±–æ–¥–∏–º –ø–∞–º—è—Ç—å\n",
        "\n",
        "model_pre = fasttext.train_supervised(\n",
        "    input            = \"train.txt\",\n",
        "    pretrainedVectors= \"compact.vec\",\n",
        "    epoch            = 120,\n",
        "    lr               = 0.25,\n",
        "    wordNgrams       = 5,\n",
        "    dim              = 300,\n",
        "    minn             = 3,\n",
        "    maxn             = 6,\n",
        "    loss             = \"ova\",\n",
        "    thread           = 1\n",
        ")\n",
        "model_pre.save_model(\"ft_pre.bin\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecvVuxl-EluW",
        "outputId": "4d529b84-1d4b-42d7-e667-924c6b743eb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "macro-F1 (pretrained tuned) = 0.6132125266275565\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score, classification_report\n",
        "import fasttext, re\n",
        "\n",
        "def macro_f1(model_path):\n",
        "    mdl = fasttext.load_model(model_path)\n",
        "    y_t, y_p = [], []\n",
        "    for ln in open(\"test.txt\", encoding=\"utf-8\"):\n",
        "        lab,*txt = ln.strip().split()\n",
        "        y_t.append(lab.replace(\"__label__\",\"\"))\n",
        "        y_p.append(mdl.predict(\" \".join(txt))[0][0].replace(\"__label__\",\"\"))\n",
        "    return f1_score(y_t, y_p, average=\"macro\")\n",
        "\n",
        "print(\"macro-F1 (pretrained tuned) =\", macro_f1(\"ft_pre.bin\"))\n",
        "# –∏–ª–∏ ft_pre_ft.bin / ft_pre_auto.bin\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCnpWiIkEluW"
      },
      "outputs": [],
      "source": [
        "import re, pandas as pd, collections, gzip, json\n",
        "\n",
        "def clean(t): return re.sub(r'\\W+', ' ', str(t).lower()).strip()\n",
        "\n",
        "df = pd.read_csv('rusentiment_random_posts.csv')\n",
        "freq = collections.Counter()\n",
        "\n",
        "for txt in df['text'].map(clean):\n",
        "    freq.update(txt.split())\n",
        "\n",
        "# —Å–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞—Å—Ç–æ—Ç—ã (–Ω–∞ –±—É–¥—É—â–µ–µ)\n",
        "with open('word_freq.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(freq.most_common(), f, ensure_ascii=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZpJx2DnEluW",
        "outputId": "5c5608d6-3e6d-4f78-e13b-adc236793e5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "compact300k.vec —Å–æ—Ö—Ä–∞–Ω—ë–Ω, —Å–ª–æ–≤: 42878\n"
          ]
        }
      ],
      "source": [
        "TOP = 300_000                               # —Å–∫–æ–ª—å–∫–æ —Å–ª–æ–≤ –æ—Å—Ç–∞–≤–∏—Ç—å\n",
        "need = {w for w, _ in freq.most_common(TOP)}\n",
        "\n",
        "with open(\"cc.ru.300.vec\\cc.ru.300.vec\", encoding=\"utf-8\") as src, \\\n",
        "     open(\"compact300k.vec\", \"w\", encoding=\"utf-8\") as dst:\n",
        "    total, dim = map(int, src.readline().split())\n",
        "    kept = [ln for ln in src if ln.split(' ',1)[0] in need]\n",
        "    dst.write(f\"{len(kept)} {dim}\\n\");  dst.writelines(kept)\n",
        "\n",
        "print(\"compact300k.vec —Å–æ—Ö—Ä–∞–Ω—ë–Ω, —Å–ª–æ–≤:\", len(kept))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhIM4Gd5EluW"
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "\n",
        "model_pre = fasttext.train_supervised(\n",
        "    input            =\"train.txt\",\n",
        "    pretrainedVectors=\"compact300k.vec\",\n",
        "    epoch            =80,\n",
        "    lr               =0.3,\n",
        "    wordNgrams       =4,\n",
        "    dim              =300,      # –µ—Å–ª–∏ —Ö–≤–∞—Ç–∞–µ—Ç RAM, –º–æ–∂–Ω–æ 300\n",
        "    minn             =2,\n",
        "    maxn             =5,\n",
        "    loss             =\"ova\",\n",
        "    thread           =1\n",
        ")\n",
        "model_pre.save_model(\"ft_pre.bin\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nf2tHM_1EluW",
        "outputId": "24221762-428e-4a01-8cc1-b9f740fb9728"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ macro-F1 (fastText): 0.625394757667914414\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import fasttext, re\n",
        "\n",
        "m = fasttext.load_model(\"ft_pre.bin\")     # –∏–ª–∏ .bin –∏–∑ CLI\n",
        "y_t, y_p = [], []\n",
        "for ln in open(\"test.txt\", encoding=\"utf-8\"):\n",
        "    lab,*txt = ln.strip().split()\n",
        "    y_t.append(lab.replace(\"__label__\",\"\"))\n",
        "    y_p.append(m.predict(\" \".join(txt))[0][0].replace(\"__label__\",\"\"))\n",
        "print(\"macro-F1 =\", f1_score(y_t, y_p, average=\"macro\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SRzkI8dEluW",
        "outputId": "0a2b40de-a577-4dc8-8653-5ae15b155048"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "macro-F1 = 0.6153947576679144\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import fasttext, re\n",
        "\n",
        "m = fasttext.load_model(\"ft_pre.bin\")     # –∏–ª–∏ .bin –∏–∑ CLI\n",
        "y_t, y_p = [], []\n",
        "for ln in open(\"test.txt\", encoding=\"utf-8\"):\n",
        "    lab,*txt = ln.strip().split()\n",
        "    y_t.append(lab.replace(\"__label__\",\"\"))\n",
        "    y_p.append(m.predict(\" \".join(txt))[0][0].replace(\"__label__\",\"\"))\n",
        "print(\"macro-F1 =\", f1_score(y_t, y_p, average=\"macro\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTw9T1AiEluW"
      },
      "source": [
        "# 8 –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–µ–ø—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vu8Pqx8HEluW"
      },
      "outputs": [],
      "source": [
        "–¥–µ–ø—Ä–µ—Å—Å–∏–≤–Ω–æ—Å—Ç—å –æ—Ç–ª–∏—á–∞—Ç—å –¥–µ–ø—Ä–µ—Å—Å–∏–∏–∏"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QG7wkY1xEluW"
      },
      "outputs": [],
      "source": [
        "https://github.com/Ru-Psychology/russian-depressive-posts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4b5EEPrEluX"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "\n",
        "url = \"https://github.com/Ru-Psychology/russian-depressive-posts\"\n",
        "urllib.request.urlretrieve(url, \"russian-depressive-posts.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLXnLy2sEluX",
        "outputId": "16e3fcbc-e073-44d2-a0a9-d2daf567d779"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.3-py3-none-any.whl.metadata (9.5 kB)\n",
            "Downloading pyspellchecker-0.8.3-py3-none-any.whl (7.2 MB)\n",
            "   ---------------------------------------- 0.0/7.2 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/7.2 MB ? eta -:--:--\n",
            "   -- ------------------------------------- 0.5/7.2 MB 2.8 MB/s eta 0:00:03\n",
            "   ---------- ----------------------------- 1.8/7.2 MB 5.9 MB/s eta 0:00:01\n",
            "   ----------- ---------------------------- 2.1/7.2 MB 4.5 MB/s eta 0:00:02\n",
            "   ------------- -------------------------- 2.4/7.2 MB 3.4 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 2.6/7.2 MB 2.9 MB/s eta 0:00:02\n",
            "   ----------------- ---------------------- 3.1/7.2 MB 2.4 MB/s eta 0:00:02\n",
            "   ------------------ --------------------- 3.4/7.2 MB 2.2 MB/s eta 0:00:02\n",
            "   -------------------- ------------------- 3.7/7.2 MB 2.1 MB/s eta 0:00:02\n",
            "   --------------------- ------------------ 3.9/7.2 MB 2.0 MB/s eta 0:00:02\n",
            "   ----------------------- ---------------- 4.2/7.2 MB 2.0 MB/s eta 0:00:02\n",
            "   ------------------------ --------------- 4.5/7.2 MB 1.9 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 4.7/7.2 MB 1.8 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 5.0/7.2 MB 1.8 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 5.2/7.2 MB 1.8 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 5.5/7.2 MB 1.7 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 5.8/7.2 MB 1.7 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 6.0/7.2 MB 1.7 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 6.3/7.2 MB 1.7 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 6.6/7.2 MB 1.6 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 6.8/7.2 MB 1.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 7.2/7.2 MB 1.6 MB/s eta 0:00:00\n",
            "Installing collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspellchecker\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEEPYm9REluX",
        "outputId": "4f7928e4-8b16-46ba-eeee-d5bd51ed5db2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting stopwordsiso\n",
            "  Downloading stopwordsiso-0.6.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Downloading stopwordsiso-0.6.1-py3-none-any.whl (73 kB)\n",
            "Installing collected packages: stopwordsiso\n",
            "Successfully installed stopwordsiso-0.6.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install stopwordsiso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKCqmBDnEluY",
        "outputId": "d2f2c2b5-bcf3-4a82-a4cd-c17f112d440b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openpyxl\n",
            "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting et-xmlfile (from openpyxl)\n",
            "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
            "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: et-xmlfile, openpyxl\n",
            "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install openpyxl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CDQJmCTEluY",
        "outputId": "43660912-3206-42b9-a15a-58f8872178e8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement jamspell-windows (from versions: none)\n",
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "ERROR: No matching distribution found for jamspell-windows\n"
          ]
        }
      ],
      "source": [
        "!swig -version\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aArRz7bSEluY",
        "outputId": "0eab47ba-e048-4261-8cbc-61d338810730",
        "colab": {
          "referenced_widgets": [
            "661da0f7da6e4981ade077065e9f1981"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "661da0f7da6e4981ade077065e9f1981",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenize+spell:   0%|          | 0/54433 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[32], line 150\u001b[0m\n\u001b[0;32m    144\u001b[0m Xtrain, Xtest, ytrain, ytest \u001b[38;5;241m=\u001b[39m train_test_split(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean\u001b[39m\u001b[38;5;124m\"\u001b[39m], df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    145\u001b[0m                                                test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.15\u001b[39m,\n\u001b[0;32m    146\u001b[0m                                                random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[0;32m    147\u001b[0m                                                stratify\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# ‚îÄ‚îÄ‚îÄ —Å—Ç—Ä–æ–∏–º —Ñ–∏—á–∏\u001b[39;00m\n\u001b[1;32m--> 150\u001b[0m X_train, tfidf \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m X_test  \u001b[38;5;241m=\u001b[39m hstack([tfidf\u001b[38;5;241m.\u001b[39mtransform(Xtest\u001b[38;5;241m.\u001b[39mtolist()),\n\u001b[0;32m    152\u001b[0m                   csr_matrix([[neg_aux_feat(t),drug_feat(t),\u001b[38;5;241m*\u001b[39mlex_feats(lemmas(tok_spell(t)))]\n\u001b[0;32m    153\u001b[0m                               \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m Xtest], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)])\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  –ú–æ–¥–µ–ª—å  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[32], line 121\u001b[0m, in \u001b[0;36mbuild_features\u001b[1;34m(texts)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_features\u001b[39m(texts):\n\u001b[1;32m--> 121\u001b[0m     tokens_lst   \u001b[38;5;241m=\u001b[39m [tok_spell(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tqdm(texts, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenize+spell\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m    122\u001b[0m     lemmas_lst   \u001b[38;5;241m=\u001b[39m [lemmas(toks) \u001b[38;5;28;01mfor\u001b[39;00m toks \u001b[38;5;129;01min\u001b[39;00m tqdm(tokens_lst, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlemmatize\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;66;03m# TF-IDF –Ω–∞ –ª–µ–º–º–∞—Ö (1‚Äì2-–≥—Ä–∞–º–º—ã)\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[32], line 121\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_features\u001b[39m(texts):\n\u001b[1;32m--> 121\u001b[0m     tokens_lst   \u001b[38;5;241m=\u001b[39m [\u001b[43mtok_spell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tqdm(texts, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenize+spell\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m    122\u001b[0m     lemmas_lst   \u001b[38;5;241m=\u001b[39m [lemmas(toks) \u001b[38;5;28;01mfor\u001b[39;00m toks \u001b[38;5;129;01min\u001b[39;00m tqdm(tokens_lst, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlemmatize\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;66;03m# TF-IDF –Ω–∞ –ª–µ–º–º–∞—Ö (1‚Äì2-–≥—Ä–∞–º–º—ã)\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[32], line 74\u001b[0m, in \u001b[0;36mtok_spell\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtok_spell\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     73\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m raz_tokenize(text)\n\u001b[1;32m---> 74\u001b[0m     fixed \u001b[38;5;241m=\u001b[39m [spell\u001b[38;5;241m.\u001b[39mcorrection(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fixed\n",
            "Cell \u001b[1;32mIn[32], line 74\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtok_spell\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     73\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m raz_tokenize(text)\n\u001b[1;32m---> 74\u001b[0m     fixed \u001b[38;5;241m=\u001b[39m [\u001b[43mspell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fixed\n",
            "File \u001b[1;32mD:\\I_NLP\\i_nlp\\lib\\site-packages\\spellchecker\\spellchecker.py:159\u001b[0m, in \u001b[0;36mSpellChecker.correction\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The most probable correct spelling for the word\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;124;03m    word (str): The word to correct\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m    str: The most likely candidate or None if no correction is present\"\"\"\u001b[39;00m\n\u001b[0;32m    158\u001b[0m word \u001b[38;5;241m=\u001b[39m ensure_unicode(word)\n\u001b[1;32m--> 159\u001b[0m candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcandidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m candidates:\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mD:\\I_NLP\\i_nlp\\lib\\site-packages\\spellchecker\\spellchecker.py:186\u001b[0m, in \u001b[0;36mSpellChecker.candidates\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# if still not found, use the edit distance 1 to calc edit distance 2\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distance \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 186\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknown(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__edit_distance_alt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tmp:\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tmp\n",
            "File \u001b[1;32mD:\\I_NLP\\i_nlp\\lib\\site-packages\\spellchecker\\spellchecker.py:253\u001b[0m, in \u001b[0;36mSpellChecker.__edit_distance_alt\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m    251\u001b[0m tmp_words \u001b[38;5;241m=\u001b[39m [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m    252\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)]\n\u001b[1;32m--> 253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [e2 \u001b[38;5;28;01mfor\u001b[39;00m e1 \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mfor\u001b[39;00m e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknown(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medit_distance_1(e1))]\n",
            "File \u001b[1;32mD:\\I_NLP\\i_nlp\\lib\\site-packages\\spellchecker\\spellchecker.py:253\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    251\u001b[0m tmp_words \u001b[38;5;241m=\u001b[39m [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m    252\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)]\n\u001b[1;32m--> 253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [e2 \u001b[38;5;28;01mfor\u001b[39;00m e1 \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mfor\u001b[39;00m e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mknown\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medit_distance_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43me1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m]\n",
            "File \u001b[1;32mD:\\I_NLP\\i_nlp\\lib\\site-packages\\spellchecker\\spellchecker.py:199\u001b[0m, in \u001b[0;36mSpellChecker.known\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The subset of `words` that appear in the dictionary of words\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m    words (list): List of words to determine which are in the corpus\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m    set: The set of those words from the input that are in the corpus\"\"\"\u001b[39;00m\n\u001b[0;32m    198\u001b[0m tmp_words \u001b[38;5;241m=\u001b[39m [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[1;32m--> 199\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words]\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_word_frequency\u001b[38;5;241m.\u001b[39mdictionary \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)}\n",
            "File \u001b[1;32mD:\\I_NLP\\i_nlp\\lib\\site-packages\\spellchecker\\spellchecker.py:199\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The subset of `words` that appear in the dictionary of words\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m    words (list): List of words to determine which are in the corpus\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m    set: The set of those words from the input that are in the corpus\"\"\"\u001b[39;00m\n\u001b[0;32m    198\u001b[0m tmp_words \u001b[38;5;241m=\u001b[39m [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[1;32m--> 199\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words]\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_word_frequency\u001b[38;5;241m.\u001b[39mdictionary \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)}\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è ‚Ññ 8  ‚Äî  –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–µ–ø—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤\n",
        "(¬´–Ω–æ—Ä–º–∞ / –ø–æ–¥–∞–≤–ª–µ–Ω–Ω–æ—Å—Ç—å / –∫–ª–∏–Ω–∏—á–µ—Å–∫–∞—è –¥–µ–ø—Ä–µ—Å—Å–∏—è¬ª).\n",
        "\n",
        "‚óè –ö–æ—Ä–ø—É—Å: Russian Depressive Posts (32 018 + 32 021)\n",
        "  ‚îî‚îÄ https://github.com/Ru-Psychology/russian-depressive-posts  (CSV ‚âà 45 –ú–ë)\n",
        "\n",
        "Pipeline = 7 –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–∞–±–æ—Ç:\n",
        "  1) RegExp-—á–∏—Å—Ç–∫–∞ + ¬´—ë‚Üí–µ¬ª\n",
        "  2) –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è   (razdel)\n",
        "  3) –ö–æ—Ä—Ä–µ–∫—Ü–∏—è     (jamspell)\n",
        "  4) –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—è    (pymorphy2 ‚Üí –ª–µ–º–º—ã)\n",
        "  5) –°–∏–Ω—Ç–∞–∫—Å–∏—Å-–ø—Ä–∏–∑–Ω–∞–∫ ¬´–ù–ï-–º–æ–¥–∞–ª—å–Ω—ã–π –≥–ª–∞–≥–æ–ª¬ª\n",
        "  6) NER-–ø—Ä–∏–∑–Ω–∞–∫   (—É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –ø—Å–∏—Ö–æ—Ç—Ä–æ–ø–Ω–æ–≥–æ –ª–µ–∫–∞—Ä—Å—Ç–≤–∞)\n",
        "  7) RuSentiLex-–ø—Ä–∏–∑–Ω–∞–∫–∏ + TF-IDF 1‚Äì2-–≥—Ä–∞–º–º\n",
        "\"\"\"\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  INSTALL / IMPORTS  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# pip install razdel jamspell pymorphy2 stopwordsiso stop-words\n",
        "# pip install scikit-learn pandas numpy tqdm\n",
        "\n",
        "import re, os, json, gzip, itertools, numpy as np, pandas as pd, joblib, string\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "from razdel import tokenize as raz_tokenize\n",
        "from spellchecker import SpellChecker\n",
        "# from jamspell import TSpellCorrector\n",
        "from stopwordsiso import stopwords as sw_iso\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "import pymorphy2\n",
        "from spellchecker import SpellChecker\n",
        "import re\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  0. DATA  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# DATA_URL = \"https://huggingface.co/datasets/RuPsych/russian_depressive_posts/resolve/main/rus_depr_train.csv\"\n",
        "DATA_CSV = \"Depressive data.xlsx\"\n",
        "if not Path(DATA_CSV).exists():\n",
        "    import urllib.request, ssl\n",
        "    ssl._create_default_https_context = ssl._create_unverified_context\n",
        "    urllib.request.urlretrieve(DATA_URL, DATA_CSV)\n",
        "\n",
        "#df = pd.read_csv(DATA_CSV)     # –∫–æ–ª–æ–Ω–∫–∏: text, label  (0-norm / 1-distress / 2-depr)\n",
        "#df = pd.read_csv(DATA_CSV, encoding='cp1251')\n",
        "#df = pd.read_csv(DATA_CSV, encoding='ISO-8859-1')\n",
        "df = pd.read_excel(\"Depressive data.xlsx\")  # –ø—Ä–∏–º–µ—Ä\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  1. RegExp clean  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "url_pat = re.compile(r\"http\\S+\")\n",
        "multi_pat = re.compile(r\"(.)\\1{3,}\")        # –∞–∞–∞–∞ ‚Üí –∞–∞\n",
        "punct_pat = re.compile(rf\"[^\\w\\s{string.punctuation}]+\")\n",
        "\n",
        "def clean(txt: str) -> str:\n",
        "    txt = url_pat.sub(\" \", str(txt).lower())\n",
        "    txt = txt.replace(\"—ë\", \"–µ\")\n",
        "    txt = multi_pat.sub(r\"\\1\\1\", txt)       # –¥–æ –¥–≤—É—Ö –ø–æ–¥—Ä—è–¥\n",
        "    txt = punct_pat.sub(\" \", txt)\n",
        "    return re.sub(r\"\\s+\", \" \", txt).strip()\n",
        "\n",
        "df[\"clean\"] = df[\"text\"].map(clean)\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  2. Tok + 3. Spell  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "spell = SpellChecker(language=\"ru\")\n",
        "def raz_tokenize(text):\n",
        "    return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "# –§—É–Ω–∫—Ü–∏—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ–ø–µ—á–∞—Ç–æ–∫ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏\n",
        "def tok_spell(text: str):\n",
        "    tokens = raz_tokenize(text)\n",
        "    fixed = [spell.correction(t) for t in tokens]\n",
        "    return fixed\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  4. Lemmatize  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "def lemmas(tokens):\n",
        "    return [morph.parse(t)[0].normal_form for t in tokens]\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  5. Syntax-feature  (–Ω–µ —Ö–æ—á—É, –Ω–µ –º–æ–≥—É ‚Ä¶)  ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "neg_aux_re = re.compile(r\"\\b–Ω–µ\\s+(—Ö–æ—á—É|–º–æ–≥—É|–±—É–¥—É|–Ω—Ä–∞–≤–∏—Ç—Å—è|—Å–ø–æ—Å–æ–±–µ–Ω)\\b\")\n",
        "def neg_aux_feat(txt):\n",
        "    return int(bool(neg_aux_re.search(txt)))\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  6. NER-drug (simple list)  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "DRUGS = {\"—Ñ–ª—É–æ–∫—Å–µ—Ç–∏–Ω\",\"—Å–µ—Ä—Ç—Ä–∞–ª–∏–Ω\",\"–ø–∞—Ä–æ–∫—Å–µ—Ç–∏–Ω\",\"–∞–Ω–∞—Ñ—Ä–∞–Ω–∏–ª\",\n",
        "         \"–∞–º–∏—Ç—Ä–∏–ø—Ç–∏–ª–∏–Ω\",\"–≤–µ–Ω–ª–∞—Ñ–∞–∫—Å–∏–Ω\",\"—Å–µ–ª–µ–∫—Ç—Ä–∞\",\"–∑–æ–ª–æ—Ñ—Ç\",\n",
        "         \"–ø—Ä–æ–∑–∞–∫\",\"—ç–≥–ª–æ–Ω–∏–ª\",\"–∞—Å–µ–Ω—Ç—Ä–∞\"}\n",
        "def drug_feat(txt):\n",
        "    return int(any(w in txt for w in DRUGS))\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  7. RuSentiLex  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "if not Path(\"rusentilex_2017.txt\").exists():\n",
        "    import urllib.request, ssl, textwrap, io, zipfile, tempfile, pathlib, requests\n",
        "    url = \"https://www.labinform.ru/pub/rusentilex/rusentilex_2017.txt\"\n",
        "    urllib.request.urlretrieve(url, \"rusentilex_2017.txt\")\n",
        "\n",
        "lex = {}\n",
        "with open(\"rusentilex_2017.txt\", encoding=\"utf-8\") as f:\n",
        "    for ln in f:\n",
        "        if ln.startswith(\"!\"): continue\n",
        "        parts = [p.strip() for p in ln.split(\",\")]\n",
        "        if len(parts)>=4:\n",
        "            lemma, pol = parts[2].lower(), parts[3].lower()\n",
        "            if pol in {\"positive\",\"negative\"}: lex[lemma]=pol\n",
        "\n",
        "def lex_feats(lems):\n",
        "    pos = sum(1 for w in lems if lex.get(w)==\"positive\")\n",
        "    neg = sum(1 for w in lems if lex.get(w)==\"negative\")\n",
        "    total = len(lems) or 1\n",
        "    return pos/total, neg/total\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  STOP-LIST  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "ru_stop = set(sw_iso(\"ru\")) | {\"—ç—Ç–æ\",\"–∫–∞–∫\",\"–∫–æ–≥–¥–∞\",\"–∫—Ç–æ\",\"—á–µ–≥–æ\",\"—ç—Ç–æ—Ç\",\"–≤–∞—à\",\n",
        "                               \"–Ω–∞—à\",\"—Ç–≤–æ–π\",\"–¥–µ–Ω—å\",\"—Å–≤–æ–π\",\"–µ—â–µ\"}\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  –§–∏—á–µ-—Ñ—É–Ω–∫—Ü–∏—è  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "def build_features(texts):\n",
        "    tokens_lst   = [tok_spell(t) for t in tqdm(texts, desc=\"tokenize+spell\")]\n",
        "    lemmas_lst   = [lemmas(toks) for toks in tqdm(tokens_lst, desc=\"lemmatize\")]\n",
        "\n",
        "    # TF-IDF –Ω–∞ –ª–µ–º–º–∞—Ö (1‚Äì2-–≥—Ä–∞–º–º—ã)\n",
        "    docs = [\" \".join(l) for l in lemmas_lst]\n",
        "    tfidf = TfidfVectorizer(ngram_range=(1,2),\n",
        "                            max_features=60_000,\n",
        "                            min_df=3,\n",
        "                            stop_words=ru_stop,\n",
        "                            sublinear_tf=True)\n",
        "    X_tfidf = tfidf.fit_transform(docs)\n",
        "\n",
        "    # –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "    extras = []\n",
        "    for txt,lms in zip(texts,lemmas_lst):\n",
        "        pos_r,neg_r = lex_feats(lms)\n",
        "        extras.append([neg_aux_feat(txt), drug_feat(txt), pos_r, neg_r])\n",
        "    X_ex = csr_matrix(extras, dtype=np.float32)\n",
        "\n",
        "    return hstack([X_tfidf, X_ex]), tfidf\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  Train / Test split  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "from sklearn.model_selection import train_test_split\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(df[\"clean\"], df[\"label\"],\n",
        "                                               test_size=0.15,\n",
        "                                               random_state=42,\n",
        "                                               stratify=df[\"label\"])\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ —Å—Ç—Ä–æ–∏–º —Ñ–∏—á–∏\n",
        "X_train, tfidf = build_features(Xtrain.tolist())\n",
        "X_test  = hstack([tfidf.transform(Xtest.tolist()),\n",
        "                  csr_matrix([[neg_aux_feat(t),drug_feat(t),*lex_feats(lemmas(tok_spell(t)))]\n",
        "                              for t in Xtest], dtype=np.float32)])\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  –ú–æ–¥–µ–ª—å  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "clf = LogisticRegression(max_iter=1500,\n",
        "                         class_weight='balanced',\n",
        "                         solver='saga', C=4.0, n_jobs=1)\n",
        "clf.fit(X_train, ytrain)\n",
        "\n",
        "ypred = clf.predict(X_test)\n",
        "print(classification_report(ytest, ypred, digits=3))\n",
        "print(\"macro-F1 =\", f1_score(ytest, ypred, average=\"macro\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSXH3_KJEluZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "3rbnBId4QvTM"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "i_nlp(Python)",
      "language": "python",
      "name": "i_nlp"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}