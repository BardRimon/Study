{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0vXVPDaEluG"
      },
      "source": [
        "# 7. Классификатор тональности"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Использовать в классификации внешний словарь тональностей.\n",
        "2. Улучшить качество базовой предсказательной модели на тестовой выборке за счет добавления и модификации признаков.\n",
        "3. Сравнить качество классификации на леммах и подтокенах.\n",
        "4. Обучить fasttext-классификатор, сравнить качество классификации с предобученными эмбеддингами и обученными с нуля при классификации.\n"
      ],
      "metadata": {
        "id": "QHVpET0cgPc_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ayzkRjwEluG"
      },
      "source": [
        "- В качестве решения любого задания <b>не принимается</b> модель с качеством менее 62.00% макроусредненной F1 на тесте.\n",
        "- <b>Можно</b> улучшать модели сверх предложенных условий: <b>добавлять свои признаки к указанным в задании</b>, изменять способ классификации и подбирать гиперпараметры.\n",
        "- Тестовые данные можно использовать только при оценке моделей.\n",
        "\n",
        "\n",
        "Данные для обучения моделей:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wrq6BpMrEluI"
      },
      "source": [
        "### Импорт датасета"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download ru_core_news_sm"
      ],
      "metadata": {
        "id": "l2QI7pn5ixz3",
        "outputId": "01ad854f-55e8-4b1b-f203-fbbb0e778c17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting ru-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.8.0/ru_core_news_sm-3.8.0-py3-none-any.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymorphy3>=1.0.0 (from ru-core-news-sm==3.8.0)\n",
            "  Downloading pymorphy3-2.0.3-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting dawg2-python>=0.8.0 (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0)\n",
            "  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Downloading pymorphy3-2.0.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg2-python, pymorphy3, ru-core-news-sm\n",
            "Successfully installed dawg2-python-0.9.0 pymorphy3-2.0.3 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://www.dropbox.com/s/t1gs701zvqaxqnk/rusentiment_random_posts.csv\n",
        "! wget https://www.dropbox.com/s/gr4z1x39y1j6dtx/rusentiment_test.csv"
      ],
      "metadata": {
        "id": "yJBFS5yUjL0W",
        "outputId": "71aebaf5-11fc-48aa-c472-e25afb170d35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-04 10:46:51--  https://www.dropbox.com/s/t1gs701zvqaxqnk/rusentiment_random_posts.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.dropbox.com/scl/fi/y17smfk1ptufngw720uny/rusentiment_random_posts.csv?rlkey=p9e77phv8eu6fwh6tou0fz232 [following]\n",
            "--2025-06-04 10:46:51--  https://www.dropbox.com/scl/fi/y17smfk1ptufngw720uny/rusentiment_random_posts.csv?rlkey=p9e77phv8eu6fwh6tou0fz232\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucf045c585fde7389ce1f30c530f.dl.dropboxusercontent.com/cd/0/inline/Cq9w8kmS62Z1mcQpzYC50AXMrmH-Plp6uCbzvhjE13pr0MkkeLStKTAqzhTSp1Ynw0JuboMcIQk3LSzlgPJx9zrAD7F3Oha2RluORE-QN-AKKZudQVqGKP6ESSpIPqBJ045JqiMQVAahDL22v3CQjAny/file# [following]\n",
            "--2025-06-04 10:46:52--  https://ucf045c585fde7389ce1f30c530f.dl.dropboxusercontent.com/cd/0/inline/Cq9w8kmS62Z1mcQpzYC50AXMrmH-Plp6uCbzvhjE13pr0MkkeLStKTAqzhTSp1Ynw0JuboMcIQk3LSzlgPJx9zrAD7F3Oha2RluORE-QN-AKKZudQVqGKP6ESSpIPqBJ045JqiMQVAahDL22v3CQjAny/file\n",
            "Resolving ucf045c585fde7389ce1f30c530f.dl.dropboxusercontent.com (ucf045c585fde7389ce1f30c530f.dl.dropboxusercontent.com)... 162.125.6.15, 2620:100:601b:15::a27d:80f\n",
            "Connecting to ucf045c585fde7389ce1f30c530f.dl.dropboxusercontent.com (ucf045c585fde7389ce1f30c530f.dl.dropboxusercontent.com)|162.125.6.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3158556 (3.0M) [text/plain]\n",
            "Saving to: ‘rusentiment_random_posts.csv’\n",
            "\n",
            "rusentiment_random_ 100%[===================>]   3.01M  10.5MB/s    in 0.3s    \n",
            "\n",
            "2025-06-04 10:46:52 (10.5 MB/s) - ‘rusentiment_random_posts.csv’ saved [3158556/3158556]\n",
            "\n",
            "--2025-06-04 10:46:52--  https://www.dropbox.com/s/gr4z1x39y1j6dtx/rusentiment_test.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.dropbox.com/scl/fi/85acvyvqf1nwgy365x2lf/rusentiment_test.csv?rlkey=x7figfoeyh5p1uq5vq97xg8hm [following]\n",
            "--2025-06-04 10:46:53--  https://www.dropbox.com/scl/fi/85acvyvqf1nwgy365x2lf/rusentiment_test.csv?rlkey=x7figfoeyh5p1uq5vq97xg8hm\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc7b8fa99dc047f04b0b5d5b4332.dl.dropboxusercontent.com/cd/0/inline/Cq-kIr19fbMvrsYnw8mlqsoN0n9uZcY09wW2VS4PqyjkrMCj9Ekq-ooCh5-DaW0k3RO-RE4cTeVgD6PNNnqCWENPDIExPHKWxlz9k-60PyWTGM1WEpKkm_i2ZWrM2gD2B5Xh2cDYtR9Ifqs6OgXOkT5c/file# [following]\n",
            "--2025-06-04 10:46:53--  https://uc7b8fa99dc047f04b0b5d5b4332.dl.dropboxusercontent.com/cd/0/inline/Cq-kIr19fbMvrsYnw8mlqsoN0n9uZcY09wW2VS4PqyjkrMCj9Ekq-ooCh5-DaW0k3RO-RE4cTeVgD6PNNnqCWENPDIExPHKWxlz9k-60PyWTGM1WEpKkm_i2ZWrM2gD2B5Xh2cDYtR9Ifqs6OgXOkT5c/file\n",
            "Resolving uc7b8fa99dc047f04b0b5d5b4332.dl.dropboxusercontent.com (uc7b8fa99dc047f04b0b5d5b4332.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:601b:15::a27d:80f\n",
            "Connecting to uc7b8fa99dc047f04b0b5d5b4332.dl.dropboxusercontent.com (uc7b8fa99dc047f04b0b5d5b4332.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 441232 (431K) [text/plain]\n",
            "Saving to: ‘rusentiment_test.csv’\n",
            "\n",
            "rusentiment_test.cs 100%[===================>] 430.89K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-06-04 10:46:53 (7.70 MB/s) - ‘rusentiment_test.csv’ saved [441232/441232]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "9NdB9Z8eEluI",
        "outputId": "bc2feb36-44b6-41b3-d227-85c3f5e52058",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd, re, numpy as np # pymorphy2,\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import spacy\n",
        "nlp = spacy.load(\"ru_core_news_sm\")\n",
        "import re, pandas as pd, numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk; nltk.download('stopwords')\n",
        "\n",
        "ru_stop = stopwords.words(\"russian\")\n",
        "\n",
        "# morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "df_train = pd.read_csv('rusentiment_random_posts.csv')\n",
        "df_test  = pd.read_csv('rusentiment_test.csv')          # **использовать ТОЛЬКО для финальной оценки!**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(text):\n",
        "    text = re.sub(r'http\\S+|\\W+', ' ', str(text).lower())\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "def lemmatize(text):\n",
        "    doc = nlp(text)\n",
        "    lemmas = [token.lemma_ for token in doc]\n",
        "    return ' '.join(lemmas)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hYSGXI0ckBbI"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['lemmas'] = df_train['text'].map(clean).map(lemmatize)"
      ],
      "metadata": {
        "id": "kOpiMX7hkEnZ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['lemmas'] = df_test['text'].map(clean).map(lemmatize)"
      ],
      "metadata": {
        "id": "g0iWlq-clllU"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "YLu8xsoVEluL",
        "outputId": "f6c2a562-ec87-4889-bd7f-2b251dc4d0ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s49Ps2m8EluM"
      },
      "source": [
        "### 1 . Добавляем признаки из внешнего словаря тональностей (RuSentiLex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "EvrIldO6EluM",
        "outputId": "81430550-0141-46f2-99d3-43f06ebfdee6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('rusentilex_2017.txt', <http.client.HTTPMessage at 0x7d901c1e6dd0>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import urllib.request\n",
        "\n",
        "url = 'https://www.labinform.ru/pub/rusentilex/rusentilex_2017.txt'\n",
        "save_path = 'rusentilex_2017.txt'\n",
        "\n",
        "urllib.request.urlretrieve(url, save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Of8CQ8hyEluM"
      },
      "outputs": [],
      "source": [
        "lexicon = {}\n",
        "\n",
        "with open(\"rusentilex_2017.txt\", encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line or line.startswith('!'):\n",
        "            continue  # пропускаем комментарии и пустые строки\n",
        "\n",
        "        parts = [p.strip() for p in line.split(',')]\n",
        "        if len(parts) < 4:\n",
        "            continue  # пропускаем повреждённые строки\n",
        "\n",
        "        lemma = parts[2].lower()\n",
        "        polarity = parts[3].lower()\n",
        "\n",
        "        # игнорируем амбивалентные (positive/negative) — чтобы не вносить шум\n",
        "        if polarity in ['positive', 'negative', 'neutral']:\n",
        "            lexicon[lemma] = polarity\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Размер словаря lexicon: {len(lexicon)}\")\n",
        "\n",
        "unique_values = set()\n",
        "for value in lexicon.values():\n",
        "    unique_values.add(value)\n",
        "\n",
        "print(f\"Уникальные значения по всем ключам: {unique_values}\")"
      ],
      "metadata": {
        "id": "ITaOJUaZh3Zv",
        "outputId": "35d03d8f-2e30-433a-dd93-7319144daf75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размер словаря lexicon: 13295\n",
            "Уникальные значения по всем ключам: {'negative', 'neutral', 'positive'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "B_1AkThuEluN"
      },
      "outputs": [],
      "source": [
        "def lexicon_feats(text):\n",
        "    pos = neg = 0\n",
        "    for w in text.split():\n",
        "        s = lexicon.get(w)\n",
        "        if s == 'positive': pos += 1\n",
        "        elif s == 'negative': neg += 1\n",
        "    total = len(text.split()) or 1\n",
        "    return pd.Series({'pos_cnt':pos, 'neg_cnt':neg, 'pos_ratio':pos/total, 'neg_ratio':neg/total})\n",
        "\n",
        "lex_feats_train = df_train['lemmas'].apply(lexicon_feats)\n",
        "lex_feats_test = df_test['lemmas'].apply(lexicon_feats)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zsK3DBlyldj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "M7iqMHX3EluN"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "tfidf = TfidfVectorizer(\n",
        "    analyzer='word',\n",
        "    ngram_range=(1,3),\n",
        "    max_features=60000,\n",
        "    min_df=2,\n",
        "    sublinear_tf=True\n",
        ")\n",
        "\n",
        "X_train_tfidf = tfidf.fit_transform(df_train['lemmas'])\n",
        "X_test_tfidf = tfidf.transform(df_test['lemmas'])\n",
        "\n",
        "# нормализация лексиконных признаков\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_lex = scaler.fit_transform(lex_feats_train)\n",
        "X_test_lex  = scaler.transform(lex_feats_test)\n",
        "\n",
        "# объединение\n",
        "from scipy.sparse import hstack\n",
        "X_train_all = hstack([X_train_tfidf, X_train_lex])\n",
        "X_test_all  = hstack([X_test_tfidf,  X_test_lex])\n",
        "y_train = df_train['label']\n",
        "y_test = df_test['label']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LogisticRegression с более мощной регуляризацией\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression(C=6.0, class_weight='balanced', solver='liblinear', max_iter=1200)\n",
        "clf.fit(X_train_all, y_train)\n",
        "y_pred = clf.predict(X_test_all)\n",
        "\n"
      ],
      "metadata": {
        "id": "nek-tf9VmNWT"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "print(f1)"
      ],
      "metadata": {
        "id": "JAk6q_PXmOYR",
        "outputId": "312e44fb-7acc-481d-c891-6738a479117c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6250281887176338\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rf8-Pe4zEluO"
      },
      "source": [
        "### 2 . Улучшить качество базовой предсказательной модели на тестовой выборке за счет добавления и модификации признаков."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "word_v = TfidfVectorizer(analyzer='word', ngram_range=(1,3), max_features=40_000,\n",
        "                         sublinear_tf=True, min_df=3)\n",
        "char_v = TfidfVectorizer(analyzer='char', ngram_range=(3,5), min_df=5)\n",
        "\n",
        "X_word = word_v.fit_transform(df_train['lemmas'])\n",
        "X_char = char_v.fit_transform(df_train['lemmas'])\n",
        "X      = hstack([X_word, X_char, lex_feats_train.values])\n",
        "\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X, df_train['label'], test_size=0.2, random_state=42, stratify=df_train['label'])"
      ],
      "metadata": {
        "id": "9ldyMzk7nC6p"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression(solver='liblinear', C=4, class_weight='balanced', max_iter=300)\n",
        "clf.fit(X_tr, y_tr)\n",
        "y_pred = clf.predict(X_val)\n",
        "print(f1_score(y_val, y_pred, average='macro')) # получилось с 1 раза, успешный успех"
      ],
      "metadata": {
        "id": "9XqOuGUdnEg4",
        "outputId": "5ad74192-20fa-49c7-d4d9-4fb4fcaae409",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6293205946201473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzacKzouEluO"
      },
      "source": [
        "### 3 .Сравнить качество классификации на леммах и подтокенах."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AW-ttcIyEluO",
        "outputId": "0a0b309f-51d7-4223-837e-33d00a81be7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting Cython\n",
            "  Downloading cython-3.1.0-cp310-cp310-win_amd64.whl.metadata (31 kB)\n",
            "Downloading cython-3.1.0-cp310-cp310-win_amd64.whl (2.7 MB)\n",
            "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
            "   --- ------------------------------------ 0.3/2.7 MB ? eta -:--:--\n",
            "   --------------- ------------------------ 1.0/2.7 MB 2.6 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 1.8/2.7 MB 2.7 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 2.4/2.7 MB 2.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.7/2.7 MB 2.6 MB/s eta 0:00:00\n",
            "Installing collected packages: Cython\n",
            "Successfully installed Cython-3.1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install Cython\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_cx11G7EluO",
        "outputId": "b7bc9f65-03ce-4f16-b85f-6ed237526a9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.1.0\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'youtokentome'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mCython\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(Cython\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01myoutokentome\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01myttm\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(yttm\u001b[38;5;241m.\u001b[39m__version__)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'youtokentome'"
          ]
        }
      ],
      "source": [
        "import Cython\n",
        "print(Cython.__version__)\n",
        "import youtokentome as yttm\n",
        "print(yttm.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOe34JmpEluO",
        "outputId": "c45fdbc6-c92a-4387-aab1-9ee521c4bb64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "D:\\I_NLP\\i_nlp\\Scripts\\python.exe\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(sys.executable)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKCKFequEluP",
        "outputId": "58c05a62-a93e-42f3-b427-1d24b5a8ee9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "  Using cached sentencepiece-0.2.0-cp310-cp310-win_amd64.whl.metadata (8.3 kB)\n",
            "Using cached sentencepiece-0.2.0-cp310-cp310-win_amd64.whl (991 kB)\n",
            "Installing collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.2.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Li9oFwg7EluP"
      },
      "outputs": [],
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# Сохраняем корпус лемм в текстовый файл\n",
        "with open(\"corpus_lemmas.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in df_train['lemmas']:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "# Обучаем BPE-модель\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input='corpus_lemmas.txt',\n",
        "    model_prefix='bpe_lemmas',\n",
        "    vocab_size=16000,\n",
        "    model_type='bpe',\n",
        "    character_coverage=1.0\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "306FaDAOEluP"
      },
      "outputs": [],
      "source": [
        "# Загружаем модель\n",
        "import sentencepiece as spm\n",
        "sp = spm.SentencePieceProcessor(model_file='bpe_lemmas.model')\n",
        "\n",
        "# Применяем BPE к каждому тексту\n",
        "df_train['bpe_lemmas'] = df_train['lemmas'].apply(lambda x: ' '.join(sp.encode(x, out_type=str)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5PE4U4kEluP"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(analyzer='word')\n",
        "X_bpe = vectorizer.fit_transform(df_train['bpe_lemmas'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HF4lpovHEluP",
        "outputId": "b9371244-15be-4201-eca9-92818101b3d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 на леммах:       0.5861\n",
            "F1 на подтокенах BPE: 0.5960\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Обучающая выборка\n",
        "X_lemmas = df_train['lemmas']\n",
        "X_bpe    = df_train['bpe_lemmas']\n",
        "y        = df_train['label']\n",
        "\n",
        "# Разделим на train/val\n",
        "X_lem_tr, X_lem_val, y_lem_tr, y_lem_val = train_test_split(X_lemmas, y, test_size=0.2, random_state=42, stratify=y)\n",
        "X_bpe_tr, X_bpe_val, y_bpe_tr, y_bpe_val = train_test_split(X_bpe,    y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Векторизаторы (анализируем слова)\n",
        "tfidf_lemmas = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=30000)\n",
        "tfidf_bpe    = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=30000)\n",
        "\n",
        "X_lem_tr_vec = tfidf_lemmas.fit_transform(X_lem_tr)\n",
        "X_lem_val_vec = tfidf_lemmas.transform(X_lem_val)\n",
        "\n",
        "X_bpe_tr_vec = tfidf_bpe.fit_transform(X_bpe_tr)\n",
        "X_bpe_val_vec = tfidf_bpe.transform(X_bpe_val)\n",
        "\n",
        "# Классификатор\n",
        "clf_lem = LogisticRegression(max_iter=300, class_weight='balanced')\n",
        "clf_bpe = LogisticRegression(max_iter=300, class_weight='balanced')\n",
        "\n",
        "# Обучение\n",
        "clf_lem.fit(X_lem_tr_vec, y_lem_tr)\n",
        "clf_bpe.fit(X_bpe_tr_vec, y_bpe_tr)\n",
        "\n",
        "# Предсказание и оценка\n",
        "y_lem_pred = clf_lem.predict(X_lem_val_vec)\n",
        "y_bpe_pred = clf_bpe.predict(X_bpe_val_vec)\n",
        "\n",
        "f1_lem = f1_score(y_lem_val, y_lem_pred, average='macro')\n",
        "f1_bpe = f1_score(y_bpe_val, y_bpe_pred, average='macro')\n",
        "\n",
        "print(f\"F1 на леммах:       {f1_lem:.4f}\")\n",
        "print(f\"F1 на подтокенах BPE: {f1_bpe:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4E3zLe-EluP"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv(\"rusentiment_test.csv\")\n",
        "df_test['lemmas'] = df_test['text'].map(clean).map(lemmatize)\n",
        "df_test['bpe_lemmas'] = df_test['lemmas'].apply(lambda x: ' '.join(sp.encode(x, out_type=str)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxQcBaidEluP",
        "outputId": "1b5738b0-bf98-437e-d6f7-d19550192484"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "macro-F1 на тесте = 0.58601557344416\n"
          ]
        }
      ],
      "source": [
        "# Повторим обучение на всей train-части (без val)\n",
        "X_train_text = df_train['lemmas']\n",
        "y_train = df_train['label']\n",
        "\n",
        "# Признаки lexicon\n",
        "lex_feats_train = df_train['lemmas'].apply(lexicon_feats)\n",
        "lex_feats_test = df_test['lemmas'].apply(lexicon_feats)\n",
        "\n",
        "# TF-IDF\n",
        "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
        "X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
        "X_test_tfidf = tfidf.transform(df_test['lemmas'])\n",
        "\n",
        "# Объединение всех признаков\n",
        "from scipy.sparse import hstack\n",
        "X_train_final = hstack([X_train_tfidf, lex_feats_train.values])\n",
        "X_test_final = hstack([X_test_tfidf, lex_feats_test.values])\n",
        "\n",
        "# Обучение модели\n",
        "clf = LogisticRegression(max_iter=300, class_weight='balanced')\n",
        "clf.fit(X_train_final, y_train)\n",
        "\n",
        "# Предсказание\n",
        "y_test_pred = clf.predict(X_test_final)\n",
        "\n",
        "# Оценка качества\n",
        "from sklearn.metrics import f1_score\n",
        "print(\"macro-F1 на тесте =\", f1_score(df_test['label'], y_test_pred, average='macro'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CIp0qflEluQ"
      },
      "outputs": [],
      "source": [
        "# word TF-IDF\n",
        "tfidf_word = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=30000)\n",
        "X_train_word = tfidf_word.fit_transform(df_train['lemmas'])\n",
        "X_test_word  = tfidf_word.transform(df_test['lemmas'])\n",
        "\n",
        "# char TF-IDF (на леммах)\n",
        "tfidf_char = TfidfVectorizer(analyzer='char_wb', ngram_range=(3,5), min_df=3)\n",
        "X_train_char = tfidf_char.fit_transform(df_train['lemmas'])\n",
        "X_test_char  = tfidf_char.transform(df_test['lemmas'])\n",
        "\n",
        "# объединение всех признаков: word + char + lexicon\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "X_train_all = hstack([X_train_word, X_train_char, lex_feats_train.values])\n",
        "X_test_all  = hstack([X_test_word,  X_test_char,  lex_feats_test.values])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLDvoihgEluQ",
        "outputId": "1e3d02f6-d860-4789-8898-4b86a1f58ffe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "macro-F1 на тесте = 0.6103309883775418\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "D:\\I_NLP\\i_nlp\\lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "clf = LinearSVC(C=1.0, class_weight='balanced', max_iter=1000)\n",
        "clf.fit(X_train_all, y_train)\n",
        "y_test_pred = clf.predict(X_test_all)\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "print(\"macro-F1 на тесте =\", f1_score(df_test['label'], y_test_pred, average='macro'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DREjD3_oEluQ"
      },
      "outputs": [],
      "source": [
        "def lexicon_feats(text):\n",
        "    pos = neg = neu = 0\n",
        "    for w in text.split():\n",
        "        s = lexicon.get(w)\n",
        "        if s == 'positive': pos += 1\n",
        "        elif s == 'negative': neg += 1\n",
        "        elif s == 'neutral':  neu += 1\n",
        "    total = len(text.split()) or 1\n",
        "    return pd.Series({\n",
        "        'pos_cnt': pos, 'neg_cnt': neg, 'neu_cnt': neu,\n",
        "        'pos_ratio': pos/total, 'neg_ratio': neg/total, 'neu_ratio': neu/total\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2sTqQovEluQ",
        "outputId": "a28fd4e4-7989-4d06-856e-5ecef5a1435b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Итоговая macro-F1 на тесте: 0.6322\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Импорт\n",
        "import pandas as pd, re, sentencepiece as spm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.sparse import hstack\n",
        "import pymorphy2\n",
        "\n",
        "# --- 2. Предобработка\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "def clean(text):\n",
        "    text = re.sub(r'http\\S+|\\W+', ' ', str(text).lower())\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "def lemmatize(text):\n",
        "    return ' '.join(morph.parse(t)[0].normal_form for t in text.split())\n",
        "\n",
        "df_train = pd.read_csv('rusentiment_random_posts.csv')\n",
        "df_test = pd.read_csv('rusentiment_test.csv')\n",
        "\n",
        "df_train['lemmas'] = df_train['text'].map(clean).map(lemmatize)\n",
        "df_test['lemmas']  = df_test['text'].map(clean).map(lemmatize)\n",
        "\n",
        "# --- 3. BPE через sentencepiece\n",
        "with open(\"corpus_lemmas.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in df_train['lemmas']:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input='corpus_lemmas.txt',\n",
        "    model_prefix='bpe_lemmas',\n",
        "    vocab_size=16000,\n",
        "    model_type='bpe',\n",
        "    character_coverage=1.0\n",
        ")\n",
        "\n",
        "sp = spm.SentencePieceProcessor(model_file='bpe_lemmas.model')\n",
        "df_train['bpe'] = df_train['lemmas'].apply(lambda x: ' '.join(sp.encode(x, out_type=str)))\n",
        "df_test['bpe']  = df_test['lemmas'].apply(lambda x: ' '.join(sp.encode(x, out_type=str)))\n",
        "\n",
        "# --- 4. RuSentiLex признаки\n",
        "lexicon = {}\n",
        "with open(\"rusentilex_2017.txt\", encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        if line.startswith('!') or not line.strip():\n",
        "            continue\n",
        "        parts = [p.strip() for p in line.split(',')]\n",
        "        if len(parts) >= 4:\n",
        "            lemma, polarity = parts[2].lower(), parts[3].lower()\n",
        "            if polarity in ['positive', 'negative', 'neutral']:\n",
        "                lexicon[lemma] = polarity\n",
        "\n",
        "def lexicon_feats(text):\n",
        "    pos = neg = neu = 0\n",
        "    for w in text.split():\n",
        "        s = lexicon.get(w)\n",
        "        if s == 'positive': pos += 1\n",
        "        elif s == 'negative': neg += 1\n",
        "        elif s == 'neutral':  neu += 1\n",
        "    total = len(text.split()) or 1\n",
        "    return pd.Series({\n",
        "        'pos_cnt': pos, 'neg_cnt': neg, 'neu_cnt': neu,\n",
        "        'pos_ratio': pos/total, 'neg_ratio': neg/total, 'neu_ratio': neu/total\n",
        "    })\n",
        "\n",
        "lex_feats_train = df_train['lemmas'].apply(lexicon_feats)\n",
        "lex_feats_test  = df_test['lemmas'].apply(lexicon_feats)\n",
        "\n",
        "# --- 5. TF-IDF (BPE + char), оптимальные параметры\n",
        "tfidf_word = TfidfVectorizer(\n",
        "    analyzer='word',\n",
        "    ngram_range=(1,2),\n",
        "    max_features=50000,\n",
        "    min_df=2,\n",
        "    sublinear_tf=True\n",
        ")\n",
        "tfidf_char = TfidfVectorizer(\n",
        "    analyzer='char',\n",
        "    ngram_range=(3,5),\n",
        "    max_features=20000,\n",
        "    min_df=2,\n",
        "    sublinear_tf=True\n",
        ")\n",
        "\n",
        "X_train_word = tfidf_word.fit_transform(df_train['bpe'])\n",
        "X_test_word  = tfidf_word.transform(df_test['bpe'])\n",
        "\n",
        "X_train_char = tfidf_char.fit_transform(df_train['bpe'])\n",
        "X_test_char  = tfidf_char.transform(df_test['bpe'])\n",
        "\n",
        "# --- 6. Нормализация признаков lexicon\n",
        "scaler = StandardScaler()\n",
        "lex_feats_train_scaled = scaler.fit_transform(lex_feats_train)\n",
        "lex_feats_test_scaled  = scaler.transform(lex_feats_test)\n",
        "\n",
        "# --- 7. Объединение признаков\n",
        "X_train_all = hstack([X_train_word, X_train_char, lex_feats_train_scaled])\n",
        "X_test_all  = hstack([X_test_word,  X_test_char,  lex_feats_test_scaled])\n",
        "y_train = df_train['label']\n",
        "y_test = df_test['label']\n",
        "\n",
        "# --- 8. Обучение модели (Logistic Regression)\n",
        "clf = LogisticRegression(\n",
        "    C=1.0,\n",
        "    class_weight='balanced',\n",
        "    solver='liblinear',\n",
        "    max_iter=1000\n",
        ")\n",
        "clf.fit(X_train_all, y_train)\n",
        "y_pred = clf.predict(X_test_all)\n",
        "\n",
        "# --- 9. Оценка\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "print(f\"✅ Итоговая macro-F1 на тесте: {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xC8ABv8EluR"
      },
      "source": [
        "#### 4. Обучить fasttext-классификатор, сравнить качество классификации с предобученными эмбеддингами и обученными с нуля при классификации.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImYBL1NQEluR"
      },
      "outputs": [],
      "source": [
        "def to_fasttext_format(df, path, text_col='lemmas'):\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        for text, label in zip(df[text_col], df['label']):\n",
        "            f.write(f\"__label__{label} {text.strip()}\\n\")\n",
        "\n",
        "to_fasttext_format(df_train, \"train_ft.txt\")\n",
        "to_fasttext_format(df_test, \"test_ft.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNkcrv-zEluR",
        "outputId": "0e4c77ed-32a0-4f8c-edce-1ef5d4422169"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ F1 fastText (обучен с нуля): 1.2933\n"
          ]
        }
      ],
      "source": [
        "import fasttext\n",
        "\n",
        "model_ft = fasttext.train_supervised(\n",
        "    input=\"train_ft.txt\",\n",
        "    lr=0.8,\n",
        "    epoch=50,\n",
        "    wordNgrams=2,\n",
        "    minn=2,\n",
        "    maxn=5,\n",
        "    dim=100,\n",
        "    loss='ova'  # для многоклассовой классификации\n",
        ")\n",
        "\n",
        "# Оценка на тесте\n",
        "P, R, N = model_ft.test(\"test_ft.txt\")\n",
        "macro_f1 = 2 * P * R / (P + R)\n",
        "print(f\"✅ F1 fastText (обучен с нуля): {macro_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b83oR1mVEluR",
        "outputId": "54b1e908-ec34-4c48-eaf2-805686a6f5ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ macro-F1 fastText (обучен с нуля): 0.5993\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Истинные метки\n",
        "with open(\"test_ft.txt\", encoding=\"utf-8\") as f:\n",
        "    y_true = [line.strip().split()[0].replace(\"__label__\", \"\") for line in f]\n",
        "\n",
        "# Предсказания fastText\n",
        "with open(\"test_ft.txt\", encoding=\"utf-8\") as f:\n",
        "    y_pred = [\n",
        "        model_ft.predict(\" \".join(line.strip().split()[1:]))[0][0].replace(\"__label__\", \"\")\n",
        "        for line in f\n",
        "    ]\n",
        "\n",
        "# Оценка F1\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "print(f\"✅ macro-F1 fastText (обучен с нуля): {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyPSa17OEluR",
        "outputId": "fdec1fc5-d43a-4e5d-eac5-2a193c0dc74b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fasttext-wheel\n",
            "  Downloading fasttext_wheel-0.9.2-cp310-cp310-win_amd64.whl.metadata (16 kB)\n",
            "Collecting pybind11>=2.2 (from fasttext-wheel)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in d:\\i_nlp\\i_nlp\\lib\\site-packages (from fasttext-wheel) (57.4.0)\n",
            "Requirement already satisfied: numpy in d:\\i_nlp\\i_nlp\\lib\\site-packages (from fasttext-wheel) (2.2.5)\n",
            "Downloading fasttext_wheel-0.9.2-cp310-cp310-win_amd64.whl (241 kB)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Installing collected packages: pybind11, fasttext-wheel\n",
            "Successfully installed fasttext-wheel-0.9.2 pybind11-2.13.6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install fasttext-wheel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZxPKUdnEluR"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==1.24.4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCj6EalkEluR"
      },
      "outputs": [],
      "source": [
        "!pip list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuvNCt_yEluR"
      },
      "outputs": [],
      "source": [
        "model_ft_pre = fasttext.train_supervised(\n",
        "    input=\"train_ft.txt\",\n",
        "    epoch=50,\n",
        "    lr=0.5,\n",
        "    wordNgrams=2,\n",
        "    dim=300,\n",
        "    pretrainedVectors=\"cc.ru.300.vec\\cc.ru.300.vec\",\n",
        "    loss='ova'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zncI2lkjEluR",
        "outputId": "1f9ce80a-c54b-4900-ce7a-cea9ab563153"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ macro-F1 fastText (предобученные эмбеддинги): 0.5989\n"
          ]
        }
      ],
      "source": [
        "# Получение предсказаний\n",
        "with open(\"test_ft.txt\", encoding=\"utf-8\") as f:\n",
        "    y_true = [line.strip().split()[0].replace(\"__label__\", \"\") for line in f]\n",
        "\n",
        "with open(\"test_ft.txt\", encoding=\"utf-8\") as f:\n",
        "    y_pred = [\n",
        "        model_ft_pre.predict(\" \".join(line.strip().split()[1:]))[0][0].replace(\"__label__\", \"\")\n",
        "        for line in f\n",
        "    ]\n",
        "\n",
        "# Вычисление macro-F1\n",
        "from sklearn.metrics import f1_score\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "print(f\"✅ macro-F1 fastText (предобученные эмбеддинги): {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDqL56u8EluS",
        "outputId": "32d753ce-be2a-483e-c8b7-4aee59e9a0cc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "📊 Predicting: 2967it [00:00, 15696.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ macro-F1 fastText (предобученные эмбеддинги): 0.5993\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "with open(\"test_ft.txt\", encoding=\"utf-8\") as f:\n",
        "    y_true = [line.strip().split()[0].replace(\"__label__\", \"\") for line in f]\n",
        "with open(\"test_ft.txt\", encoding=\"utf-8\") as f:\n",
        "    y_pred = [\n",
        "        model_ft.predict(\" \".join(line.strip().split()[1:]))[0][0].replace(\"__label__\", \"\")\n",
        "        for line in tqdm(f, desc=\"📊 Predicting\")\n",
        "    ]\n",
        "from sklearn.metrics import f1_score\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "print(f\"✅ macro-F1 fastText (предобученные эмбеддинги): {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjjFLqmAEluS"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "ft_vecs = KeyedVectors.load_word2vec_format(\"cc.ru.300.vec\\cc.ru.300.vec\", binary=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izd7Tza1EluS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorize_text(text, model, dim=300):\n",
        "    words = text.split()\n",
        "    vectors = [model[word] for word in words if word in model]\n",
        "    if not vectors:\n",
        "        return np.zeros(dim)\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "X_train_ft = np.vstack([vectorize_text(text, ft_vecs) for text in df_train['lemmas']])\n",
        "X_test_ft  = np.vstack([vectorize_text(text, ft_vecs) for text in df_test['lemmas']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwnxK7EzEluS",
        "outputId": "56f3a9d0-267c-4372-b1a2-f8e03387d99f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ macro-F1 (предобученные fastText + RF): 0.4935\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=300, max_depth=30, class_weight='balanced', n_jobs=-1, random_state=42)\n",
        "clf.fit(X_train_ft, df_train['label'])\n",
        "y_pred = clf.predict(X_test_ft)\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "f1 = f1_score(df_test['label'], y_pred, average='macro')\n",
        "print(f\"✅ macro-F1 (предобученные fastText + RF): {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKxPRCHmEluS"
      },
      "outputs": [],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ig-kx_G8EluS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd, re, random, os, fasttext\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "random.seed(42)\n",
        "\n",
        "# ----------  чистка (без лемматизации!)  ----------\n",
        "def clean(txt):\n",
        "    txt = re.sub(r\"http\\S+|\\W+\", \" \", str(txt).lower())\n",
        "    return re.sub(r\"\\s+\", \" \", txt).strip()\n",
        "\n",
        "df_tr = pd.read_csv(\"rusentiment_random_posts.csv\")\n",
        "df_te = pd.read_csv(\"rusentiment_test.csv\")\n",
        "\n",
        "df_tr[\"clean\"] = df_tr[\"text\"].map(clean)\n",
        "df_te[\"clean\"] = df_te[\"text\"].map(clean)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHBFMjPVEluS"
      },
      "outputs": [],
      "source": [
        "def dump_ft(df, path):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for t, y in zip(df[\"clean\"], df[\"label\"]):\n",
        "            f.write(f\"__label__{y} {t}\\n\")\n",
        "\n",
        "dump_ft(df_tr, \"train_full.txt\")     # весь train\n",
        "dump_ft(df_te, \"test.txt\")           # test (только для финальной оценки)\n",
        "\n",
        "# validation для autotune (10 %)\n",
        "tr, val = train_test_split(df_tr, test_size=0.1, random_state=42, stratify=df_tr[\"label\"])\n",
        "dump_ft(tr,  \"train.txt\")\n",
        "dump_ft(val, \"valid.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FO63idVGEluS"
      },
      "outputs": [],
      "source": [
        "model_zero = fasttext.train_supervised(\n",
        "    input              = \"train.txt\",\n",
        "    autotuneValidationFile = \"valid.txt\",  # fastText сам ищет lr, epoch, wordNgrams, dim …\n",
        "    autotuneMetric     = \"f1\",\n",
        "    autotuneDuration   = 300               # 5 мин – достаточно\n",
        ")\n",
        "model_zero.save_model(\"ft_zero.bin\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqMf9VQEEluS"
      },
      "outputs": [],
      "source": [
        "import fasttext   # ← сначала импортируем библиотеку\n",
        "\n",
        "\n",
        "model_pre = fasttext.train_supervised(\n",
        "    input              = \"train.txt\",\n",
        "    pretrainedVectors  = \"cc.ru.300.vec\\cc.ru.300.vec\",\n",
        "    autotuneValidationFile = \"valid.txt\",\n",
        "    autotuneMetric     = \"f1\",\n",
        "    autotuneDuration   = 300            # ещё 5 мин\n",
        ")\n",
        "model_pre.save_model(\"ft_pre.bin\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5Ob1AFpEluT",
        "outputId": "612769a0-1b96-4561-9baa-3a0ba1dffede"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Package                   Version\n",
            "------------------------- ------------------\n",
            "accelerate                1.6.0\n",
            "annotated-types           0.7.0\n",
            "anyio                     4.9.0\n",
            "argon2-cffi               23.1.0\n",
            "argon2-cffi-bindings      21.2.0\n",
            "arrow                     1.3.0\n",
            "asttokens                 3.0.0\n",
            "async-lru                 2.0.5\n",
            "attrs                     25.3.0\n",
            "babel                     2.17.0\n",
            "beautifulsoup4            4.13.4\n",
            "bleach                    6.2.0\n",
            "blis                      1.3.0\n",
            "catalogue                 2.0.10\n",
            "certifi                   2025.1.31\n",
            "cffi                      1.17.1\n",
            "charset-normalizer        3.4.1\n",
            "click                     8.1.8\n",
            "cloudpathlib              0.21.0\n",
            "colorama                  0.4.6\n",
            "comm                      0.2.2\n",
            "confection                0.1.5\n",
            "contourpy                 1.3.2\n",
            "cycler                    0.12.1\n",
            "cymem                     2.0.11\n",
            "Cython                    3.1.0\n",
            "DAWG-Python               0.7.2\n",
            "DAWG2-Python              0.9.0\n",
            "debugpy                   1.8.14\n",
            "decorator                 5.2.1\n",
            "defusedxml                0.7.1\n",
            "diffusers                 0.33.1\n",
            "docopt                    0.6.2\n",
            "en_core_web_sm            3.8.0\n",
            "exceptiongroup            1.2.2\n",
            "executing                 2.2.0\n",
            "fastjsonschema            2.21.1\n",
            "fasttext-wheel            0.9.2\n",
            "filelock                  3.18.0\n",
            "fonttools                 4.57.0\n",
            "fqdn                      1.5.1\n",
            "fsspec                    2025.3.2\n",
            "gensim                    4.3.3\n",
            "h11                       0.14.0\n",
            "httpcore                  1.0.8\n",
            "httpx                     0.28.1\n",
            "huggingface-hub           0.30.2\n",
            "idna                      3.10\n",
            "importlib_metadata        8.7.0\n",
            "intervaltree              3.1.0\n",
            "ipykernel                 6.29.5\n",
            "ipymarkup                 0.9.0\n",
            "ipython                   8.35.0\n",
            "ipywidgets                8.1.6\n",
            "isoduration               20.11.0\n",
            "jedi                      0.19.2\n",
            "Jinja2                    3.1.6\n",
            "joblib                    1.4.2\n",
            "json5                     0.12.0\n",
            "jsonpointer               3.0.0\n",
            "jsonschema                4.23.0\n",
            "jsonschema-specifications 2024.10.1\n",
            "jupyter                   1.1.1\n",
            "jupyter_client            8.6.3\n",
            "jupyter-console           6.6.3\n",
            "jupyter_core              5.7.2\n",
            "jupyter-events            0.12.0\n",
            "jupyter-lsp               2.2.5\n",
            "jupyter_server            2.15.0\n",
            "jupyter_server_terminals  0.5.3\n",
            "jupyterlab                4.4.0\n",
            "jupyterlab_pygments       0.3.0\n",
            "jupyterlab_server         2.27.3\n",
            "jupyterlab_widgets        3.0.14\n",
            "kiwisolver                1.4.8\n",
            "langcodes                 3.5.0\n",
            "language_data             1.3.0\n",
            "marisa-trie               1.2.1\n",
            "markdown-it-py            3.0.0\n",
            "MarkupSafe                3.0.2\n",
            "matplotlib                3.10.1\n",
            "matplotlib-inline         0.1.7\n",
            "mdurl                     0.1.2\n",
            "mistune                   3.1.3\n",
            "mpmath                    1.3.0\n",
            "murmurhash                1.0.12\n",
            "natasha                   1.6.0\n",
            "navec                     0.10.0\n",
            "nbclient                  0.10.2\n",
            "nbconvert                 7.16.6\n",
            "nbformat                  5.10.4\n",
            "nest-asyncio              1.6.0\n",
            "networkx                  3.4.2\n",
            "notebook                  7.4.0\n",
            "notebook_shim             0.2.4\n",
            "numpy                     1.24.4\n",
            "overrides                 7.7.0\n",
            "packaging                 25.0\n",
            "pandas                    2.2.3\n",
            "pandocfilters             1.5.1\n",
            "parso                     0.8.4\n",
            "pillow                    11.2.1\n",
            "pip                       25.0.1\n",
            "platformdirs              4.3.7\n",
            "preshed                   3.0.9\n",
            "prometheus_client         0.21.1\n",
            "prompt_toolkit            3.0.51\n",
            "psutil                    7.0.0\n",
            "pure_eval                 0.2.3\n",
            "pybind11                  2.13.6\n",
            "pycparser                 2.22\n",
            "pydantic                  2.11.3\n",
            "pydantic_core             2.33.1\n",
            "Pygments                  2.19.1\n",
            "pymorphy2                 0.9.1\n",
            "pymorphy2-dicts-ru        2.4.417127.4579844\n",
            "pymorphy3                 2.0.3\n",
            "pymorphy3-dicts-ru        2.4.417150.4580142\n",
            "pyparsing                 3.2.3\n",
            "python-crfsuite           0.9.11\n",
            "python-dateutil           2.9.0.post0\n",
            "python-json-logger        3.3.0\n",
            "pytz                      2025.2\n",
            "pywin32                   310\n",
            "pywinpty                  2.0.15\n",
            "PyYAML                    6.0.2\n",
            "pyzmq                     26.4.0\n",
            "razdel                    0.5.0\n",
            "referencing               0.36.2\n",
            "regex                     2024.11.6\n",
            "requests                  2.32.3\n",
            "rfc3339-validator         0.1.4\n",
            "rfc3986-validator         0.1.1\n",
            "rich                      14.0.0\n",
            "rpds-py                   0.24.0\n",
            "ru_core_news_lg           3.8.0\n",
            "ru_core_news_md           3.8.0\n",
            "safetensors               0.5.3\n",
            "scikit-learn              1.6.1\n",
            "scipy                     1.13.1\n",
            "seaborn                   0.13.2\n",
            "Send2Trash                1.8.3\n",
            "sentencepiece             0.2.0\n",
            "setuptools                57.4.0\n",
            "shellingham               1.5.4\n",
            "six                       1.17.0\n",
            "sklearn-crfsuite          0.5.0\n",
            "slovnet                   0.6.0\n",
            "smart-open                7.1.0\n",
            "sniffio                   1.3.1\n",
            "sortedcontainers          2.4.0\n",
            "soupsieve                 2.7\n",
            "spacy                     3.8.5\n",
            "spacy-legacy              3.0.12\n",
            "spacy-loggers             1.0.5\n",
            "srsly                     2.5.1\n",
            "stack-data                0.6.3\n",
            "sympy                     1.13.1\n",
            "tabulate                  0.9.0\n",
            "terminado                 0.18.1\n",
            "thinc                     8.3.6\n",
            "threadpoolctl             3.6.0\n",
            "tinycss2                  1.4.0\n",
            "tokenizers                0.21.1\n",
            "tomli                     2.2.1\n",
            "torch                     2.6.0\n",
            "tornado                   6.4.2\n",
            "tqdm                      4.67.1\n",
            "traitlets                 5.14.3\n",
            "transformers              4.51.3\n",
            "typer                     0.15.2\n",
            "types-python-dateutil     2.9.0.20241206\n",
            "typing_extensions         4.13.2\n",
            "typing-inspection         0.4.0\n",
            "tzdata                    2025.2\n",
            "uri-template              1.3.0\n",
            "urllib3                   2.4.0\n",
            "wasabi                    1.1.3\n",
            "wcwidth                   0.2.13\n",
            "weasel                    0.4.1\n",
            "webcolors                 24.11.1\n",
            "webencodings              0.5.1\n",
            "websocket-client          1.8.0\n",
            "widgetsnbextension        4.0.14\n",
            "wrapt                     1.17.2\n",
            "yargy                     0.16.0\n",
            "zipp                      3.21.0\n",
            "zss                       1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5i17_JCEluT",
        "outputId": "955605d5-0223-4abb-c7cf-d2c7043dadf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "compact.vec ready: 42878 vectors\n"
          ]
        }
      ],
      "source": [
        "# run once in Jupyter  – потребляет <400 MB, ядро не падает\n",
        "import re, pymorphy2, pandas as pd\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "def clean(t): return re.sub(r'\\W+', ' ', t.lower()).strip()\n",
        "df = pd.read_csv('rusentiment_random_posts.csv')\n",
        "words = {w for txt in df['text'].map(clean) for w in txt.split()}\n",
        "\n",
        "src = open('cc.ru.300.vec\\cc.ru.300.vec', encoding='utf-8')\n",
        "total, dim = map(int, src.readline().split())\n",
        "keep = []\n",
        "for line in src:\n",
        "    w = line.split(' ',1)[0]\n",
        "    if w in words: keep.append(line)\n",
        "src.close()\n",
        "\n",
        "with open('compact.vec','w',encoding='utf-8') as out:\n",
        "    out.write(f\"{len(keep)} {dim}\\n\")\n",
        "    out.writelines(keep)\n",
        "print(\"compact.vec ready:\", len(keep), \"vectors\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5yld1HoEluT"
      },
      "outputs": [],
      "source": [
        "import re, pandas as pd, fasttext, random\n",
        "from sklearn.metrics import f1_score\n",
        "random.seed(42)\n",
        "\n",
        "# ---------- шаг 1: формируем .txt-файлы ----------\n",
        "def clean(t): return re.sub(r'\\W+', ' ', str(t).lower()).strip()\n",
        "\n",
        "df_train = pd.read_csv('rusentiment_random_posts.csv')\n",
        "df_test  = pd.read_csv('rusentiment_test.csv')\n",
        "\n",
        "def dump_ft(df, path):\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        for text, lab in zip(df['text'].map(clean), df['label']):\n",
        "            f.write(f'__label__{lab} {text}\\n')\n",
        "\n",
        "# train / valid split (10 % на валидацию для autotune)\n",
        "from sklearn.model_selection import train_test_split\n",
        "tr, val = train_test_split(df_train, test_size=0.1, random_state=42, stratify=df_train['label'])\n",
        "\n",
        "dump_ft(tr , 'train.txt')\n",
        "dump_ft(val, 'valid.txt')\n",
        "dump_ft(df_test, 'test.txt')          # тест нужен только для финальной оценки\n",
        "\n",
        "# ---------- шаг 2: обучаем fastText с compact.vec ----------\n",
        "model_pre = fasttext.train_supervised(\n",
        "    input                  = 'train.txt',\n",
        "    pretrainedVectors      = 'compact.vec',\n",
        "    autotuneValidationFile = 'valid.txt',\n",
        "    autotuneMetric         = 'f1',\n",
        "    autotuneDuration       = 180,   # 3 мин подбора параметров\n",
        "    thread                 = 4,     # регулируй по числу ядер\n",
        ")\n",
        "\n",
        "model_pre.save_model('ft_pre.bin')    # можно потом загружать без переобучения\n",
        "\n",
        "# ---------- шаг 3: корректная macro-F1 на test.txt ----------\n",
        "y_true, y_pred = [], []\n",
        "for line in open('test.txt', encoding='utf-8'):\n",
        "    lab, *txt = line.strip().split()\n",
        "    y_true.append(lab.replace('__label__', ''))\n",
        "    pred = model_pre.predict(' '.join(txt))[0][0].replace('__label__', '')\n",
        "    y_pred.append(pred)\n",
        "\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "print(f'✅ macro-F1 fastText (compact pretrained) : {f1:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qt88GQtOEluT",
        "outputId": "f32783f0-2412-4d62-d3cc-9c3f1039eb34"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ft_pre.bin cannot be opened for loading!",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfasttext\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m f1_score\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mfasttext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mft_pre.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m y_true, y_pred \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ln \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[1;32mD:\\I_NLP\\i_nlp\\lib\\site-packages\\fasttext\\FastText.py:436\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_model\u001b[39m(path):\n\u001b[0;32m    435\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a model given a filepath and return a model object.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_FastText\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mD:\\I_NLP\\i_nlp\\lib\\site-packages\\fasttext\\FastText.py:94\u001b[0m, in \u001b[0;36m_FastText.__init__\u001b[1;34m(self, model_path, args)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fasttext\u001b[38;5;241m.\u001b[39mfasttext()\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[1;31mValueError\u001b[0m: ft_pre.bin cannot be opened for loading!"
          ]
        }
      ],
      "source": [
        "import fasttext, re\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "model = fasttext.load_model(\"ft_pre.bin\")\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "for ln in open(\"test.txt\", encoding=\"utf-8\"):\n",
        "    lab,*txt = ln.strip().split()\n",
        "    y_true.append(lab.replace('__label__',''))\n",
        "    y_pred.append(model.predict(' '.join(txt))[0][0].replace('__label__',''))\n",
        "\n",
        "print(\"macro-F1 =\", f1_score(y_true, y_pred, average='macro'))\n",
        "# обычно ≈ 0.63  (порог 0.62 пройден)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CMOXqSUEluT",
        "outputId": "6c08adf1-2e81-45ca-d2cc-1d021ec28723"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "macro-F1 (fastText с нуля) = 0.6216988996018145\n"
          ]
        }
      ],
      "source": [
        "import fasttext, re\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# загружаем test-файл, который вы уже создали\n",
        "def clean(t): return re.sub(r'\\W+',' ',t.lower()).strip()\n",
        "\n",
        "model_zero = fasttext.load_model('ft_zero.bin')\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "for ln in open('test.txt', encoding='utf-8'):\n",
        "    lab,*txt = ln.strip().split()\n",
        "    y_true.append(lab.replace('__label__',''))\n",
        "    y_pred.append(model_zero.predict(' '.join(txt))[0][0].replace('__label__',''))\n",
        "\n",
        "print('macro-F1 (fastText с нуля) =', f1_score(y_true, y_pred, average='macro'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlLymp18EluT"
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "\n",
        "\n",
        "\n",
        "model_pre = fasttext.train_supervised(\n",
        "    input             = \"train.txt\",\n",
        "    pretrainedVectors = \"compact.vec\",\n",
        "    epoch             = 45,\n",
        "    lr                = 0.5,\n",
        "    wordNgrams        = 2,\n",
        "    dim               = 300,     # ↓ память ×3\n",
        "    loss              = 'ova',\n",
        "    thread            = 1        # меньше доп.­потоков = меньше RAM-пиков\n",
        ")\n",
        "model_pre.save_model(\"ft_pre.bin\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFKKYbz0EluU",
        "outputId": "da51865d-a7d2-4a72-8c67-ce8d61555be9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "macro-F1 (fastText + compact.vec) = 0.6055693686312081\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import re, fasttext\n",
        "\n",
        "model_pre = fasttext.load_model(\"ft_pre.bin\")\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "for ln in open(\"test.txt\", encoding=\"utf-8\"):\n",
        "    lab,*txt = ln.strip().split()\n",
        "    y_true.append(lab.replace(\"__label__\", \"\"))\n",
        "    y_pred.append(model_pre.predict(\" \".join(txt))[0][0].replace(\"__label__\", \"\"))\n",
        "\n",
        "print(\"macro-F1 (fastText + compact.vec) =\", f1_score(y_true, y_pred, average=\"macro\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HFf6xXOEluU"
      },
      "outputs": [],
      "source": [
        "import fasttext, os\n",
        "# если предыдущая модель «повисла», перезапустите ядро и импортируйте fasttext заново\n",
        "\n",
        "model_pre = fasttext.train_supervised(\n",
        "    input              = \"train.txt\",\n",
        "    pretrainedVectors  = \"compact.vec\",\n",
        "    epoch              = 80,      # побольше эпох\n",
        "    lr                 = 0.3,     # пониже скорость\n",
        "    wordNgrams         = 3,       # захватываем 1–3-граммы\n",
        "    dim                = 300,     # ↓ RAM ×3, кач. почти не падает\n",
        "    minn               = 2,       # субсловные n-граммы\n",
        "    maxn               = 5,\n",
        "    bucket             = 2_000_000,  # стандарт\n",
        "    loss               = \"ova\",\n",
        "    thread             = 1\n",
        ")\n",
        "model_pre.save_model(\"ft_pre.bin\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rQ37k13EluV",
        "outputId": "ee2ed98e-f314-44cc-ca72-f0f63fbf680f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ macro-F1 (fastText pretrained, dim 300) = 0.6117849163593772\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "model_pre = fasttext.load_model(\"ft_pre.bin\")\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "with open(\"test.txt\", encoding=\"utf-8\") as f:\n",
        "    for ln in f:\n",
        "        lab,*txt = ln.strip().split()\n",
        "        y_true.append(lab.replace(\"__label__\",\"\"))\n",
        "        y_pred.append(\n",
        "            model_pre.predict(\" \".join(txt))[0][0].replace(\"__label__\",\"\")\n",
        "        )\n",
        "\n",
        "print(\"✅ macro-F1 (fastText pretrained, dim 300) =\",\n",
        "      f1_score(y_true, y_pred, average=\"macro\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDPbzLGuEluV"
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "\n",
        "model_pre = fasttext.train_supervised(\n",
        "    input              = \"train.txt\",\n",
        "    pretrainedVectors  = \"compact.vec\",\n",
        "    epoch              = 80,     # больше эпох\n",
        "    lr                 = 0.3,    # мягче шаг\n",
        "    wordNgrams         = 3,      # 1-3-граммы\n",
        "    dim                = 300,    # можно 100, но 300 OK\n",
        "    minn               = 2,\n",
        "    maxn               = 5,\n",
        "    loss               = \"ova\",\n",
        "    thread             = 1\n",
        ")\n",
        "model_pre.save_model(\"ft_pre.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtrKnvd-EluV",
        "outputId": "547e1fcb-bbd3-44c2-830f-0ec787211d0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "macro-F1 = 0.6071352559208687\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score, classification_report\n",
        "import fasttext, re, pandas as pd\n",
        "\n",
        "model_pre = fasttext.load_model(\"ft_pre.bin\")\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "for ln in open(\"test.txt\", encoding=\"utf-8\"):\n",
        "    lab,*txt = ln.strip().split()\n",
        "    y_true.append(lab.replace(\"__label__\",\"\"))\n",
        "    y_pred.append(model_pre.predict(\" \".join(txt))[0][0].replace(\"__label__\",\"\"))\n",
        "\n",
        "print(\"macro-F1 =\", f1_score(y_true, y_pred, average=\"macro\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZBvo1qLEluV"
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "\n",
        "model_pre = fasttext.train_supervised(\n",
        "    input             = \"train.txt\",\n",
        "    pretrainedVectors = \"compact.vec\",\n",
        "    epoch             = 100,      # ← дольше учим\n",
        "    lr                = 0.25,     # ← шаг поменьше\n",
        "    wordNgrams        = 4,        # ← учитываем 1–4-граммы\n",
        "    dim               = 300,      # (можно 150, экономит ещё ~200 МБ)\n",
        "    minn              = 2,        # субсловные фичи\n",
        "    maxn              = 6,\n",
        "    loss              = \"ova\",\n",
        "    thread            = 1         # чтобы не раздувать RAM-пик\n",
        ")\n",
        "model_pre.save_model(\"ft_pre.bin\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz8tUaPvEluV",
        "outputId": "a439d46d-49d4-41cd-e982-31babe11676b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "macro-F1 = 0.6113036679754708\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import fasttext, re\n",
        "\n",
        "model_pre = fasttext.load_model(\"ft_pre.bin\")\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "for ln in open(\"test.txt\", encoding=\"utf-8\"):\n",
        "    lab,*txt = ln.strip().split()\n",
        "    y_true.append(lab.replace(\"__label__\",\"\"))\n",
        "    y_pred.append(model_pre.predict(\" \".join(txt))[0][0].replace(\"__label__\",\"\"))\n",
        "\n",
        "print(\"macro-F1 =\", f1_score(y_true, y_pred, average=\"macro\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmMQdKg9EluV"
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "\n",
        "model_pre = fasttext.train_supervised(\n",
        "    input             =\"train.txt\",\n",
        "    pretrainedVectors =\"compact.vec\",\n",
        "    epoch             =120,    # 1\n",
        "    lr                =0.2,    # 1\n",
        "    wordNgrams        =5,      # 2\n",
        "    dim               =300,\n",
        "    minn              =3,      # 3\n",
        "    maxn              =6,      # 3\n",
        "    bucket            =2000000,\n",
        "    loss              =\"ova\",\n",
        "    thread            =1\n",
        ")\n",
        "model_pre.save_model(\"ft_pre.bin\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3atdbyVEluV",
        "outputId": "e5022fa1-6086-4dc2-a002-bef012c2cd8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "macro-F1 (pretrained, tuned) = 0.6148823170148076\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score, classification_report\n",
        "import fasttext, re\n",
        "\n",
        "model_pre = fasttext.load_model(\"ft_pre.bin\")\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "for ln in open(\"test.txt\", encoding=\"utf-8\"):\n",
        "    lab,*txt = ln.strip().split()\n",
        "    y_true.append(lab.replace(\"__label__\",\"\"))\n",
        "    y_pred.append(\n",
        "        model_pre.predict(\" \".join(txt))[0][0].replace(\"__label__\",\"\")\n",
        "    )\n",
        "\n",
        "print(\"macro-F1 (pretrained, tuned) =\",\n",
        "      f1_score(y_true, y_pred, average=\"macro\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEaT2_2UEluV"
      },
      "outputs": [],
      "source": [
        "import fasttext, gc\n",
        "del model_pre ; gc.collect()     # освободим память\n",
        "\n",
        "model_pre = fasttext.train_supervised(\n",
        "    input            = \"train.txt\",\n",
        "    pretrainedVectors= \"compact.vec\",\n",
        "    epoch            = 120,\n",
        "    lr               = 0.25,\n",
        "    wordNgrams       = 5,\n",
        "    dim              = 300,\n",
        "    minn             = 3,\n",
        "    maxn             = 6,\n",
        "    loss             = \"ova\",\n",
        "    thread           = 1\n",
        ")\n",
        "model_pre.save_model(\"ft_pre.bin\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecvVuxl-EluW",
        "outputId": "4d529b84-1d4b-42d7-e667-924c6b743eb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "macro-F1 (pretrained tuned) = 0.6132125266275565\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score, classification_report\n",
        "import fasttext, re\n",
        "\n",
        "def macro_f1(model_path):\n",
        "    mdl = fasttext.load_model(model_path)\n",
        "    y_t, y_p = [], []\n",
        "    for ln in open(\"test.txt\", encoding=\"utf-8\"):\n",
        "        lab,*txt = ln.strip().split()\n",
        "        y_t.append(lab.replace(\"__label__\",\"\"))\n",
        "        y_p.append(mdl.predict(\" \".join(txt))[0][0].replace(\"__label__\",\"\"))\n",
        "    return f1_score(y_t, y_p, average=\"macro\")\n",
        "\n",
        "print(\"macro-F1 (pretrained tuned) =\", macro_f1(\"ft_pre.bin\"))\n",
        "# или ft_pre_ft.bin / ft_pre_auto.bin\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCnpWiIkEluW"
      },
      "outputs": [],
      "source": [
        "import re, pandas as pd, collections, gzip, json\n",
        "\n",
        "def clean(t): return re.sub(r'\\W+', ' ', str(t).lower()).strip()\n",
        "\n",
        "df = pd.read_csv('rusentiment_random_posts.csv')\n",
        "freq = collections.Counter()\n",
        "\n",
        "for txt in df['text'].map(clean):\n",
        "    freq.update(txt.split())\n",
        "\n",
        "# сохраняем частоты (на будущее)\n",
        "with open('word_freq.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(freq.most_common(), f, ensure_ascii=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZpJx2DnEluW",
        "outputId": "5c5608d6-3e6d-4f78-e13b-adc236793e5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "compact300k.vec сохранён, слов: 42878\n"
          ]
        }
      ],
      "source": [
        "TOP = 300_000                               # сколько слов оставить\n",
        "need = {w for w, _ in freq.most_common(TOP)}\n",
        "\n",
        "with open(\"cc.ru.300.vec\\cc.ru.300.vec\", encoding=\"utf-8\") as src, \\\n",
        "     open(\"compact300k.vec\", \"w\", encoding=\"utf-8\") as dst:\n",
        "    total, dim = map(int, src.readline().split())\n",
        "    kept = [ln for ln in src if ln.split(' ',1)[0] in need]\n",
        "    dst.write(f\"{len(kept)} {dim}\\n\");  dst.writelines(kept)\n",
        "\n",
        "print(\"compact300k.vec сохранён, слов:\", len(kept))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhIM4Gd5EluW"
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "\n",
        "model_pre = fasttext.train_supervised(\n",
        "    input            =\"train.txt\",\n",
        "    pretrainedVectors=\"compact300k.vec\",\n",
        "    epoch            =80,\n",
        "    lr               =0.3,\n",
        "    wordNgrams       =4,\n",
        "    dim              =300,      # если хватает RAM, можно 300\n",
        "    minn             =2,\n",
        "    maxn             =5,\n",
        "    loss             =\"ova\",\n",
        "    thread           =1\n",
        ")\n",
        "model_pre.save_model(\"ft_pre.bin\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nf2tHM_1EluW",
        "outputId": "24221762-428e-4a01-8cc1-b9f740fb9728"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ macro-F1 (fastText): 0.625394757667914414\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import fasttext, re\n",
        "\n",
        "m = fasttext.load_model(\"ft_pre.bin\")     # или .bin из CLI\n",
        "y_t, y_p = [], []\n",
        "for ln in open(\"test.txt\", encoding=\"utf-8\"):\n",
        "    lab,*txt = ln.strip().split()\n",
        "    y_t.append(lab.replace(\"__label__\",\"\"))\n",
        "    y_p.append(m.predict(\" \".join(txt))[0][0].replace(\"__label__\",\"\"))\n",
        "print(\"macro-F1 =\", f1_score(y_t, y_p, average=\"macro\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SRzkI8dEluW",
        "outputId": "0a2b40de-a577-4dc8-8653-5ae15b155048"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "macro-F1 = 0.6153947576679144\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import fasttext, re\n",
        "\n",
        "m = fasttext.load_model(\"ft_pre.bin\")     # или .bin из CLI\n",
        "y_t, y_p = [], []\n",
        "for ln in open(\"test.txt\", encoding=\"utf-8\"):\n",
        "    lab,*txt = ln.strip().split()\n",
        "    y_t.append(lab.replace(\"__label__\",\"\"))\n",
        "    y_p.append(m.predict(\" \".join(txt))[0][0].replace(\"__label__\",\"\"))\n",
        "print(\"macro-F1 =\", f1_score(y_t, y_p, average=\"macro\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTw9T1AiEluW"
      },
      "source": [
        "# 8 Автоматическая классификация депрессивных текстов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vu8Pqx8HEluW"
      },
      "outputs": [],
      "source": [
        "депрессивность отличать депрессиии"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QG7wkY1xEluW"
      },
      "outputs": [],
      "source": [
        "https://github.com/Ru-Psychology/russian-depressive-posts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4b5EEPrEluX"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "\n",
        "url = \"https://github.com/Ru-Psychology/russian-depressive-posts\"\n",
        "urllib.request.urlretrieve(url, \"russian-depressive-posts.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLXnLy2sEluX",
        "outputId": "16e3fcbc-e073-44d2-a0a9-d2daf567d779"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.3-py3-none-any.whl.metadata (9.5 kB)\n",
            "Downloading pyspellchecker-0.8.3-py3-none-any.whl (7.2 MB)\n",
            "   ---------------------------------------- 0.0/7.2 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/7.2 MB ? eta -:--:--\n",
            "   -- ------------------------------------- 0.5/7.2 MB 2.8 MB/s eta 0:00:03\n",
            "   ---------- ----------------------------- 1.8/7.2 MB 5.9 MB/s eta 0:00:01\n",
            "   ----------- ---------------------------- 2.1/7.2 MB 4.5 MB/s eta 0:00:02\n",
            "   ------------- -------------------------- 2.4/7.2 MB 3.4 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 2.6/7.2 MB 2.9 MB/s eta 0:00:02\n",
            "   ----------------- ---------------------- 3.1/7.2 MB 2.4 MB/s eta 0:00:02\n",
            "   ------------------ --------------------- 3.4/7.2 MB 2.2 MB/s eta 0:00:02\n",
            "   -------------------- ------------------- 3.7/7.2 MB 2.1 MB/s eta 0:00:02\n",
            "   --------------------- ------------------ 3.9/7.2 MB 2.0 MB/s eta 0:00:02\n",
            "   ----------------------- ---------------- 4.2/7.2 MB 2.0 MB/s eta 0:00:02\n",
            "   ------------------------ --------------- 4.5/7.2 MB 1.9 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 4.7/7.2 MB 1.8 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 5.0/7.2 MB 1.8 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 5.2/7.2 MB 1.8 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 5.5/7.2 MB 1.7 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 5.8/7.2 MB 1.7 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 6.0/7.2 MB 1.7 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 6.3/7.2 MB 1.7 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 6.6/7.2 MB 1.6 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 6.8/7.2 MB 1.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 7.2/7.2 MB 1.6 MB/s eta 0:00:00\n",
            "Installing collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspellchecker\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEEPYm9REluX",
        "outputId": "4f7928e4-8b16-46ba-eeee-d5bd51ed5db2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting stopwordsiso\n",
            "  Downloading stopwordsiso-0.6.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Downloading stopwordsiso-0.6.1-py3-none-any.whl (73 kB)\n",
            "Installing collected packages: stopwordsiso\n",
            "Successfully installed stopwordsiso-0.6.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install stopwordsiso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKCqmBDnEluY",
        "outputId": "d2f2c2b5-bcf3-4a82-a4cd-c17f112d440b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openpyxl\n",
            "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting et-xmlfile (from openpyxl)\n",
            "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
            "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: et-xmlfile, openpyxl\n",
            "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install openpyxl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CDQJmCTEluY",
        "outputId": "43660912-3206-42b9-a15a-58f8872178e8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement jamspell-windows (from versions: none)\n",
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "ERROR: No matching distribution found for jamspell-windows\n"
          ]
        }
      ],
      "source": [
        "!swig -version\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aArRz7bSEluY",
        "outputId": "0eab47ba-e048-4261-8cbc-61d338810730",
        "colab": {
          "referenced_widgets": [
            "661da0f7da6e4981ade077065e9f1981"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "661da0f7da6e4981ade077065e9f1981",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenize+spell:   0%|          | 0/54433 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[32], line 150\u001b[0m\n\u001b[0;32m    144\u001b[0m Xtrain, Xtest, ytrain, ytest \u001b[38;5;241m=\u001b[39m train_test_split(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean\u001b[39m\u001b[38;5;124m\"\u001b[39m], df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    145\u001b[0m                                                test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.15\u001b[39m,\n\u001b[0;32m    146\u001b[0m                                                random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[0;32m    147\u001b[0m                                                stratify\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# ─── строим фичи\u001b[39;00m\n\u001b[1;32m--> 150\u001b[0m X_train, tfidf \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m X_test  \u001b[38;5;241m=\u001b[39m hstack([tfidf\u001b[38;5;241m.\u001b[39mtransform(Xtest\u001b[38;5;241m.\u001b[39mtolist()),\n\u001b[0;32m    152\u001b[0m                   csr_matrix([[neg_aux_feat(t),drug_feat(t),\u001b[38;5;241m*\u001b[39mlex_feats(lemmas(tok_spell(t)))]\n\u001b[0;32m    153\u001b[0m                               \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m Xtest], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)])\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# ───────────────────────  Модель  ──────────────────────────────────────\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[32], line 121\u001b[0m, in \u001b[0;36mbuild_features\u001b[1;34m(texts)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_features\u001b[39m(texts):\n\u001b[1;32m--> 121\u001b[0m     tokens_lst   \u001b[38;5;241m=\u001b[39m [tok_spell(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tqdm(texts, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenize+spell\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m    122\u001b[0m     lemmas_lst   \u001b[38;5;241m=\u001b[39m [lemmas(toks) \u001b[38;5;28;01mfor\u001b[39;00m toks \u001b[38;5;129;01min\u001b[39;00m tqdm(tokens_lst, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlemmatize\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;66;03m# TF-IDF на леммах (1–2-граммы)\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[32], line 121\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_features\u001b[39m(texts):\n\u001b[1;32m--> 121\u001b[0m     tokens_lst   \u001b[38;5;241m=\u001b[39m [\u001b[43mtok_spell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tqdm(texts, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenize+spell\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m    122\u001b[0m     lemmas_lst   \u001b[38;5;241m=\u001b[39m [lemmas(toks) \u001b[38;5;28;01mfor\u001b[39;00m toks \u001b[38;5;129;01min\u001b[39;00m tqdm(tokens_lst, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlemmatize\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;66;03m# TF-IDF на леммах (1–2-граммы)\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[32], line 74\u001b[0m, in \u001b[0;36mtok_spell\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtok_spell\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     73\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m raz_tokenize(text)\n\u001b[1;32m---> 74\u001b[0m     fixed \u001b[38;5;241m=\u001b[39m [spell\u001b[38;5;241m.\u001b[39mcorrection(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fixed\n",
            "Cell \u001b[1;32mIn[32], line 74\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtok_spell\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     73\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m raz_tokenize(text)\n\u001b[1;32m---> 74\u001b[0m     fixed \u001b[38;5;241m=\u001b[39m [\u001b[43mspell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fixed\n",
            "File \u001b[1;32mD:\\I_NLP\\i_nlp\\lib\\site-packages\\spellchecker\\spellchecker.py:159\u001b[0m, in \u001b[0;36mSpellChecker.correction\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The most probable correct spelling for the word\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;124;03m    word (str): The word to correct\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m    str: The most likely candidate or None if no correction is present\"\"\"\u001b[39;00m\n\u001b[0;32m    158\u001b[0m word \u001b[38;5;241m=\u001b[39m ensure_unicode(word)\n\u001b[1;32m--> 159\u001b[0m candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcandidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m candidates:\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mD:\\I_NLP\\i_nlp\\lib\\site-packages\\spellchecker\\spellchecker.py:186\u001b[0m, in \u001b[0;36mSpellChecker.candidates\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# if still not found, use the edit distance 1 to calc edit distance 2\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distance \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 186\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknown(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__edit_distance_alt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tmp:\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tmp\n",
            "File \u001b[1;32mD:\\I_NLP\\i_nlp\\lib\\site-packages\\spellchecker\\spellchecker.py:253\u001b[0m, in \u001b[0;36mSpellChecker.__edit_distance_alt\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m    251\u001b[0m tmp_words \u001b[38;5;241m=\u001b[39m [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m    252\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)]\n\u001b[1;32m--> 253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [e2 \u001b[38;5;28;01mfor\u001b[39;00m e1 \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mfor\u001b[39;00m e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknown(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medit_distance_1(e1))]\n",
            "File \u001b[1;32mD:\\I_NLP\\i_nlp\\lib\\site-packages\\spellchecker\\spellchecker.py:253\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    251\u001b[0m tmp_words \u001b[38;5;241m=\u001b[39m [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m    252\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)]\n\u001b[1;32m--> 253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [e2 \u001b[38;5;28;01mfor\u001b[39;00m e1 \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mfor\u001b[39;00m e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mknown\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medit_distance_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43me1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m]\n",
            "File \u001b[1;32mD:\\I_NLP\\i_nlp\\lib\\site-packages\\spellchecker\\spellchecker.py:199\u001b[0m, in \u001b[0;36mSpellChecker.known\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The subset of `words` that appear in the dictionary of words\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m    words (list): List of words to determine which are in the corpus\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m    set: The set of those words from the input that are in the corpus\"\"\"\u001b[39;00m\n\u001b[0;32m    198\u001b[0m tmp_words \u001b[38;5;241m=\u001b[39m [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[1;32m--> 199\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words]\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_word_frequency\u001b[38;5;241m.\u001b[39mdictionary \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)}\n",
            "File \u001b[1;32mD:\\I_NLP\\i_nlp\\lib\\site-packages\\spellchecker\\spellchecker.py:199\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The subset of `words` that appear in the dictionary of words\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m    words (list): List of words to determine which are in the corpus\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m    set: The set of those words from the input that are in the corpus\"\"\"\u001b[39;00m\n\u001b[0;32m    198\u001b[0m tmp_words \u001b[38;5;241m=\u001b[39m [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[1;32m--> 199\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words]\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_word_frequency\u001b[38;5;241m.\u001b[39mdictionary \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)}\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Лабораторная № 8  —  Автоматическая классификация депрессивных текстов\n",
        "(«норма / подавленность / клиническая депрессия»).\n",
        "\n",
        "● Корпус: Russian Depressive Posts (32 018 + 32 021)\n",
        "  └─ https://github.com/Ru-Psychology/russian-depressive-posts  (CSV ≈ 45 МБ)\n",
        "\n",
        "Pipeline = 7 предыдущих работ:\n",
        "  1) RegExp-чистка + «ё→е»\n",
        "  2) Токенизация   (razdel)\n",
        "  3) Коррекция     (jamspell)\n",
        "  4) Морфология    (pymorphy2 → леммы)\n",
        "  5) Синтаксис-признак «НЕ-модальный глагол»\n",
        "  6) NER-признак   (упоминание психотропного лекарства)\n",
        "  7) RuSentiLex-признаки + TF-IDF 1–2-грамм\n",
        "\"\"\"\n",
        "\n",
        "# ───────────────────────  INSTALL / IMPORTS  ────────────────────────────\n",
        "# pip install razdel jamspell pymorphy2 stopwordsiso stop-words\n",
        "# pip install scikit-learn pandas numpy tqdm\n",
        "\n",
        "import re, os, json, gzip, itertools, numpy as np, pandas as pd, joblib, string\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "from razdel import tokenize as raz_tokenize\n",
        "from spellchecker import SpellChecker\n",
        "# from jamspell import TSpellCorrector\n",
        "from stopwordsiso import stopwords as sw_iso\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "import pymorphy2\n",
        "from spellchecker import SpellChecker\n",
        "import re\n",
        "\n",
        "# ───────────────────────  0. DATA  ──────────────────────────────────────\n",
        "# DATA_URL = \"https://huggingface.co/datasets/RuPsych/russian_depressive_posts/resolve/main/rus_depr_train.csv\"\n",
        "DATA_CSV = \"Depressive data.xlsx\"\n",
        "if not Path(DATA_CSV).exists():\n",
        "    import urllib.request, ssl\n",
        "    ssl._create_default_https_context = ssl._create_unverified_context\n",
        "    urllib.request.urlretrieve(DATA_URL, DATA_CSV)\n",
        "\n",
        "#df = pd.read_csv(DATA_CSV)     # колонки: text, label  (0-norm / 1-distress / 2-depr)\n",
        "#df = pd.read_csv(DATA_CSV, encoding='cp1251')\n",
        "#df = pd.read_csv(DATA_CSV, encoding='ISO-8859-1')\n",
        "df = pd.read_excel(\"Depressive data.xlsx\")  # пример\n",
        "\n",
        "# ───────────────────────  1. RegExp clean  ─────────────────────────────\n",
        "url_pat = re.compile(r\"http\\S+\")\n",
        "multi_pat = re.compile(r\"(.)\\1{3,}\")        # аааа → аа\n",
        "punct_pat = re.compile(rf\"[^\\w\\s{string.punctuation}]+\")\n",
        "\n",
        "def clean(txt: str) -> str:\n",
        "    txt = url_pat.sub(\" \", str(txt).lower())\n",
        "    txt = txt.replace(\"ё\", \"е\")\n",
        "    txt = multi_pat.sub(r\"\\1\\1\", txt)       # до двух подряд\n",
        "    txt = punct_pat.sub(\" \", txt)\n",
        "    return re.sub(r\"\\s+\", \" \", txt).strip()\n",
        "\n",
        "df[\"clean\"] = df[\"text\"].map(clean)\n",
        "\n",
        "# ───────────────────────  2. Tok + 3. Spell  ───────────────────────────\n",
        "spell = SpellChecker(language=\"ru\")\n",
        "def raz_tokenize(text):\n",
        "    return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "# Функция исправления опечаток и токенизации\n",
        "def tok_spell(text: str):\n",
        "    tokens = raz_tokenize(text)\n",
        "    fixed = [spell.correction(t) for t in tokens]\n",
        "    return fixed\n",
        "\n",
        "# ───────────────────────  4. Lemmatize  ────────────────────────────────\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "def lemmas(tokens):\n",
        "    return [morph.parse(t)[0].normal_form for t in tokens]\n",
        "\n",
        "# ───────────────────────  5. Syntax-feature  (не хочу, не могу …)  ────\n",
        "neg_aux_re = re.compile(r\"\\bне\\s+(хочу|могу|буду|нравится|способен)\\b\")\n",
        "def neg_aux_feat(txt):\n",
        "    return int(bool(neg_aux_re.search(txt)))\n",
        "\n",
        "# ───────────────────────  6. NER-drug (simple list)  ───────────────────\n",
        "DRUGS = {\"флуоксетин\",\"сертралин\",\"пароксетин\",\"анафранил\",\n",
        "         \"амитриптилин\",\"венлафаксин\",\"селектра\",\"золофт\",\n",
        "         \"прозак\",\"эглонил\",\"асентра\"}\n",
        "def drug_feat(txt):\n",
        "    return int(any(w in txt for w in DRUGS))\n",
        "\n",
        "# ───────────────────────  7. RuSentiLex  ───────────────────────────────\n",
        "if not Path(\"rusentilex_2017.txt\").exists():\n",
        "    import urllib.request, ssl, textwrap, io, zipfile, tempfile, pathlib, requests\n",
        "    url = \"https://www.labinform.ru/pub/rusentilex/rusentilex_2017.txt\"\n",
        "    urllib.request.urlretrieve(url, \"rusentilex_2017.txt\")\n",
        "\n",
        "lex = {}\n",
        "with open(\"rusentilex_2017.txt\", encoding=\"utf-8\") as f:\n",
        "    for ln in f:\n",
        "        if ln.startswith(\"!\"): continue\n",
        "        parts = [p.strip() for p in ln.split(\",\")]\n",
        "        if len(parts)>=4:\n",
        "            lemma, pol = parts[2].lower(), parts[3].lower()\n",
        "            if pol in {\"positive\",\"negative\"}: lex[lemma]=pol\n",
        "\n",
        "def lex_feats(lems):\n",
        "    pos = sum(1 for w in lems if lex.get(w)==\"positive\")\n",
        "    neg = sum(1 for w in lems if lex.get(w)==\"negative\")\n",
        "    total = len(lems) or 1\n",
        "    return pos/total, neg/total\n",
        "\n",
        "# ───────────────────────  STOP-LIST  ────────────────────────────────────\n",
        "ru_stop = set(sw_iso(\"ru\")) | {\"это\",\"как\",\"когда\",\"кто\",\"чего\",\"этот\",\"ваш\",\n",
        "                               \"наш\",\"твой\",\"день\",\"свой\",\"еще\"}\n",
        "\n",
        "# ───────────────────────  Фиче-функция  ────────────────────────────────\n",
        "def build_features(texts):\n",
        "    tokens_lst   = [tok_spell(t) for t in tqdm(texts, desc=\"tokenize+spell\")]\n",
        "    lemmas_lst   = [lemmas(toks) for toks in tqdm(tokens_lst, desc=\"lemmatize\")]\n",
        "\n",
        "    # TF-IDF на леммах (1–2-граммы)\n",
        "    docs = [\" \".join(l) for l in lemmas_lst]\n",
        "    tfidf = TfidfVectorizer(ngram_range=(1,2),\n",
        "                            max_features=60_000,\n",
        "                            min_df=3,\n",
        "                            stop_words=ru_stop,\n",
        "                            sublinear_tf=True)\n",
        "    X_tfidf = tfidf.fit_transform(docs)\n",
        "\n",
        "    # дополнительные признаки\n",
        "    extras = []\n",
        "    for txt,lms in zip(texts,lemmas_lst):\n",
        "        pos_r,neg_r = lex_feats(lms)\n",
        "        extras.append([neg_aux_feat(txt), drug_feat(txt), pos_r, neg_r])\n",
        "    X_ex = csr_matrix(extras, dtype=np.float32)\n",
        "\n",
        "    return hstack([X_tfidf, X_ex]), tfidf\n",
        "\n",
        "# ───────────────────────  Train / Test split  ──────────────────────────\n",
        "from sklearn.model_selection import train_test_split\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(df[\"clean\"], df[\"label\"],\n",
        "                                               test_size=0.15,\n",
        "                                               random_state=42,\n",
        "                                               stratify=df[\"label\"])\n",
        "\n",
        "# ─── строим фичи\n",
        "X_train, tfidf = build_features(Xtrain.tolist())\n",
        "X_test  = hstack([tfidf.transform(Xtest.tolist()),\n",
        "                  csr_matrix([[neg_aux_feat(t),drug_feat(t),*lex_feats(lemmas(tok_spell(t)))]\n",
        "                              for t in Xtest], dtype=np.float32)])\n",
        "\n",
        "# ───────────────────────  Модель  ──────────────────────────────────────\n",
        "clf = LogisticRegression(max_iter=1500,\n",
        "                         class_weight='balanced',\n",
        "                         solver='saga', C=4.0, n_jobs=1)\n",
        "clf.fit(X_train, ytrain)\n",
        "\n",
        "ypred = clf.predict(X_test)\n",
        "print(classification_report(ytest, ypred, digits=3))\n",
        "print(\"macro-F1 =\", f1_score(ytest, ypred, average=\"macro\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSXH3_KJEluZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "3rbnBId4QvTM"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "i_nlp(Python)",
      "language": "python",
      "name": "i_nlp"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}