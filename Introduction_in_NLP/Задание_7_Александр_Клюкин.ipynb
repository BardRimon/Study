{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0vXVPDaEluG"
      },
      "source": [
        "# 7. Классификатор тональности"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Использовать в классификации внешний словарь тональностей.\n",
        "2. Улучшить качество базовой предсказательной модели на тестовой выборке за счет добавления и модификации признаков.\n",
        "3. Сравнить качество классификации на леммах и подтокенах.\n",
        "4. Обучить fasttext-классификатор, сравнить качество классификации с предобученными эмбеддингами и обученными с нуля при классификации.\n"
      ],
      "metadata": {
        "id": "QHVpET0cgPc_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ayzkRjwEluG"
      },
      "source": [
        "- В качестве решения любого задания <b>не принимается</b> модель с качеством менее 62.00% макроусредненной F1 на тесте.\n",
        "- <b>Можно</b> улучшать модели сверх предложенных условий: <b>добавлять свои признаки к указанным в задании</b>, изменять способ классификации и подбирать гиперпараметры.\n",
        "- Тестовые данные можно использовать только при оценке моделей.\n",
        "\n",
        "\n",
        "Данные для обучения моделей:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wrq6BpMrEluI"
      },
      "source": [
        "### Импорт датасета"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download ru_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2QI7pn5ixz3",
        "outputId": "01ad854f-55e8-4b1b-f203-fbbb0e778c17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting ru-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.8.0/ru_core_news_sm-3.8.0-py3-none-any.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymorphy3>=1.0.0 (from ru-core-news-sm==3.8.0)\n",
            "  Downloading pymorphy3-2.0.3-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting dawg2-python>=0.8.0 (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0)\n",
            "  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Downloading pymorphy3-2.0.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg2-python, pymorphy3, ru-core-news-sm\n",
            "Successfully installed dawg2-python-0.9.0 pymorphy3-2.0.3 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://www.dropbox.com/s/t1gs701zvqaxqnk/rusentiment_random_posts.csv\n",
        "! wget https://www.dropbox.com/s/gr4z1x39y1j6dtx/rusentiment_test.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJBFS5yUjL0W",
        "outputId": "71aebaf5-11fc-48aa-c472-e25afb170d35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-04 10:46:51--  https://www.dropbox.com/s/t1gs701zvqaxqnk/rusentiment_random_posts.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.dropbox.com/scl/fi/y17smfk1ptufngw720uny/rusentiment_random_posts.csv?rlkey=p9e77phv8eu6fwh6tou0fz232 [following]\n",
            "--2025-06-04 10:46:51--  https://www.dropbox.com/scl/fi/y17smfk1ptufngw720uny/rusentiment_random_posts.csv?rlkey=p9e77phv8eu6fwh6tou0fz232\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucf045c585fde7389ce1f30c530f.dl.dropboxusercontent.com/cd/0/inline/Cq9w8kmS62Z1mcQpzYC50AXMrmH-Plp6uCbzvhjE13pr0MkkeLStKTAqzhTSp1Ynw0JuboMcIQk3LSzlgPJx9zrAD7F3Oha2RluORE-QN-AKKZudQVqGKP6ESSpIPqBJ045JqiMQVAahDL22v3CQjAny/file# [following]\n",
            "--2025-06-04 10:46:52--  https://ucf045c585fde7389ce1f30c530f.dl.dropboxusercontent.com/cd/0/inline/Cq9w8kmS62Z1mcQpzYC50AXMrmH-Plp6uCbzvhjE13pr0MkkeLStKTAqzhTSp1Ynw0JuboMcIQk3LSzlgPJx9zrAD7F3Oha2RluORE-QN-AKKZudQVqGKP6ESSpIPqBJ045JqiMQVAahDL22v3CQjAny/file\n",
            "Resolving ucf045c585fde7389ce1f30c530f.dl.dropboxusercontent.com (ucf045c585fde7389ce1f30c530f.dl.dropboxusercontent.com)... 162.125.6.15, 2620:100:601b:15::a27d:80f\n",
            "Connecting to ucf045c585fde7389ce1f30c530f.dl.dropboxusercontent.com (ucf045c585fde7389ce1f30c530f.dl.dropboxusercontent.com)|162.125.6.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3158556 (3.0M) [text/plain]\n",
            "Saving to: ‘rusentiment_random_posts.csv’\n",
            "\n",
            "rusentiment_random_ 100%[===================>]   3.01M  10.5MB/s    in 0.3s    \n",
            "\n",
            "2025-06-04 10:46:52 (10.5 MB/s) - ‘rusentiment_random_posts.csv’ saved [3158556/3158556]\n",
            "\n",
            "--2025-06-04 10:46:52--  https://www.dropbox.com/s/gr4z1x39y1j6dtx/rusentiment_test.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.dropbox.com/scl/fi/85acvyvqf1nwgy365x2lf/rusentiment_test.csv?rlkey=x7figfoeyh5p1uq5vq97xg8hm [following]\n",
            "--2025-06-04 10:46:53--  https://www.dropbox.com/scl/fi/85acvyvqf1nwgy365x2lf/rusentiment_test.csv?rlkey=x7figfoeyh5p1uq5vq97xg8hm\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc7b8fa99dc047f04b0b5d5b4332.dl.dropboxusercontent.com/cd/0/inline/Cq-kIr19fbMvrsYnw8mlqsoN0n9uZcY09wW2VS4PqyjkrMCj9Ekq-ooCh5-DaW0k3RO-RE4cTeVgD6PNNnqCWENPDIExPHKWxlz9k-60PyWTGM1WEpKkm_i2ZWrM2gD2B5Xh2cDYtR9Ifqs6OgXOkT5c/file# [following]\n",
            "--2025-06-04 10:46:53--  https://uc7b8fa99dc047f04b0b5d5b4332.dl.dropboxusercontent.com/cd/0/inline/Cq-kIr19fbMvrsYnw8mlqsoN0n9uZcY09wW2VS4PqyjkrMCj9Ekq-ooCh5-DaW0k3RO-RE4cTeVgD6PNNnqCWENPDIExPHKWxlz9k-60PyWTGM1WEpKkm_i2ZWrM2gD2B5Xh2cDYtR9Ifqs6OgXOkT5c/file\n",
            "Resolving uc7b8fa99dc047f04b0b5d5b4332.dl.dropboxusercontent.com (uc7b8fa99dc047f04b0b5d5b4332.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:601b:15::a27d:80f\n",
            "Connecting to uc7b8fa99dc047f04b0b5d5b4332.dl.dropboxusercontent.com (uc7b8fa99dc047f04b0b5d5b4332.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 441232 (431K) [text/plain]\n",
            "Saving to: ‘rusentiment_test.csv’\n",
            "\n",
            "rusentiment_test.cs 100%[===================>] 430.89K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-06-04 10:46:53 (7.70 MB/s) - ‘rusentiment_test.csv’ saved [441232/441232]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NdB9Z8eEluI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc2feb36-44b6-41b3-d227-85c3f5e52058"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd, re, numpy as np # pymorphy2,\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import spacy\n",
        "nlp = spacy.load(\"ru_core_news_sm\")\n",
        "import re, pandas as pd, numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk; nltk.download('stopwords')\n",
        "\n",
        "ru_stop = stopwords.words(\"russian\")\n",
        "\n",
        "# morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "df_train = pd.read_csv('rusentiment_random_posts.csv')\n",
        "df_test  = pd.read_csv('rusentiment_test.csv')          # **использовать ТОЛЬКО для финальной оценки!**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(text):\n",
        "    text = re.sub(r'http\\S+|\\W+', ' ', str(text).lower())\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "def lemmatize(text):\n",
        "    doc = nlp(text)\n",
        "    lemmas = [token.lemma_ for token in doc]\n",
        "    return ' '.join(lemmas)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hYSGXI0ckBbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['lemmas'] = df_train['text'].map(clean).map(lemmatize)"
      ],
      "metadata": {
        "id": "kOpiMX7hkEnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['lemmas'] = df_test['text'].map(clean).map(lemmatize)"
      ],
      "metadata": {
        "id": "g0iWlq-clllU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLu8xsoVEluL",
        "outputId": "f6c2a562-ec87-4889-bd7f-2b251dc4d0ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s49Ps2m8EluM"
      },
      "source": [
        "### 1 . Добавляем признаки из внешнего словаря тональностей (RuSentiLex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvrIldO6EluM",
        "outputId": "81430550-0141-46f2-99d3-43f06ebfdee6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('rusentilex_2017.txt', <http.client.HTTPMessage at 0x7d901c1e6dd0>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import urllib.request\n",
        "\n",
        "url = 'https://www.labinform.ru/pub/rusentilex/rusentilex_2017.txt'\n",
        "save_path = 'rusentilex_2017.txt'\n",
        "\n",
        "urllib.request.urlretrieve(url, save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Of8CQ8hyEluM"
      },
      "outputs": [],
      "source": [
        "lexicon = {}\n",
        "\n",
        "with open(\"rusentilex_2017.txt\", encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line or line.startswith('!'):\n",
        "            continue  # пропускаем комментарии и пустые строки\n",
        "\n",
        "        parts = [p.strip() for p in line.split(',')]\n",
        "        if len(parts) < 4:\n",
        "            continue  # пропускаем повреждённые строки\n",
        "\n",
        "        lemma = parts[2].lower()\n",
        "        polarity = parts[3].lower()\n",
        "\n",
        "        # игнорируем амбивалентные (positive/negative) — чтобы не вносить шум\n",
        "        if polarity in ['positive', 'negative', 'neutral']:\n",
        "            lexicon[lemma] = polarity\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Размер словаря lexicon: {len(lexicon)}\")\n",
        "\n",
        "unique_values = set()\n",
        "for value in lexicon.values():\n",
        "    unique_values.add(value)\n",
        "\n",
        "print(f\"Уникальные значения по всем ключам: {unique_values}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITaOJUaZh3Zv",
        "outputId": "35d03d8f-2e30-433a-dd93-7319144daf75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размер словаря lexicon: 13295\n",
            "Уникальные значения по всем ключам: {'negative', 'neutral', 'positive'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_1AkThuEluN"
      },
      "outputs": [],
      "source": [
        "def lexicon_feats(text):\n",
        "    pos = neg = 0\n",
        "    for w in text.split():\n",
        "        s = lexicon.get(w)\n",
        "        if s == 'positive': pos += 1\n",
        "        elif s == 'negative': neg += 1\n",
        "    total = len(text.split()) or 1\n",
        "    return pd.Series({'pos_cnt':pos, 'neg_cnt':neg, 'pos_ratio':pos/total, 'neg_ratio':neg/total})\n",
        "\n",
        "lex_feats_train = df_train['lemmas'].apply(lexicon_feats)\n",
        "lex_feats_test = df_test['lemmas'].apply(lexicon_feats)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zsK3DBlyldj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7iqMHX3EluN"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "tfidf = TfidfVectorizer(\n",
        "    analyzer='word',\n",
        "    ngram_range=(1,3),\n",
        "    max_features=60000,\n",
        "    min_df=2,\n",
        "    sublinear_tf=True\n",
        ")\n",
        "\n",
        "X_train_tfidf = tfidf.fit_transform(df_train['lemmas'])\n",
        "X_test_tfidf = tfidf.transform(df_test['lemmas'])\n",
        "\n",
        "# нормализация лексиконных признаков\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_lex = scaler.fit_transform(lex_feats_train)\n",
        "X_test_lex  = scaler.transform(lex_feats_test)\n",
        "\n",
        "# объединение\n",
        "from scipy.sparse import hstack\n",
        "X_train_all = hstack([X_train_tfidf, X_train_lex])\n",
        "X_test_all  = hstack([X_test_tfidf,  X_test_lex])\n",
        "y_train = df_train['label']\n",
        "y_test = df_test['label']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LogisticRegression с более мощной регуляризацией\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression(C=6.0, class_weight='balanced', solver='liblinear', max_iter=1200)\n",
        "clf.fit(X_train_all, y_train)\n",
        "y_pred = clf.predict(X_test_all)\n",
        "\n"
      ],
      "metadata": {
        "id": "nek-tf9VmNWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "print(f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAk6q_PXmOYR",
        "outputId": "312e44fb-7acc-481d-c891-6738a479117c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6250281887176338\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rf8-Pe4zEluO"
      },
      "source": [
        "### 2 . Улучшить качество базовой предсказательной модели на тестовой выборке за счет добавления и модификации признаков."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "word_v = TfidfVectorizer(analyzer='word', ngram_range=(1,3), max_features=40_000,\n",
        "                         sublinear_tf=True, min_df=3)\n",
        "char_v = TfidfVectorizer(analyzer='char', ngram_range=(3,5), min_df=5)\n",
        "\n",
        "X_word = word_v.fit_transform(df_train['lemmas'])\n",
        "X_char = char_v.fit_transform(df_train['lemmas'])\n",
        "X      = hstack([X_word, X_char, lex_feats_train.values])\n",
        "\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X, df_train['label'], test_size=0.2, random_state=42, stratify=df_train['label'])"
      ],
      "metadata": {
        "id": "9ldyMzk7nC6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression(solver='liblinear', C=4, class_weight='balanced', max_iter=300)\n",
        "clf.fit(X_tr, y_tr)\n",
        "y_pred = clf.predict(X_val)\n",
        "print(f1_score(y_val, y_pred, average='macro')) # получилось с 1 раза, успешный успех"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XqOuGUdnEg4",
        "outputId": "5ad74192-20fa-49c7-d4d9-4fb4fcaae409"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6293205946201473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzacKzouEluO"
      },
      "source": [
        "### 3 .Сравнить качество классификации на леммах и подтокенах."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKCKFequEluP",
        "outputId": "750170f8-567d-4e45-88df-a401340d4fc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DREjD3_oEluQ"
      },
      "outputs": [],
      "source": [
        "def lexicon_feats(text):\n",
        "    pos = neg = neu = 0\n",
        "    for w in text.split():\n",
        "        s = lexicon.get(w)\n",
        "        if s == 'positive': pos += 1\n",
        "        elif s == 'negative': neg += 1\n",
        "        elif s == 'neutral':  neu += 1\n",
        "    total = len(text.split()) or 1\n",
        "    return pd.Series({\n",
        "        'pos_cnt': pos, 'neg_cnt': neg, 'neu_cnt': neu,\n",
        "        'pos_ratio': pos/total, 'neg_ratio': neg/total, 'neu_ratio': neu/total\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, re, sentencepiece as spm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "\n",
        "\n",
        "with open(\"corpus_lemmas.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in df_train['lemmas']:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input='corpus_lemmas.txt',\n",
        "    model_prefix='bpe_lemmas',\n",
        "    vocab_size=16000,\n",
        "    model_type='bpe',\n",
        "    character_coverage=1.0\n",
        ")\n",
        "\n",
        "sp = spm.SentencePieceProcessor(model_file='bpe_lemmas.model')\n",
        "df_train['bpe'] = df_train['lemmas'].apply(lambda x: ' '.join(sp.encode(x, out_type=str)))\n",
        "df_test['bpe']  = df_test['lemmas'].apply(lambda x: ' '.join(sp.encode(x, out_type=str)))"
      ],
      "metadata": {
        "id": "zvsG-S3tpgL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lexicon = {}\n",
        "with open(\"rusentilex_2017.txt\", encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        if line.startswith('!') or not line.strip():\n",
        "            continue\n",
        "        parts = [p.strip() for p in line.split(',')]\n",
        "        if len(parts) >= 4:\n",
        "            lemma, polarity = parts[2].lower(), parts[3].lower()\n",
        "            if polarity in ['positive', 'negative', 'neutral']:\n",
        "                lexicon[lemma] = polarity\n",
        "\n",
        "def lexicon_feats(text):\n",
        "    pos = neg = neu = 0\n",
        "    for w in text.split():\n",
        "        s = lexicon.get(w)\n",
        "        if s == 'positive': pos += 1\n",
        "        elif s == 'negative': neg += 1\n",
        "        elif s == 'neutral':  neu += 1\n",
        "    total = len(text.split()) or 1\n",
        "    return pd.Series({\n",
        "        'pos_cnt': pos, 'neg_cnt': neg, 'neu_cnt': neu,\n",
        "        'pos_ratio': pos/total, 'neg_ratio': neg/total, 'neu_ratio': neu/total\n",
        "    })\n",
        "\n",
        "lex_feats_train = df_train['lemmas'].apply(lexicon_feats)\n",
        "lex_feats_test  = df_test['lemmas'].apply(lexicon_feats)"
      ],
      "metadata": {
        "id": "h6JMMmfEpmGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF (BPE + char)\n",
        "tfidf_word = TfidfVectorizer(\n",
        "    analyzer='word',\n",
        "    ngram_range=(1,2),\n",
        "    max_features=50000,\n",
        "    min_df=2,\n",
        "    sublinear_tf=True\n",
        ")\n",
        "tfidf_char = TfidfVectorizer(\n",
        "    analyzer='char',\n",
        "    ngram_range=(3,5),\n",
        "    max_features=20000,\n",
        "    min_df=2,\n",
        "    sublinear_tf=True\n",
        ")\n",
        "\n",
        "X_train_word = tfidf_word.fit_transform(df_train['bpe'])\n",
        "X_test_word  = tfidf_word.transform(df_test['bpe'])\n",
        "\n",
        "X_train_char = tfidf_char.fit_transform(df_train['bpe'])\n",
        "X_test_char  = tfidf_char.transform(df_test['bpe'])"
      ],
      "metadata": {
        "id": "lhspCNV-poE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. Нормализация признаков lexicon\n",
        "scaler = StandardScaler()\n",
        "lex_feats_train_scaled = scaler.fit_transform(lex_feats_train)\n",
        "lex_feats_test_scaled  = scaler.transform(lex_feats_test)\n",
        "\n",
        "# --- 7. Объединение признаков\n",
        "X_train_all = hstack([X_train_word, X_train_char, lex_feats_train_scaled])\n",
        "X_test_all  = hstack([X_test_word,  X_test_char,  lex_feats_test_scaled])\n",
        "y_train = df_train['label']\n",
        "y_test = df_test['label']"
      ],
      "metadata": {
        "id": "3BzY_E7ipub-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression(\n",
        "    C=1.0,\n",
        "    class_weight='balanced',\n",
        "    solver='liblinear',\n",
        "    max_iter=1000\n",
        ")\n",
        "clf.fit(X_train_all, y_train)\n",
        "y_pred = clf.predict(X_test_all)"
      ],
      "metadata": {
        "id": "5IZ8J0P_pZn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 9. Оценка\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "print(f\"f1 для подтокенов = {f1}\")\n",
        "print(f\"f1 на леммах получился = 0.6293205946201473\") # значение из 2 пункта\n",
        "print(f\"f1 мера на подтокенах получается лучше\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGuBRkJFpwCF",
        "outputId": "53a09138-ec02-489d-87d8-13243caee2aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1 для подтокенов = 0.6360764709254842\n",
            "f1 на леммах получился = 0.6293205946201473\n",
            "f1 мера на подтокенах получается лучше\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xC8ABv8EluR"
      },
      "source": [
        "#### 4. Обучить fasttext-классификатор, сравнить качество классификации с предобученными эмбеддингами и обученными с нуля при классификации.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install fasttext"
      ],
      "metadata": {
        "id": "SnSsDVt6su1H",
        "outputId": "e766c166-e1f4-42cb-db48-0b401c903830",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext) (2.0.2)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp311-cp311-linux_x86_64.whl size=4313504 sha256=8fdf03c825d5a28928319abcf0190e3f103b3757bfb8f60afab9b1c0752f1806\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/35/5057db0249224e9ab55a513fa6b79451473ceb7713017823c3\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImYBL1NQEluR"
      },
      "outputs": [],
      "source": [
        "def to_fasttext_format(df, path, text_col='lemmas'):\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        for text, label in zip(df[text_col], df['label']):\n",
        "            f.write(f\"__label__{label} {text.strip()}\\n\")\n",
        "\n",
        "to_fasttext_format(df_train, \"train_ft.txt\")\n",
        "to_fasttext_format(df_test, \"test_ft.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "\n",
        "model_ft = fasttext.train_supervised(\n",
        "    input=\"train_ft.txt\",\n",
        "    lr=0.8,\n",
        "    epoch=50,\n",
        "    wordNgrams=2,\n",
        "    minn=2,\n",
        "    maxn=5,\n",
        "    dim=100,\n",
        "    loss='ova'\n",
        ")"
      ],
      "metadata": {
        "id": "7vYYveAhtPRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N, P, R = model_ft.test(\"test_ft.txt\")\n",
        "f1 = 2 * P * R / (P + R)\n",
        "print(f\"f1 fastText: {f1}\")"
      ],
      "metadata": {
        "id": "lKSDM18n8mNv",
        "outputId": "a3ed1a79-a2a3-43f0-c2a5-6845fb8515f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1 fastText (без предобученных эмб): 0.6545331985170205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "а теперь используем предобученную:"
      ],
      "metadata": {
        "id": "0mNoEV_3xz3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget\n",
        "# нагло нагуглил, как импортировать\n",
        "import wget\n",
        "import os\n",
        "\n",
        "url = 'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ru.300.vec.gz'\n",
        "\n",
        "gz_filename = 'cc.ru.300.vec.gz'\n",
        "vec_filename = 'cc.ru.300.vec'\n",
        "\n",
        "if not os.path.exists(gz_filename):\n",
        "    print(f\"Скачивание {gz_filename}...\")\n",
        "    wget.download(url, gz_filename)\n",
        "    print(f\"\\nСкачивание {gz_filename} завершено.\")\n",
        "else:\n",
        "    print(f\"Файл {gz_filename} уже существует, скачивание пропущено.\")\n",
        "\n",
        "if not os.path.exists(vec_filename):\n",
        "    print(f\"Распаковка {gz_filename} в {vec_filename}...\")\n",
        "    import gzip\n",
        "    with gzip.open(gz_filename, 'rb') as f_in:\n",
        "        with open(vec_filename, 'wb') as f_out:\n",
        "            # Чтение по частям для экономии памяти\n",
        "            chunk_size = 4096\n",
        "            while True:\n",
        "                chunk = f_in.read(chunk_size)\n",
        "                if not chunk:\n",
        "                    break\n",
        "                f_out.write(chunk)\n",
        "    print(f\"Распаковка завершена.\")\n",
        "else:\n",
        "     print(f\"Файл {vec_filename} уже существует, распаковка пропущена.\")\n",
        "\n",
        "print(f\"Файл с предобученными векторами доступен по пути: {vec_filename}\")"
      ],
      "metadata": {
        "id": "lmbbBTYEyNB5",
        "outputId": "869a33f5-7cb3-4814-c7cf-b775e04763db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=eb44b048c04b76d67ac3bdedba389ee4fbf87b4acb98f1e7adc5548f52716361\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Скачивание cc.ru.300.vec.gz...\n",
            "\n",
            "Скачивание cc.ru.300.vec.gz завершено.\n",
            "Распаковка cc.ru.300.vec.gz в cc.ru.300.vec...\n",
            "Распаковка завершена.\n",
            "Файл с предобученными векторами доступен по пути: cc.ru.300.vec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_vectors_path = \"cc.ru.300.vec\"\n",
        "model_pre = fasttext.train_supervised(\n",
        "    input=\"train_ft.txt\",\n",
        "    pretrainedVectors=pretrained_vectors_path, # Путь к предобученным векторам\n",
        "    epoch=50,                          # вернул 50\n",
        "    lr=0.5,\n",
        "    wordNgrams=2,\n",
        "    dim=300,                           # Размерность векторов (должна совпадать с dim предобученных векторов)\n",
        "    loss='ova',\n",
        ")"
      ],
      "metadata": {
        "id": "ZU5zrQ6sx3LS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N, P, R = model_pre.test(\"test_ft.txt\")\n",
        "f1 = 2 * P * R / (P + R)\n",
        "print(f\"f1 fastText: {f1}\")"
      ],
      "metadata": {
        "id": "YlsUZy3p9emG",
        "outputId": "86833173-8719-443a-aee6-0c1133d34cb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1 fastText (c предобученных эмб): 0.6819472685037192\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "3rbnBId4QvTM"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}