{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBodCyQQ1ef3YWCjUUHn0+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BardRimon/Study/blob/main/Introduction_in_NLP/%D0%97%D0%B0%D0%B4%D0%B0%D0%BD%D0%B8%D0%B5_5_%D0%90%D0%BB%D0%B5%D0%BA%D1%81%D0%B0%D0%BD%D0%B4%D1%80_%D0%9A%D0%BB%D1%8E%D0%BA%D0%B8%D0%BD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Тема 7. Определение тональности текста\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fn3VT1zm85Ho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## импорт датасетов"
      ],
      "metadata": {
        "id": "rozf3MdT_CFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://www.dropbox.com/s/t1gs701zvqaxqnk/rusentiment_random_posts.csv\n",
        "! wget https://www.dropbox.com/s/gr4z1x39y1j6dtx/rusentiment_test.csv"
      ],
      "metadata": {
        "id": "9CuGEZZr-Rhd",
        "outputId": "27aa0a9f-334f-4089-ba00-9acb8342eebb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-03 13:26:56--  https://www.dropbox.com/s/t1gs701zvqaxqnk/rusentiment_random_posts.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.4.18, 2620:100:601c:18::a27d:612\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.4.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.dropbox.com/scl/fi/y17smfk1ptufngw720uny/rusentiment_random_posts.csv?rlkey=p9e77phv8eu6fwh6tou0fz232 [following]\n",
            "--2025-06-03 13:26:57--  https://www.dropbox.com/scl/fi/y17smfk1ptufngw720uny/rusentiment_random_posts.csv?rlkey=p9e77phv8eu6fwh6tou0fz232\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc54c7fc393f0dbadbb9d1944504.dl.dropboxusercontent.com/cd/0/inline/Cq7p8tudso0A18mOIrDdxLcX7US5x3v_msoH0DO1glJUt4qzuF4VOylWZ0787wi_RnRz0VYxVVpj5ok01LZMlOq4y4vqqI40nveFo27mE7272L6lL2mQtOx82izrJ2JbNUciaf1rpq9Mi1Z7fh1ZZqXI/file# [following]\n",
            "--2025-06-03 13:26:57--  https://uc54c7fc393f0dbadbb9d1944504.dl.dropboxusercontent.com/cd/0/inline/Cq7p8tudso0A18mOIrDdxLcX7US5x3v_msoH0DO1glJUt4qzuF4VOylWZ0787wi_RnRz0VYxVVpj5ok01LZMlOq4y4vqqI40nveFo27mE7272L6lL2mQtOx82izrJ2JbNUciaf1rpq9Mi1Z7fh1ZZqXI/file\n",
            "Resolving uc54c7fc393f0dbadbb9d1944504.dl.dropboxusercontent.com (uc54c7fc393f0dbadbb9d1944504.dl.dropboxusercontent.com)... 162.125.6.15, 2620:100:601c:15::a27d:60f\n",
            "Connecting to uc54c7fc393f0dbadbb9d1944504.dl.dropboxusercontent.com (uc54c7fc393f0dbadbb9d1944504.dl.dropboxusercontent.com)|162.125.6.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3158556 (3.0M) [text/plain]\n",
            "Saving to: ‘rusentiment_random_posts.csv’\n",
            "\n",
            "rusentiment_random_ 100%[===================>]   3.01M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-06-03 13:26:57 (89.8 MB/s) - ‘rusentiment_random_posts.csv’ saved [3158556/3158556]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: импортируй файлы .csv и разбей на train and test\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_train = pd.read_csv('rusentiment_random_posts.csv')\n",
        "df_test = pd.read_csv('rusentiment_test.csv')\n",
        "\n",
        "# Assuming your CSV has a column named 'text' for text data and 'label' for sentiment label\n",
        "X_train = df_train['text']\n",
        "y_train = df_train['label']\n",
        "\n",
        "X_test = df_test['text']\n",
        "y_test = df_test['label']\n",
        "\n",
        "# If you want to split the training data further into train and validation sets\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "ZrGUi9wU-bzL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.value_counts()"
      ],
      "metadata": {
        "id": "H7pblqbCFkx8",
        "outputId": "035d22e9-5cae-4d4d-afbe-7b39284b500b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "neutral     8323\n",
              "positive    4635\n",
              "skip        3190\n",
              "speech      2826\n",
              "negative    2294\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>neutral</th>\n",
              "      <td>8323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>4635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>skip</th>\n",
              "      <td>3190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>speech</th>\n",
              "      <td>2826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>2294</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задача 1: Использовать в классификации внешний словарь тональностей.\n"
      ],
      "metadata": {
        "id": "zYRFXOlz86bk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "W_V6_82dF5rw",
        "outputId": "0c7cd783-2436-4c2c-96bc-ed5eaeb01aa8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: используя большой словарь тональностей реши задачу классификации для текстов на русском языке из интернета, имея следующие метки классов: neutral\t8323\n",
        "# positive\t4635\n",
        "# skip\t3190\n",
        "# speech\t2826\n",
        "# negative\t2294\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Simplified external sentiment dictionary (example)\n",
        "# In a real scenario, you would load a comprehensive dictionary.\n",
        "sentiment_dict = {\n",
        "    'хорошо': 'positive',\n",
        "    'отлично': 'positive',\n",
        "    'прекрасно': 'positive',\n",
        "    'плохо': 'negative',\n",
        "    'ужасно': 'negative',\n",
        "    'кошмар': 'negative',\n",
        "    'нейтрально': 'neutral',\n",
        "    'обычно': 'neutral',\n",
        "    'нормально': 'neutral',\n",
        "}\n",
        "\n",
        "stop_words = set(stopwords.words('russian'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'http\\S+', '', text) # Remove URLs\n",
        "    text = re.sub(r'[^a-zA-Zа-яА-ЯёЁ\\s]', '', text) # Remove special characters\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def get_sentiment_from_dict(text):\n",
        "    sentiment_scores = {'positive': 0, 'negative': 0, 'neutral': 0}\n",
        "    words = text.split()\n",
        "    for word in words:\n",
        "        if word in sentiment_dict:\n",
        "            sentiment = sentiment_dict[word]\n",
        "            sentiment_scores[sentiment] += 1\n",
        "\n",
        "    # Basic rule: if more positive words than negative, classify as positive, etc.\n",
        "    if sentiment_scores['positive'] > sentiment_scores['negative']:\n",
        "        return 'positive'\n",
        "    elif sentiment_scores['negative'] > sentiment_scores['positive']:\n",
        "        return 'negative'\n",
        "    elif sentiment_scores['neutral'] > 0 and sentiment_scores['positive'] == 0 and sentiment_scores['negative'] == 0:\n",
        "        return 'neutral'\n",
        "    else:\n",
        "        return 'skip' # Or another fallback class\n",
        "\n",
        "# Apply preprocessing\n",
        "X_train_processed = X_train.apply(preprocess_text)\n",
        "X_test_processed = X_test.apply(preprocess_text)\n",
        "\n",
        "# Option 1: Use the sentiment dictionary directly (simpler, less accurate)\n",
        "y_train_dict_pred = X_train_processed.apply(get_sentiment_from_dict)\n",
        "y_test_dict_pred = X_test_processed.apply(get_sentiment_from_dict)\n",
        "\n",
        "print(\"Classification Report using Sentiment Dictionary Directly:\")\n",
        "print(classification_report(y_test, y_test_dict_pred, zero_division=0))\n",
        "\n",
        "\n",
        "# Option 2: Incorporate sentiment dictionary as a feature (more complex, potentially better)\n",
        "# This is a basic example of feature engineering with the dictionary.\n",
        "# You could also use TF-IDF and then add dictionary features.\n",
        "\n",
        "def add_sentiment_features(text):\n",
        "    sentiment_scores = {'positive_score': 0, 'negative_score': 0, 'neutral_score': 0}\n",
        "    words = text.split()\n",
        "    for word in words:\n",
        "        if word in sentiment_dict:\n",
        "            sentiment = sentiment_dict[word]\n",
        "            if sentiment == 'positive':\n",
        "                sentiment_scores['positive_score'] += 1\n",
        "            elif sentiment == 'negative':\n",
        "                sentiment_scores['negative_score'] += 1\n",
        "            elif sentiment == 'neutral':\n",
        "                sentiment_scores['neutral_score'] += 1\n",
        "    return list(sentiment_scores.values())\n",
        "\n",
        "train_sentiment_features = pd.DataFrame(list(X_train_processed.apply(add_sentiment_features)),\n",
        "                                        columns=['positive_score', 'negative_score', 'neutral_score'])\n",
        "test_sentiment_features = pd.DataFrame(list(X_test_processed.apply(add_sentiment_features)),\n",
        "                                       columns=['positive_score', 'negative_score', 'neutral_score'])\n",
        "\n",
        "# Combine sentiment features with TF-IDF features\n",
        "tfidf = TfidfVectorizer(max_features=5000) # Limit features for simplicity\n",
        "X_train_tfidf = tfidf.fit_transform(X_train_processed)\n",
        "X_test_tfidf = tfidf.transform(X_test_processed)\n",
        "\n",
        "import scipy.sparse as sp\n",
        "X_train_combined = sp.hstack((X_train_tfidf, sp.csr_matrix(train_sentiment_features.values)))\n",
        "X_test_combined = sp.hstack((X_test_tfidf, sp.csr_matrix(test_sentiment_features.values)))\n",
        "\n",
        "\n",
        "# Train a classifier on combined features\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_combined, y_train)\n",
        "y_test_combined_pred = model.predict(X_test_combined)\n",
        "\n",
        "print(\"\\nClassification Report using TF-IDF and Sentiment Dictionary Features:\")\n",
        "print(classification_report(y_test, y_test_combined_pred, zero_division=0))\n",
        "\n"
      ],
      "metadata": {
        "id": "7Q3IiJV79GpI",
        "outputId": "2d14daad-3477-4e12-a67a-f8eb0260b046",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report using Sentiment Dictionary Directly:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.35      0.02      0.04       258\n",
            "     neutral       0.80      0.01      0.01      1420\n",
            "    positive       0.62      0.01      0.02       536\n",
            "        skip       0.12      0.99      0.21       346\n",
            "      speech       0.00      0.00      0.00       407\n",
            "\n",
            "    accuracy                           0.12      2967\n",
            "   macro avg       0.38      0.21      0.06      2967\n",
            "weighted avg       0.54      0.12      0.04      2967\n",
            "\n",
            "\n",
            "Classification Report using TF-IDF and Sentiment Dictionary Features:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.47      0.23      0.31       258\n",
            "     neutral       0.65      0.84      0.73      1420\n",
            "    positive       0.52      0.46      0.49       536\n",
            "        skip       0.56      0.25      0.35       346\n",
            "      speech       0.90      0.83      0.86       407\n",
            "\n",
            "    accuracy                           0.65      2967\n",
            "   macro avg       0.62      0.52      0.55      2967\n",
            "weighted avg       0.63      0.65      0.62      2967\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задача 2: Улучшить качество базовой предсказательной модели на тестовой выборке за счет добавления и модификации признаков.\n"
      ],
      "metadata": {
        "id": "xEDepcfY87ud"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0mlK7oqG9G5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задача 3: Сравнить качество классификации на леммах и подтокенах.\n"
      ],
      "metadata": {
        "id": "Pt-yMfGu8-XF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uP97TpDX9HPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задача 4: Обучить fasttext-классификатор, сравнить качество классификации с предобученными эмбеддингами и обученными с нуля при классификации."
      ],
      "metadata": {
        "id": "zW9eJH6c8_fH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iLllNbe_9FWf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
